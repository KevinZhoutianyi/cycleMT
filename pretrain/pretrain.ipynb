{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_dataset,load_metric\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import time\n",
    "from transformers.optimization import Adafactor\n",
    "import os\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/17 10:15:57 PM |\t  test step: 992\n",
      "06/17 10:15:57 PM |\t  rep step: 96\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "local_test = 1\n",
    "if(local_test==0):\n",
    "    max_length= 512\n",
    "    test_step = 500000\n",
    "    report_step = 10000\n",
    "    seed = 2\n",
    "    bs = 512 \n",
    "    lr = 1e-4\n",
    "    train_num = 1000000\n",
    "    valid_num = 2000\n",
    "else:\n",
    "    max_length= 512\n",
    "    test_step = 1000\n",
    "    report_step = 100\n",
    "    seed = 2\n",
    "    bs = 32\n",
    "    lr = 1e-4\n",
    "    train_num = 200\n",
    "    valid_num = 100\n",
    "\n",
    "\n",
    "test_step = test_step//bs * bs\n",
    "report_step = report_step//bs * bs\n",
    "\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'),'w',encoding = \"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "logging.info(f\"test step: {test_step}\")\n",
    "logging.info(f\"rep step: {report_step}\")\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgrageMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val*n #TODO:its just for W\n",
    "        self.cnt += n\n",
    "        self.avg = self.sum / self.cnt\n",
    "\n",
    "def tokenize(text_data, tokenizer, max_length, padding = True):\n",
    "    \n",
    "    encoding = tokenizer(text_data, return_tensors='pt', padding=padding, truncation = True, max_length = max_length)\n",
    "\n",
    "    input_ids = encoding['input_ids']\n",
    "    \n",
    "    attention_mask = encoding['attention_mask']\n",
    "    \n",
    "    return input_ids, attention_mask\n",
    "def get_Dataset(dataset, tokenizer):\n",
    "    train_sentence = [x['de'] for x in dataset]\n",
    "    train_target = [x['en'] for x in dataset]\n",
    "\n",
    "  \n",
    "    model1_input_ids, model1_input_attention_mask = tokenize(train_sentence, tokenizer, max_length = max_length)\n",
    "  \n",
    "    model1_target_ids, model1_target_attention_mask = tokenize(train_target, tokenizer, max_length = max_length)\n",
    " \n",
    "    train_data = TensorDataset(model1_input_ids, model1_input_attention_mask, model1_target_ids, model1_target_attention_mask)\n",
    "   \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"google/t5-small-lm-adapt\").to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/t5-small-lm-adapt\")\n",
    "optimizer = Adafactor(model.parameters(), lr = lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/17 10:16:01 PM |\t  Reusing dataset wmt14 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 24.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/17 10:16:01 PM |\t  Loading cached shuffled indices for dataset at C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2\\cache-43836e47740ac86b.arrow\n",
      "06/17 10:16:01 PM |\t  Loading cached shuffled indices for dataset at C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2\\cache-c90fbde9f39353e7.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset('wmt14','de-en')\n",
    "train = dataset['train'].shuffle(seed=seed).select(range(train_num))\n",
    "valid = dataset['validation'].shuffle(seed=seed).select(range(valid_num))\n",
    "train = train['translation']\n",
    "valid = valid['translation']\n",
    "\n",
    "def preprocess(dat):\n",
    "    for t in dat:\n",
    "        t['de'] = \"translate German to English: \" + t['de']  #needed for T5\n",
    "preprocess(train)\n",
    "preprocess(valid)\n",
    "\n",
    "train_data = get_Dataset(train, tokenizer)\n",
    "train_dataloader = DataLoader(train_data, sampler= SequentialSampler(train_data), \n",
    "                        batch_size=bs, pin_memory=True, num_workers=4)\n",
    "valid_data = get_Dataset(valid, tokenizer)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=bs, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(_dataloader,model,optimizer,iter):\n",
    "    objs = AvgrageMeter()\n",
    "    \n",
    "    for step,batch in enumerate(_dataloader):\n",
    "        iter[0] += batch[0].shape[0]\n",
    "        optimizer.zero_grad()\n",
    "        train_x = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)\n",
    "        train_x_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)\n",
    "        train_y = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)    \n",
    "        train_y_attn = Variable(batch[3], requires_grad=False).to(device, non_blocking=False)    \n",
    "        train_y[train_y == tokenizer.pad_token_id] = -100\n",
    "        loss = model(input_ids=train_x, attention_mask=train_x_attn, labels=train_y).loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        objs.update(loss.item(), bs)\n",
    "        if(iter[0]%report_step==0 and iter[0]!=0):\n",
    "            logging.info(f'iter:{iter[0]}\\t,avgloss:{objs.avg}')\n",
    "            objs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "@torch.no_grad()\n",
    "def my_test(_dataloader,model,epoch):\n",
    "    # logging.info(f\"GPU mem before test:{getGPUMem(device)}%\")\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    metric_sacrebleu =  load_metric('sacrebleu')\n",
    "    \n",
    "    # for step, batch in enumerate(tqdm(_dataloader,desc =\"test for epoch\"+str(epoch))):\n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        \n",
    "        test_dataloaderx = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloaderx_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery_attn = Variable(batch[3], requires_grad=False).to(device, non_blocking=False)\n",
    "        target_ids = copy.deepcopy(test_dataloadery)\n",
    "        target_ids[target_ids == tokenizer.pad_token_id] = -100\n",
    "        ls = model(input_ids=test_dataloaderx, attention_mask=test_dataloaderx_attn, labels=target_ids).loss\n",
    "        acc+= ls.item()\n",
    "        counter+= 1\n",
    "        pre = model.generate(test_dataloaderx ,num_beams = 4, early_stopping = True, max_length = max_length, length_penalty =0.6, repetition_penalty = 0.8)\n",
    "        x_decoded = tokenizer.batch_decode(test_dataloaderx,skip_special_tokens=True)\n",
    "        pred_decoded = tokenizer.batch_decode(pre,skip_special_tokens=True)\n",
    "        label_decoded =  tokenizer.batch_decode(test_dataloadery,skip_special_tokens=True)\n",
    "        \n",
    "        pred_str = [x  for x in pred_decoded]\n",
    "        label_str = [[x] for x in label_decoded]\n",
    "        metric_sacrebleu.add_batch(predictions=pred_str, references=label_str)\n",
    "        if  step%100==0:\n",
    "            logging.info(f'x_decoded[:2]:{x_decoded[:2]}')\n",
    "            logging.info(f'pred_decoded[:2]:{pred_decoded[:2]}')\n",
    "            logging.info(f'label_decoded[:2]:{label_decoded[:2]}')\n",
    "            \n",
    "            \n",
    "    sacrebleu_score = metric_sacrebleu.compute()\n",
    "    logging.info('sacreBLEU : %f',sacrebleu_score['score'])#TODO:bleu may be wrong cuz max length\n",
    "    logging.info('test loss : %f',acc/(counter))\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    # logging.info(f\"GPU mem after test:{getGPUMem(device)}%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/17 10:19:18 PM |\t  x_decoded[:2]:['translate German to English: Wenn Sie nach Israel fahren, machen Sie sich keine Sorgen wegen Ihrer schlechten Englischkenntnisse: Ungefähr 30% der Israelis sprechen Russisch.', 'translate German to English: Das fünfte Museum, nämlich das Holocaust-Museum bzw. Yad Vashem in Tel Aviv, erzählt von einem der erschütterndsten Kapitel der Geschichte.']\n",
      "06/17 10:19:18 PM |\t  pred_decoded[:2]:['If you are a Israeli citizen, make an effort to read more Israeli newspapers.', 'The museum consists of a Holocaust-Museum versus a Yad Vashem in Tel Aviv, illustrated by an’s ersatz Kapitel de the Geschichte.']\n",
      "06/17 10:19:18 PM |\t  label_decoded[:2]:[\"When you go to Israel, don't worry about your bad English knowledge: approximately 30% of the country's population speaks Russian.\", 'The fifth one is the Holocaust Museum or Yad Vashem in Tel-Aviv, which tells one of the most dramatic stories in history.']\n",
      "06/17 10:19:52 PM |\t  sacreBLEU : 1.084478\n",
      "06/17 10:19:52 PM |\t  test loss : 4.110823\n",
      "06/17 10:19:52 PM |\t  \n",
      "\n",
      "  ----------------epoch:0----------------\n",
      "06/17 10:19:55 PM |\t  iter:96\t,avgloss:2.680225928624471\n",
      "06/17 10:19:56 PM |\t  iter:192\t,avgloss:2.6674416859944663\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_test(valid_dataloader,model,-1)\n",
    "for epoch in range(10):\n",
    "    iter = [0]\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch}----------------\")\n",
    "    my_train(train_dataloader,model,optimizer,iter)\n",
    "    my_test(valid_dataloader,model,epoch) \n",
    "    torch.save(model,'./model/'+now+'model.pt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d33c3b0ef123e851f98887a8750ca7da758e4ff258891935cfe6ff9c0394387"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
