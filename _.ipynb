{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./checkpoint/checkpoint/D_B.pt').eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l,l_attn  = en(['als die Unbekannten, und doch bekannt; als die Sterbenden, und siehe, wir leben; als die Gezüchtigten, und doch nicht ertötet;', 'als die Traurigen, aber allezeit fröhlich; als die Armen, aber die doch viele reich machen; als die nichts innehaben, und doch alles haben.', 'O ihr Korinther! unser Mund hat sich zu euch aufgetan, unser Herz ist weit.', \"Ihr habt nicht engen Raum in uns; aber eng ist's in euren Herzen.\", 'Ich rede mit euch als mit meinen Kindern, daß ihr euch auch also gegen mich stellet und werdet auch weit.', 'Ziehet nicht am fremden Joch mit den Ungläubigen. Denn was hat die Gerechtigkeit zu schaffen mit der Ungerechtigkeit? Was hat das Licht für Gemeinschaft mit der Finsternis?', 'Wie stimmt Christus mit Belial? Oder was für ein Teil hat der Gläubige mit dem Ungläubigen?', 'Was hat der Tempel Gottes für Gleichheit mit den Götzen? Ihr aber seid der Tempel des lebendigen Gottes; wie denn Gott spricht: \"Ich will unter ihnen wohnen und unter ihnen wandeln und will ihr Gott sein, und sie sollen mein Volk sein.', 'Darum gehet aus von ihnen und sondert euch ab, spricht der HERR, und rührt kein Unreines an, so will ich euch annehmen', 'und euer Vater sein, und ihr sollt meine Söhne und Töchter sein, spricht der allmächtige HERR.\"'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,p_attn = en(['As unknown, und yet well known; as dying, and, behold, we live; as chastened, and not killed;', 'Als traurig, doch alway rejoicing; als arm, obwohl viele reich; als having nichts, und  yet possessing all things.', 'O ye Corinthians, unser Mund ist offen unto euch, unser Herz ist vergrößert.', 'Ye sind nicht straitened in uns, aber ye sind straitened in eurem eigenen Darm.', 'Nun für eine Rekompensation in dem gleichen, (Ich rede wie unto meinen Kindern,) sei ye auch enlarged.', 'Be ye nicht ungleich yoked zusammen mit ungläubigen: denn welche Gemeinschaft hath Rechtschaffenheit mit unrechteousness? und welche Kommunion hath Licht mit Finsternis?', 'Und was concord hat Christus mit Belial? oder welcher Teil hat er hat er, der glaubt mit einem Unidel?', 'Und welche Vereinbarung hat der Tempel Gottes mit idolen? denn ihr sind der Tempel des lebenden Gottes; wie Gott hat gesagt, ich will wohn in ihnen wohnen und trachten in ihnen; und ich werde ihnen meinen Gott sein, und sie werden mein Volk sein.', 'Wherefore come out from among them, and be ye separate, spricht the Lord, and touch not the unclean thing; and I will receive you,', 'Und wird ein Vater unto euch sein, und ihr werdet meine Sonen und Töchter sein, spricht der Herr mighty.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = l.cuda()\n",
    "pred_attn = l.cuda()\n",
    "label = p.cuda()\n",
    "label_attn = p.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2182],\n",
      "        [-0.2139],\n",
      "        [-0.1539],\n",
      "        [-0.3964],\n",
      "        [-0.2606],\n",
      "        [ 0.0069],\n",
      "        [-0.0368],\n",
      "        [-0.2651],\n",
      "        [ 0.0657],\n",
      "        [-0.0921]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.0795],\n",
      "        [ 0.0727],\n",
      "        [-0.4837],\n",
      "        [-0.0967],\n",
      "        [-0.2072],\n",
      "        [-0.3011],\n",
      "        [-0.1369],\n",
      "        [-0.2582],\n",
      "        [ 0.0871],\n",
      "        [-0.0553]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(pred,pred_attn))\n",
    "print(model(label,label_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label,label_attn  = en(['I have some sympathy with the last speaker: sometimes the Committee on Budgetary Control gets so involved with the important work they are doing, that they are unable to see the wood for the trees.', 'I also welcome the presentation on behalf of the Legal Affairs Committee and the Economic and Monetary Affairs Committee which took a balanced approach to the major and indeed legitimate concerns of the Committee on Budgetary Control about fraud.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(pred,pred_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8662])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpu = torch.rand(1,requires_grad=False)\n",
    "inpu\n",
    "inpuB = torch.rand(1,requires_grad=False)\n",
    "inpuB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9243], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.rand(1,requires_grad=True)\n",
    "W\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = W*inpu + inpuB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors does not require grad",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_53824/206016131.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monly_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         inputs, allow_unused, accumulate_grad=False)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: One of the differentiated Tensors does not require grad"
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(outputs=output, inputs=inpu,create_graph=True, retain_graph=True, only_inputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha = torch.rand((5, 1, 1),device=torch.device('cuda:0'))\n",
    "onehot_real = torch.ones(5,4,3)\n",
    "alpha = alpha.expand(onehot_real.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3,2,3)\n",
    "b = torch.ones(3,4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_53824/3434515637.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "torch.sum(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Denn eines ist klar: Selbst wenn wir in der Europäischen Union zu einer ausgezeichneten Regelung kommen, hört der Verkehr nicht an unseren Grenzen, er überschreitet sie.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d([[ 7272,   266,     7,   229,  8330,    10,  8262,  1301,   558,    16,\n",
    "           74,     3, 30604,    29,  3545,   170,   645, 23072,    35,  7510,\n",
    "          425,  4714,     6,     3, 20740,    74,   781, 12319,   311,    46,\n",
    "         1324,    29, 23877,     6,     3,    49,   510,   860,    60,   155,\n",
    "           15,    17,   680,     5,     1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ich möchte daher dem Berichterstatter, Herrn Koch, für seine Arbeit zu dieser Frage danken.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d([[ 1674,  6509,  8301,   340, 19146,    49, 15679,    49,     6,  8816,\n",
    "           29, 16608,     6,   218,  1834,  5512,   170,     3,  1878,  6973,\n",
    "            3, 23020,     5,     1,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_penalty = torch.load('./checkpoint/gradient_penalty.pt')\n",
    "gradient = torch.load('./checkpoint/gradient.pt')\n",
    "interpolates = torch.load('./checkpoint/interpolates.pt')\n",
    "onehot_real = torch.load('./checkpoint/onehot_real.pt')\n",
    "fake = torch.load('./checkpoint/fake.pt')\n",
    "output = torch.load('./checkpoint/output.pt')\n",
    "real = torch.load('./checkpoint/real.pt')\n",
    "fake_ = torch.load('./checkpoint/fake_.pt')\n",
    "alpha = torch.load('./checkpoint/alpha.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6736]],\n",
       "\n",
       "        [[0.1732]],\n",
       "\n",
       "        [[0.0100]],\n",
       "\n",
       "        [[0.3951]]], device='cuda:0')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clip(alpha, min=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I would like, first of all, to thank the rapporteur for his exceptionally accurate and technical work on the report and, secondly, the Commission for the proposal it has submitted.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d([real[-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We believe that this is a question that the Commission should take account of the decisions of this Parliament, especially with regard to the mid-term review of these guidelines.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d([torch.argmax(fake_[-2],-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We believe that this is a question that the Commission should take account of the decisions of this Parliament, especially with regard to the mid-term review of these guidelines.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d([torch.argmax(fake[-2],-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We believe that this is a question that the Commission should take account of the decisions of this Parliament, especially with regard to the mid-term review of these guidelines.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d([torch.argmax(onehot_real[-2],-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "        9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "        9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "        9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "        9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "        9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "        9.9991e-01, 9.9991e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05], device='cuda:0', grad_fn=<MaxBackward0>),\n",
       "indices=tensor([  101,   857,    24,    48,    19,     3,     9,   822,    24,     8,\n",
       "         3527,   225,   240,   905,    13,     8,  3055,    13,    48, 12876,\n",
       "            6,   902,    28,  3553,    12,     8,  2076,    18,  1987,  1132,\n",
       "           13,   175,  5749,     5,     1,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0], device='cuda:0'))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(interpolates[-2],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-(interpolates[:, :,0]>1e-3).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1348],\n",
       "        [0.0587],\n",
       "        [0.0330],\n",
       "        [0.0602]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.1048e-02, 7.9653e-03, 6.0113e-03, 5.5646e-03, 1.9542e-02, 1.3272e-02,\n",
       "        7.2185e-03, 1.3174e-02, 8.0519e-03, 1.2683e-02, 4.3365e-03, 3.6778e-03,\n",
       "        3.5361e-03, 3.4416e-03, 5.2472e-03, 1.0689e-02, 4.5117e-03, 5.3209e-03,\n",
       "        5.8149e-03, 4.3310e-03, 1.0768e-02, 6.1060e-03, 2.5659e-03, 5.4847e-03,\n",
       "        1.3335e-02, 1.1511e-02, 4.0305e-03, 3.6598e-03, 5.6789e-03, 9.4820e-03,\n",
       "        1.5657e-02, 8.7381e-03, 7.0189e-03, 3.3296e-02, 7.5614e-02, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1302e+01, 2.4101e+01,\n",
       "        2.0388e+01, 1.9873e+01, 2.0791e+01, 2.4457e+01, 2.1025e+01, 4.8850e+01,\n",
       "        9.2354e+01, 2.7918e+01, 1.2031e+01, 1.7346e+01, 1.7141e+01, 1.1054e+01,\n",
       "        1.7246e+01, 1.7372e+01, 1.8897e+01, 1.8529e+01, 1.7869e+01, 2.4627e+01,\n",
       "        1.3377e+01, 1.4612e+01, 1.4314e+01, 1.5957e+01, 1.5814e+01, 1.5940e+01,\n",
       "        1.6343e+01, 2.2256e+01, 2.0878e+01, 1.8622e+01, 1.8829e+01, 1.9721e+01,\n",
       "        1.5845e+01, 1.8168e+01, 1.4348e+01, 1.5535e+01, 1.3767e+01, 1.6253e+01,\n",
       "        1.7634e+01, 1.9419e+01, 1.7846e+01, 1.6917e+01, 1.7321e+01, 1.9377e+01,\n",
       "        1.9849e+01, 1.9074e+01, 1.7648e+01, 2.0494e+01, 6.9177e+01, 1.8633e+01,\n",
       "        2.4217e+01, 1.7190e+01, 3.3160e+01, 6.9020e+01, 6.6365e+02, 4.6579e+01,\n",
       "        2.9107e+01, 1.4033e+01, 1.9144e+01, 2.2289e+01, 2.6459e+01, 5.0241e+01,\n",
       "        1.5561e+02, 2.2274e+01, 2.3887e+01, 2.3326e+01, 4.8697e+01, 1.6848e+02,\n",
       "        3.0805e+02, 3.0201e+01, 4.0442e+01, 2.1500e+01, 2.2832e+01, 1.4072e+01,\n",
       "        1.6963e+01, 1.5488e+01, 1.9049e+01, 1.5402e+01, 1.6874e+01, 1.9536e+01,\n",
       "        1.6996e+01], device='cuda:0', grad_fn=<MaxBackward0>),\n",
       "indices=tensor([10273, 31245, 29752, 31930,  9109, 29624, 28294, 22830, 30515, 24889,\n",
       "        13959, 31645, 29692, 16907, 28294, 25355, 20654, 30513,     0,  3527,\n",
       "        11001, 28533, 11433, 26939,     0, 31245, 20654, 24596, 24287, 31245,\n",
       "            0,     0, 29429, 23315, 30759,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0, 13959,  9686, 25355, 18572,\n",
       "        19859, 25355, 29752, 28081, 23782, 27205, 25562, 28800, 25355, 25355,\n",
       "        23054, 29752, 28125, 26478, 27205, 27000, 18977, 27000, 18977, 26478,\n",
       "        25355, 20911, 28221, 27205, 27136, 27205, 27205, 29351, 27416, 25355,\n",
       "        26944, 26478, 26944, 27205, 25355, 25355, 30299, 30392, 30392, 19139,\n",
       "        25562, 27000, 28237, 24287, 31507, 28852, 29752, 25355, 29752, 29752,\n",
       "        31507, 31507, 29752, 28182, 29752, 29752, 29752, 29752, 31507, 30759,\n",
       "        29752, 28182, 29752, 31507, 31894, 29752, 29752, 29752, 10658, 12810,\n",
       "        24777, 31502, 30121, 21603,  4697, 31526, 28085], device='cuda:0'))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(gradient[-2],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(40464.6172, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((gradient.norm(2, dim=1) - 1) ** 2).mean(-1)[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3, 2, 1]]), tensor([[1, 1, 1]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en(['周'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7102,    1]]), tensor([[1, 1]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en(['hi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   3,   29,   23,   26,  210,   29, 6146,   29,    9,   23,  210,   26,\n",
       "          1024,   23,  210,  107,   26,    9,    1]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en(['nidwnaidnaiwdhaiwhda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration de-en-lang1=de,lang2=en\n",
      "Reusing dataset bible_para (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\bible_para\\de-en-lang1=de,lang2=en\\0.0.0\\b6cc20bcbfb0299beeba1dcc80a8420b975938ca0eef75b3ed30b50df7d950b1)\n",
      "100%|██████████| 1/1 [00:00<00:00, 100.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': (62195, 2)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset,load_metric\n",
    "load_dataset(\"bible_para\", lang1=\"de\", lang2=\"en\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d33c3b0ef123e851f98887a8750ca7da758e4ff258891935cfe6ff9c0394387"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
