{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./checkpoint/checkpoint/D_A.pt').eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,p_attn  = en(['As unknown, and yet well known; as dying, and, behold, we live; as chastened, and not killed;', 'As sorrowful, yet alway rejoicing; as poor, yet making many rich; as having nothing, and yet possessing all things.', 'O ye Corinthians, our mouth is open unto you, our heart is enlarged.', 'Ye are not straitened in us, but ye are straitened in your own bowels.', 'Now for a recompence in the same, (I speak as unto my children,) be ye also enlarged.', 'Be ye not unequally yoked together with unbelievers: for what fellowship hath righteousness with unrighteousness? and what communion hath light with darkness?', 'And what concord hath Christ with Belial? or what part hath he that believeth with an infidel?', 'And what agreement hath the temple of God with idols? for ye are the temple of the living God; as God hath said, I will dwell in them, and walk in them; and I will be their God, and they shall be my people.', 'Wherefore come out from among them, and be ye separate, saith the Lord, and touch not the unclean thing; and I will receive you,', 'And will be a Father unto you, and ye shall be my sons and daughters, saith the Lord Almighty.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "l,l_attn  = en(['as the unknown, and yet known; as the dead, and siehe, we live; as the gods, and not ert;', 'as the Traurigens, but all time fröhlich; as the poor, but who make many rich; as the who do nothing, and yet all have.', 'O their corinther! our mouth has risen to you, our heart is far.', \"Your habt not engen space in us; but eng is's in euren heart.\", 'I talk with you as with my children, that your also against me, and will also go far.', 'Ziehet not am fremden Joch with the ungläubigen. For what has the justice to create with the injustice? What has the light for the Community with the Finsternis?', 'How does Christ obey with Belial? Or what for a part has the God with the Ungläubigen?', 'What has the temple of God for equality with the gods? Your but the temple of the living God; as God says: \"I will living under them and under them... and will their God, and they will my people.', 'Therefore het from them and sonder you ab, he speaks theHER, and does not touch an unreine, so I will take you.', 'and your father be, and her shall be my son and techter, spricht the all-mighty HERR.\"'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = p.cuda()\n",
    "pred_attn = p_attn.cuda()\n",
    "label =l.cuda()\n",
    "label_attn = l_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2123],\n",
      "        [ 0.0591],\n",
      "        [ 0.1070],\n",
      "        [ 0.2407],\n",
      "        [ 0.0479],\n",
      "        [ 0.0761],\n",
      "        [-0.0413],\n",
      "        [-0.1143],\n",
      "        [-0.0653],\n",
      "        [ 0.0745]], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor([[ 0.0900],\n",
      "        [-0.1230],\n",
      "        [ 0.0365],\n",
      "        [ 0.1014],\n",
      "        [-0.0792],\n",
      "        [ 0.0841],\n",
      "        [ 0.0655],\n",
      "        [-0.2247],\n",
      "        [-0.0354],\n",
      "        [ 0.0821]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = (model(label,label_attn)-model(pred,pred_attn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label,label_attn  = en(['I have some sympathy with the last speaker: sometimes the Committee on Budgetary Control gets so involved with the important work they are doing, that they are unable to see the wood for the trees.', 'I also welcome the presentation on behalf of the Legal Affairs Committee and the Economic and Monetary Affairs Committee which took a balanced approach to the major and indeed legitimate concerns of the Committee on Budgetary Control about fraud.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(pred,pred_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8662])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpu = torch.rand(1,requires_grad=False)\n",
    "inpu\n",
    "inpuB = torch.rand(1,requires_grad=False)\n",
    "inpuB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9243], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.rand(1,requires_grad=True)\n",
    "W\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = W*inpu + inpuB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors does not require grad",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_53824/206016131.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monly_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         inputs, allow_unused, accumulate_grad=False)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: One of the differentiated Tensors does not require grad"
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(outputs=output, inputs=inpu,create_graph=True, retain_graph=True, only_inputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha = torch.rand((5, 1, 1),device=torch.device('cuda:0'))\n",
    "onehot_real = torch.ones(5,4,3)\n",
    "alpha = alpha.expand(onehot_real.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3,2,3)\n",
    "b = torch.ones(3,4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_53824/3434515637.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "torch.sum(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Denn eines ist klar: Selbst wenn wir in der Europäischen Union zu einer ausgezeichneten Regelung kommen, hört der Verkehr nicht an unseren Grenzen, er überschreitet sie.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d([[ 7272,   266,     7,   229,  8330,    10,  8262,  1301,   558,    16,\n",
    "           74,     3, 30604,    29,  3545,   170,   645, 23072,    35,  7510,\n",
    "          425,  4714,     6,     3, 20740,    74,   781, 12319,   311,    46,\n",
    "         1324,    29, 23877,     6,     3,    49,   510,   860,    60,   155,\n",
    "           15,    17,   680,     5,     1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ich möchte daher dem Berichterstatter, Herrn Koch, für seine Arbeit zu dieser Frage danken.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d([[ 1674,  6509,  8301,   340, 19146,    49, 15679,    49,     6,  8816,\n",
    "           29, 16608,     6,   218,  1834,  5512,   170,     3,  1878,  6973,\n",
    "            3, 23020,     5,     1,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_penalty = torch.load('./checkpoint/gradient_penalty.pt')\n",
    "gradient = torch.load('./checkpoint/gradient.pt')\n",
    "interpolates = torch.load('./checkpoint/interpolates.pt')\n",
    "onehot_real = torch.load('./checkpoint/onehot_real.pt')\n",
    "fake = torch.load('./checkpoint/fake.pt')\n",
    "output = torch.load('./checkpoint/output.pt')\n",
    "real = torch.load('./checkpoint/real.pt')\n",
    "fake_ = torch.load('./checkpoint/fake_.pt')\n",
    "alpha = torch.load('./checkpoint/alpha.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6736]],\n",
       "\n",
       "        [[0.1732]],\n",
       "\n",
       "        [[0.0100]],\n",
       "\n",
       "        [[0.3951]]], device='cuda:0')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clip(alpha, min=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I would like, first of all, to thank the rapporteur for his exceptionally accurate and technical work on the report and, secondly, the Commission for the proposal it has submitted.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d([real[-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We believe that this is a question that the Commission should take account of the decisions of this Parliament, especially with regard to the mid-term review of these guidelines.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d([torch.argmax(fake_[-2],-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We believe that this is a question that the Commission should take account of the decisions of this Parliament, especially with regard to the mid-term review of these guidelines.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d([torch.argmax(fake[-2],-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We believe that this is a question that the Commission should take account of the decisions of this Parliament, especially with regard to the mid-term review of these guidelines.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d([torch.argmax(onehot_real[-2],-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "        9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "        9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "        9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "        9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "        9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01, 9.9991e-01,\n",
       "        9.9991e-01, 9.9991e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05, 9.2154e-05,\n",
       "        9.2154e-05], device='cuda:0', grad_fn=<MaxBackward0>),\n",
       "indices=tensor([  101,   857,    24,    48,    19,     3,     9,   822,    24,     8,\n",
       "         3527,   225,   240,   905,    13,     8,  3055,    13,    48, 12876,\n",
       "            6,   902,    28,  3553,    12,     8,  2076,    18,  1987,  1132,\n",
       "           13,   175,  5749,     5,     1,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0], device='cuda:0'))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(interpolates[-2],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-(interpolates[:, :,0]>1e-3).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1348],\n",
       "        [0.0587],\n",
       "        [0.0330],\n",
       "        [0.0602]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.1048e-02, 7.9653e-03, 6.0113e-03, 5.5646e-03, 1.9542e-02, 1.3272e-02,\n",
       "        7.2185e-03, 1.3174e-02, 8.0519e-03, 1.2683e-02, 4.3365e-03, 3.6778e-03,\n",
       "        3.5361e-03, 3.4416e-03, 5.2472e-03, 1.0689e-02, 4.5117e-03, 5.3209e-03,\n",
       "        5.8149e-03, 4.3310e-03, 1.0768e-02, 6.1060e-03, 2.5659e-03, 5.4847e-03,\n",
       "        1.3335e-02, 1.1511e-02, 4.0305e-03, 3.6598e-03, 5.6789e-03, 9.4820e-03,\n",
       "        1.5657e-02, 8.7381e-03, 7.0189e-03, 3.3296e-02, 7.5614e-02, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1302e+01, 2.4101e+01,\n",
       "        2.0388e+01, 1.9873e+01, 2.0791e+01, 2.4457e+01, 2.1025e+01, 4.8850e+01,\n",
       "        9.2354e+01, 2.7918e+01, 1.2031e+01, 1.7346e+01, 1.7141e+01, 1.1054e+01,\n",
       "        1.7246e+01, 1.7372e+01, 1.8897e+01, 1.8529e+01, 1.7869e+01, 2.4627e+01,\n",
       "        1.3377e+01, 1.4612e+01, 1.4314e+01, 1.5957e+01, 1.5814e+01, 1.5940e+01,\n",
       "        1.6343e+01, 2.2256e+01, 2.0878e+01, 1.8622e+01, 1.8829e+01, 1.9721e+01,\n",
       "        1.5845e+01, 1.8168e+01, 1.4348e+01, 1.5535e+01, 1.3767e+01, 1.6253e+01,\n",
       "        1.7634e+01, 1.9419e+01, 1.7846e+01, 1.6917e+01, 1.7321e+01, 1.9377e+01,\n",
       "        1.9849e+01, 1.9074e+01, 1.7648e+01, 2.0494e+01, 6.9177e+01, 1.8633e+01,\n",
       "        2.4217e+01, 1.7190e+01, 3.3160e+01, 6.9020e+01, 6.6365e+02, 4.6579e+01,\n",
       "        2.9107e+01, 1.4033e+01, 1.9144e+01, 2.2289e+01, 2.6459e+01, 5.0241e+01,\n",
       "        1.5561e+02, 2.2274e+01, 2.3887e+01, 2.3326e+01, 4.8697e+01, 1.6848e+02,\n",
       "        3.0805e+02, 3.0201e+01, 4.0442e+01, 2.1500e+01, 2.2832e+01, 1.4072e+01,\n",
       "        1.6963e+01, 1.5488e+01, 1.9049e+01, 1.5402e+01, 1.6874e+01, 1.9536e+01,\n",
       "        1.6996e+01], device='cuda:0', grad_fn=<MaxBackward0>),\n",
       "indices=tensor([10273, 31245, 29752, 31930,  9109, 29624, 28294, 22830, 30515, 24889,\n",
       "        13959, 31645, 29692, 16907, 28294, 25355, 20654, 30513,     0,  3527,\n",
       "        11001, 28533, 11433, 26939,     0, 31245, 20654, 24596, 24287, 31245,\n",
       "            0,     0, 29429, 23315, 30759,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0, 13959,  9686, 25355, 18572,\n",
       "        19859, 25355, 29752, 28081, 23782, 27205, 25562, 28800, 25355, 25355,\n",
       "        23054, 29752, 28125, 26478, 27205, 27000, 18977, 27000, 18977, 26478,\n",
       "        25355, 20911, 28221, 27205, 27136, 27205, 27205, 29351, 27416, 25355,\n",
       "        26944, 26478, 26944, 27205, 25355, 25355, 30299, 30392, 30392, 19139,\n",
       "        25562, 27000, 28237, 24287, 31507, 28852, 29752, 25355, 29752, 29752,\n",
       "        31507, 31507, 29752, 28182, 29752, 29752, 29752, 29752, 31507, 30759,\n",
       "        29752, 28182, 29752, 31507, 31894, 29752, 29752, 29752, 10658, 12810,\n",
       "        24777, 31502, 30121, 21603,  4697, 31526, 28085], device='cuda:0'))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(gradient[-2],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(40464.6172, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((gradient.norm(2, dim=1) - 1) ** 2).mean(-1)[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3, 2, 1]]), tensor([[1, 1, 1]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en(['周'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7102,    1]]), tensor([[1, 1]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en(['hi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   3,   29,   23,   26,  210,   29, 6146,   29,    9,   23,  210,   26,\n",
       "          1024,   23,  210,  107,   26,    9,    1]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en(['nidwnaidnaiwdhaiwhda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration de-en-lang1=de,lang2=en\n",
      "Reusing dataset bible_para (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\bible_para\\de-en-lang1=de,lang2=en\\0.0.0\\b6cc20bcbfb0299beeba1dcc80a8420b975938ca0eef75b3ed30b50df7d950b1)\n",
      "100%|██████████| 1/1 [00:00<00:00, 100.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': (62195, 2)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset,load_metric\n",
    "load_dataset(\"bible_para\", lang1=\"de\", lang2=\"en\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d33c3b0ef123e851f98887a8750ca7da758e4ff258891935cfe6ff9c0394387"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
