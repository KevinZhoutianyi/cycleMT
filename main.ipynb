{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "from parameter import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.test_iter 500\n",
      "args.rep_iter 100\n"
     ]
    }
   ],
   "source": [
    "if(True):\n",
    "    parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "    parser.add_argument('--valid_num_points', type=int,             default = 300, help='validation data number')\n",
    "    parser.add_argument('--train_num_points', type=int,             default = 1000, help='train data number')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int,                   default=4,     help='Batch size')\n",
    "    parser.add_argument('--max_length', type=int,                   default=128,     help='max_length')\n",
    "\n",
    "    parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "    parser.add_argument('--G_AB_model_name', type=str,              default='t5-small',      help='model_name')\n",
    "    parser.add_argument('--G_BA_model_name', type=str,              default='Onlydrinkwater/T5-small-de-en',      help='model_name')\n",
    "    parser.add_argument('--D_A_model_name', type=str,               default='Onlydrinkwater/T5-small-de-en',      help='model_name')\n",
    "    parser.add_argument('--D_B_model_name', type=str,               default='t5-small',      help='model_name')\n",
    "    parser.add_argument('--exp_name', type=str,                     default='CYCLE!',      help='experiment name')\n",
    "    parser.add_argument('--rep_iter', type=int,                     default=100,      help='report times for 1 epoch')\n",
    "    parser.add_argument('--test_iter', type=int,                    default=500,      help='report times for 1 epoch')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int,                       default=50,     help='num of training epochs')\n",
    "\n",
    "    parser.add_argument('--G_lr', type=float,                       default=5e-6,   help='learning rate for G')\n",
    "    parser.add_argument('--G_weight_decay', type=float,             default=1e-3,   help='learning de for G')\n",
    "    parser.add_argument('--G_gamma', type=float,                    default=1,    help='lr*gamma after each test')\n",
    "    parser.add_argument('--G_grad_clip', type=float,                default=1,   help='grad_clip')\n",
    "    parser.add_argument('--D_lr', type=float,                       default=5e-5,   help='learning rate for D')\n",
    "    parser.add_argument('--D_weight_decay', type=float,             default=1e-3,   help='learning de for D')\n",
    "    parser.add_argument('--D_gamma', type=float,                    default=1,    help='lr*gamma after each test')\n",
    "    parser.add_argument('--D_grad_clip', type=float,                default=1e-2,   help='grad_clip')\n",
    "    parser.add_argument('--lambda_identity', type=float,            default=0.5,   help='')\n",
    "    parser.add_argument('--lambda_A', type=float,                   default=1,   help='')\n",
    "    parser.add_argument('--lambda_B', type=float,                   default=1,   help='')\n",
    "    parser.add_argument('--lambda_once', type=float,                default=0,   help='')\n",
    "    parser.add_argument('--lambda_GP', type=float,                  default=10,   help='WGANGP pentalty')\n",
    "    parser.add_argument('--DperG', type=int,                        default=2,    help='n_critc')\n",
    "    parser.add_argument('--GperD', type=int,                        default=2,    help='n_g')\n",
    "    parser.add_argument('--smoothing', type=float,                  default=0.5,    help='labelsmoothing')\n",
    "\n",
    "    parser.add_argument('--load_D', type=int,                       default=0,      help='load pretrained D')\n",
    "    parser.add_argument('--load_G', type=int,                       default=0,      help='load pretrained D')\n",
    "    parser.add_argument('--num_workers', type=int,                  default=0,      help='num_workers')\n",
    "    parser.add_argument('--valid_begin', type=int,                  default=0,      help='whether valid before train')\n",
    "    parser.add_argument('--train_G', type=int,                      default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--train_D', type=int,                      default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--D_pretrain_iter', type=int,              default=0,      help='whether valid before train')\n",
    "    parser.add_argument('--poolsize', type=int,                     default=1,      help='whether valid before train')\n",
    "\n",
    "\n",
    "    args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "    args.test_iter = args.test_iter//args.batch_size * args.batch_size\n",
    "    args.rep_iter = args.rep_iter//args.batch_size * args.batch_size\n",
    "    print('args.test_iter',args.test_iter)\n",
    "    print('args.rep_iter',args.rep_iter)#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\GitCode\\cycleMT\\wandb\\run-20220518_101541-2iy8d4vp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/cycleWMT/runs/2iy8d4vp\" target=\"_blank\">CYCLE!</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/cycleWMT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/cycleWMT/runs/2iy8d4vp?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2beffbd7a60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_API_KEY']='a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME']=args.exp_name\n",
    "wandb.init(project=\"cycleWMT\",config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/18 10:15:48 AM |\t  Namespace(D_A_model_name='Onlydrinkwater/T5-small-de-en', D_B_model_name='t5-small', D_gamma=1, D_grad_clip=0.01, D_lr=5e-05, D_pretrain_iter=0, D_weight_decay=0.001, DperG=2, G_AB_model_name='t5-small', G_BA_model_name='Onlydrinkwater/T5-small-de-en', G_gamma=1, G_grad_clip=1, G_lr=5e-06, G_weight_decay=0.001, GperD=2, batch_size=4, epochs=50, exp_name='CYCLE!', gpu=0, lambda_A=1, lambda_B=1, lambda_GP=10, lambda_identity=0.5, lambda_once=0, load_D=0, load_G=0, max_length=128, num_workers=0, poolsize=1, rep_iter=100, smoothing=0.5, test_iter=500, train_D=1, train_G=1, train_num_points=1000, valid_begin=0, valid_num_points=300)\n"
     ]
    }
   ],
   "source": [
    "#logging file\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'),'w',encoding = \"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "logging.info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/18 10:15:53 AM |\t  Gmodelsize:60.506624MB\n",
      "05/18 10:15:53 AM |\t  Dmodelsize:60.506624MB\n"
     ]
    }
   ],
   "source": [
    "GABmodelname = args.G_AB_model_name\n",
    "GBAmodelname = args.G_BA_model_name\n",
    "DAmodelname = args.D_A_model_name\n",
    "DBmodelname = args.D_B_model_name\n",
    "GABpretrained  =  AutoModelForSeq2SeqLM.from_pretrained(GABmodelname)\n",
    "GBApretrained  =  AutoModelForSeq2SeqLM.from_pretrained(GBAmodelname)\n",
    "DApretrained  =  AutoModelForSeq2SeqLM.from_pretrained(DAmodelname)\n",
    "DBpretrained  =  AutoModelForSeq2SeqLM.from_pretrained(DBmodelname)\n",
    "logging.info(f'Gmodelsize:{count_parameters_in_MB(GABpretrained)}MB')\n",
    "logging.info(f'Dmodelsize:{count_parameters_in_MB(DApretrained)}MB')\n",
    "tokenizer = AutoTokenizer.from_pretrained(GABmodelname)\n",
    "# tokenizerBA = AutoTokenizer.from_pretrained(GBAmodelname)#its the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/18 10:15:57 AM |\t  Reusing dataset wmt16 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt16\\de-en\\1.0.0\\0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 28.38it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('wmt16',language+'-en')#load_dataset(\"bible_para\", lang1=\"de\", lang2=\"en\")\n",
    "train = dataset['train']['translation'][:args.train_num_points]\n",
    "valid = dataset['validation']['translation'][-args.valid_num_points:]#TODO:\n",
    "\n",
    "\n",
    "train_data = get_Dataset_chaos(train, tokenizer,max_length=args.max_length)\n",
    "train_dataloader = DataLoader(train_data, sampler= RandomSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "valid_data = get_Dataset(valid, tokenizer,max_length=args.max_length)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycleGAN = CycleGAN(args,GABpretrained,GBApretrained,DApretrained,DBpretrained,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/18 10:16:35 AM |\t  \n",
      "\n",
      "  ----------------epoch:0----------------\n",
      "05/18 10:16:35 AM |\t  total iter:[0] \t G_lr:5e-06 \t DA_lr:5e-05 \t DB_lr:5e-05\n",
      "05/18 10:18:16 AM |\t  {'GB_cycle_meter': 10.263800621032715, 'GA_cycle_meter': 10.229955037434896, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.0007165225595235824, 'DB_meter': -0.015991679951548578, 'DA_GP_meter': 0.027418245747685432, 'DB_GP_meter': 0.06045604407787323}\n",
      "05/18 10:18:16 AM |\t  9.6%\n",
      "05/18 10:19:59 AM |\t  {'GB_cycle_meter': 10.264096339543661, 'GA_cycle_meter': 10.256047328313192, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.0064862571284174915, 'DB_meter': 0.017953311428427696, 'DA_GP_meter': 0.01712181456387043, 'DB_GP_meter': 0.029439268410205843}\n",
      "05/18 10:19:59 AM |\t  19.6%\n",
      "05/18 10:21:33 AM |\t  {'GB_cycle_meter': 10.207444777855507, 'GA_cycle_meter': 10.250503686758188, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.0019056402146816255, 'DB_meter': 0.002129863128066063, 'DA_GP_meter': 0.014543219394981861, 'DB_GP_meter': 0.019417285583913327}\n",
      "05/18 10:21:33 AM |\t  29.599999999999998%\n",
      "05/18 10:23:23 AM |\t  {'GB_cycle_meter': 10.284404118855795, 'GA_cycle_meter': 10.221627871195475, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.009625081941485404, 'DB_meter': 0.001206848993897438, 'DA_GP_meter': 0.010983968786895275, 'DB_GP_meter': 0.010926266573369503}\n",
      "05/18 10:23:23 AM |\t  39.6%\n",
      "05/18 10:25:12 AM |\t  {'GB_cycle_meter': 10.264009622427134, 'GA_cycle_meter': 10.253041634192833, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.007661733441054821, 'DB_meter': -0.004705551750957966, 'DA_GP_meter': 0.009433247353881598, 'DB_GP_meter': 0.008905746098607779}\n",
      "05/18 10:25:12 AM |\t  49.6%\n",
      "05/18 10:25:20 AM |\t  DB_a_: -0.12,  -0.08,  -0.05,  -0.04,  \n",
      "05/18 10:25:20 AM |\t  DB_pred_dis: -0.10,  -0.06,  -0.04,  -0.03,  \n",
      "05/18 10:25:20 AM |\t  DA_b: 0.072,  0.090,  0.084,  0.086,  \n",
      "05/18 10:25:20 AM |\t  DA_pred_dis: 0.071,  0.100,  0.098,  0.087,  \n",
      "05/18 10:25:20 AM |\t  GABloss:\t5.150293827056885\n",
      "05/18 10:25:20 AM |\t  GBAloss:\t10.569684028625488\n",
      "05/18 10:25:20 AM |\t  a_decoded[:2]:['He learned that lesson the hard way.', 'A book he purchased was back-ordered for four weeks, and he ended up paying full price at the college bookstore.', \"Why the Guardians of the Galaxy couldn't save the box office\", \"Sylvester Stallone's The Expendables 3 has made back less than $30 million of its $90 million budget in the US, while Sin City: A Dame to Kill For has made back only $12 million of its $70 million budget.\"]\n",
      "05/18 10:25:20 AM |\t  pred_b_decoded[:2]:['He has learned this lesson on the hard road.', 'He bought a book that had four weeks of delivery and finally paid the full price in the College bookstore.', \"Why was a 'Guardians of the Galaxy' not a bomber?\", \"The 'Expendable 3' by Sylvester Stallone played less than USD 30 million in his 90 million budget in the United States, while 'Sin City: A Lady To Kill For' played only USD 12 million in his 70 million budget.\"]\n",
      "05/18 10:25:20 AM |\t  b_decoded[:2]:['Diese Lektion lernte er auf die harte Tour.', 'Er kaufte ein Buch, dass vier Wochen Lieferzeit hatte und zahlte schließlich den vollen Preis im College-Buchladen.', 'Warum \"Guardians of the Galaxy\" kein Kassenschlager wurde', '\"The Expendables 3\" von Sylvester Stallone spielte weniger als 30 Millionen Dollar seines 90 Millionen Dollar Budgets in den USA ein, während \"Sin City: A Dame to Kill For\" lediglich 12 Millionen Dollar seines Budgets von 70 Millionen Dollar einspielte.']\n",
      "05/18 10:25:20 AM |\t  pred_a_decoded[:2]:['Er hat diese Lektion auf harte Weise gelernt.', 'Während der letzten vier Wochen wurde ein Buch gekauft, und er zahlte schließlich den vollen Preis in der College Buchhandlung.', 'Warum die Guardians of the Galaxy die Box Office nicht retten konnten', 'Die Ausgaben 3 von Sylvester Stallone hat weniger als 30 Millionen Dollar aus ihrem 90 Millionen Dollar Budget in den USA zurückerstattet, während Sin City: A Dame to Kill For lediglich 12 Millionen Dollar aus ihrem 70 Millionen Dollar Budget zurückerstattet hat.']\n",
      "05/18 10:27:16 AM |\t  computing score...\n",
      "05/18 10:27:17 AM |\t  G_AB GAB sacreBLEU : 25.460437\n",
      "05/18 10:27:17 AM |\t  G_BA GBA sacreBLEU : 21.345087\n",
      "05/18 10:27:17 AM |\t  G_AB GAB test loss : 5.473073\n",
      "05/18 10:27:17 AM |\t  G_BA GBA test loss : 9.208864\n",
      "05/18 10:27:17 AM |\t  D_A DA test loss : -0.001125\n",
      "05/18 10:27:17 AM |\t  D_B DB test loss : -0.004007\n",
      "05/18 10:27:17 AM |\t  D_A DA test accuracy : 0.513333\n",
      "05/18 10:27:17 AM |\t  D_B DB test accuracy : 0.590000\n",
      "05/18 10:28:59 AM |\t  {'GB_cycle_meter': 10.2313551902771, 'GA_cycle_meter': 10.220928430557251, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.0023573799431324005, 'DB_meter': -0.0021712439507246017, 'DA_GP_meter': 0.0034427162120118735, 'DB_GP_meter': 0.002044307868927717}\n",
      "05/18 10:28:59 AM |\t  59.599999999999994%\n",
      "05/18 10:30:37 AM |\t  {'GB_cycle_meter': 10.203895788926344, 'GA_cycle_meter': 10.187931207510141, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.010308597944676876, 'DB_meter': -0.0053050417453050615, 'DA_GP_meter': 0.003257913365960121, 'DB_GP_meter': 0.0020462638093158604}\n",
      "05/18 10:30:37 AM |\t  69.6%\n",
      "05/18 10:32:15 AM |\t  {'GB_cycle_meter': 10.227960427602133, 'GA_cycle_meter': 10.156319777170816, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.0036951845698058607, 'DB_meter': 0.00535489471629262, 'DA_GP_meter': 0.0029878190625458956, 'DB_GP_meter': 0.0016447083745151758}\n",
      "05/18 10:32:15 AM |\t  79.60000000000001%\n",
      "05/18 10:33:54 AM |\t  {'GB_cycle_meter': 10.202399694002592, 'GA_cycle_meter': 10.156035276559683, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.0026886019483208655, 'DB_meter': -0.0006052981317043305, 'DA_GP_meter': 0.0032417074032127856, 'DB_GP_meter': 0.0014013212244026363}\n",
      "05/18 10:33:54 AM |\t  89.60000000000001%\n",
      "05/18 10:35:35 AM |\t  {'GB_cycle_meter': 10.184456825256348, 'GA_cycle_meter': 10.180290381113688, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.0028410755656659605, 'DB_meter': 0.004537300392985344, 'DA_GP_meter': 0.0027769058244302867, 'DB_GP_meter': 0.0011183803901076317}\n",
      "05/18 10:35:35 AM |\t  99.6%\n",
      "05/18 10:35:43 AM |\t  DB_a_: -0.07,  -0.03,  -0.03,  -0.03,  \n",
      "05/18 10:35:43 AM |\t  DB_pred_dis: -0.06,  -0.03,  -0.03,  -0.03,  \n",
      "05/18 10:35:43 AM |\t  DA_b: 0.047,  0.063,  0.062,  0.056,  \n",
      "05/18 10:35:43 AM |\t  DA_pred_dis: 0.048,  0.061,  0.057,  0.068,  \n",
      "05/18 10:35:43 AM |\t  GABloss:\t5.786654472351074\n",
      "05/18 10:35:43 AM |\t  GBAloss:\t11.032490730285645\n",
      "05/18 10:35:43 AM |\t  a_decoded[:2]:['He learned that lesson the hard way.', 'A book he purchased was back-ordered for four weeks, and he ended up paying full price at the college bookstore.', \"Why the Guardians of the Galaxy couldn't save the box office\", \"Sylvester Stallone's The Expendables 3 has made back less than $30 million of its $90 million budget in the US, while Sin City: A Dame to Kill For has made back only $12 million of its $70 million budget.\"]\n",
      "05/18 10:35:43 AM |\t  pred_b_decoded[:2]:['He has learned this lesson on the hard road.', 'He bought a book that had four weeks of delivery and finally paid the full price in the College bookstore.', \"Why 'Guardians of the Galaxy' was not a bomber?\", \"The 'Expendable 3' by Sylvester Stallone played less than USD 30 million in his 90 million budget in the United States, while 'Sin City: A Lady To Kill For' played only USD 12 million in his 70 million budget.\"]\n",
      "05/18 10:35:43 AM |\t  b_decoded[:2]:['Diese Lektion lernte er auf die harte Tour.', 'Er kaufte ein Buch, dass vier Wochen Lieferzeit hatte und zahlte schließlich den vollen Preis im College-Buchladen.', 'Warum \"Guardians of the Galaxy\" kein Kassenschlager wurde', '\"The Expendables 3\" von Sylvester Stallone spielte weniger als 30 Millionen Dollar seines 90 Millionen Dollar Budgets in den USA ein, während \"Sin City: A Dame to Kill For\" lediglich 12 Millionen Dollar seines Budgets von 70 Millionen Dollar einspielte.']\n",
      "05/18 10:35:43 AM |\t  pred_a_decoded[:2]:['Er hat diese Lektion auf harte Weise gelernt.', 'Er hat ein Buch gekauft, das vier Wochen lang zurückbestellt wurde, und schließlich zahlte er den vollen Preis in der College-Buchhandlung.', 'Warum die Guardians of the Galaxy die Box Office nicht retten konnten', 'Die Ausgaben 3 von Sylvester Stallone hat weniger als 30 Millionen Dollar aus ihrem 90 Millionen Dollar Budget in den USA zurückerstattet, während Sin City: A Dame to Kill For nur 12 Millionen Dollar aus ihrem 70 Millionen Dollar Budget zurückerstattet hat.']\n",
      "05/18 10:37:48 AM |\t  computing score...\n",
      "05/18 10:37:48 AM |\t  G_AB GAB sacreBLEU : 25.830033\n",
      "05/18 10:37:48 AM |\t  G_BA GBA sacreBLEU : 21.811813\n",
      "05/18 10:37:48 AM |\t  G_AB GAB test loss : 5.836922\n",
      "05/18 10:37:48 AM |\t  G_BA GBA test loss : 9.725415\n",
      "05/18 10:37:48 AM |\t  D_A DA test loss : -0.008227\n",
      "05/18 10:37:48 AM |\t  D_B DB test loss : -0.004908\n",
      "05/18 10:37:48 AM |\t  D_A DA test accuracy : 0.660000\n",
      "05/18 10:37:48 AM |\t  D_B DB test accuracy : 0.676667\n",
      "05/18 10:37:48 AM |\t  \n",
      "\n",
      "  ----------------epoch:1----------------\n",
      "05/18 10:37:48 AM |\t  total iter:[1000] \t G_lr:5e-06 \t DA_lr:5e-05 \t DB_lr:5e-05\n",
      "05/18 10:39:38 AM |\t  {'GB_cycle_meter': 10.261021773020426, 'GA_cycle_meter': 10.200304826100668, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.002415177021175623, 'DB_meter': -0.011378353908658028, 'DA_GP_meter': 0.006799147902056575, 'DB_GP_meter': 0.0071562066162005064}\n",
      "05/18 10:39:38 AM |\t  9.6%\n",
      "05/18 10:41:26 AM |\t  {'GB_cycle_meter': 10.255535125732422, 'GA_cycle_meter': 10.18209687868754, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.014170524775981902, 'DB_meter': -0.004892501104623079, 'DA_GP_meter': 0.005426974818110466, 'DB_GP_meter': 0.0046704749017953875}\n",
      "05/18 10:41:26 AM |\t  19.6%\n",
      "05/18 10:43:17 AM |\t  {'GB_cycle_meter': 10.263704666724571, 'GA_cycle_meter': 10.212818219111515, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.0030527190491557122, 'DB_meter': -0.004149677827954292, 'DA_GP_meter': 0.008309505181387067, 'DB_GP_meter': 0.003983673639595509}\n",
      "05/18 10:43:17 AM |\t  29.599999999999998%\n"
     ]
    }
   ],
   "source": [
    "if(args.valid_begin==1):\n",
    "    my_test(valid_dataloader,cycleGAN,tokenizer,logging,wandb)\n",
    "total_iter = [0]  \n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch}----------------\")\n",
    "    my_train(train_dataloader,cycleGAN,total_iter,args,logging,valid_dataloader,tokenizer,wandb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d33c3b0ef123e851f98887a8750ca7da758e4ff258891935cfe6ff9c0394387"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
