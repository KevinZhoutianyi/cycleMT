{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "from parameter import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.test_iter 500\n",
      "args.rep_iter 100\n"
     ]
    }
   ],
   "source": [
    "if(True):\n",
    "    parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "    parser.add_argument('--valid_num_points', type=int,             default = 100, help='validation data number')\n",
    "    parser.add_argument('--train_num_points', type=int,             default = 500, help='train data number')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int,                   default=4,     help='Batch size')\n",
    "    parser.add_argument('--max_length', type=int,                   default=128,     help='max_length')\n",
    "\n",
    "    parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "    parser.add_argument('--G_AB_model_name', type=str,              default='t5-small',      help='model_name')\n",
    "    parser.add_argument('--G_BA_model_name', type=str,              default='Onlydrinkwater/T5-small-de-en',      help='model_name')\n",
    "    parser.add_argument('--D_A_model_name', type=str,               default='t5-small',      help='model_name')\n",
    "    parser.add_argument('--D_B_model_name', type=str,               default='Onlydrinkwater/T5-small-de-en',      help='model_name')\n",
    "    parser.add_argument('--exp_name', type=str,                     default='CYCLE!',      help='experiment name')\n",
    "    parser.add_argument('--rep_iter', type=int,                     default=100,      help='report times for 1 epoch')\n",
    "    parser.add_argument('--test_iter', type=int,                    default=500,      help='report times for 1 epoch')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int,                       default=50,     help='num of training epochs')\n",
    "\n",
    "    parser.add_argument('--G_lr', type=float,                       default=5e-6,   help='learning rate for G')\n",
    "    parser.add_argument('--G_weight_decay', type=float,             default=1e-3,   help='learning de for G')\n",
    "    parser.add_argument('--G_gamma', type=float,                    default=1,    help='lr*gamma after each test')\n",
    "    parser.add_argument('--G_grad_clip', type=float,                default=1,   help='grad_clip')\n",
    "    parser.add_argument('--D_lr', type=float,                       default=5e-5,   help='learning rate for D')\n",
    "    parser.add_argument('--D_weight_decay', type=float,             default=1e-3,   help='learning de for D')\n",
    "    parser.add_argument('--D_gamma', type=float,                    default=1,    help='lr*gamma after each test')\n",
    "    parser.add_argument('--D_grad_clip', type=float,                default=1e-2,   help='grad_clip')\n",
    "    parser.add_argument('--lambda_identity', type=float,            default=0.5,   help='')\n",
    "    parser.add_argument('--lambda_A', type=float,                   default=50,   help='')\n",
    "    parser.add_argument('--lambda_B', type=float,                   default=50,   help='')\n",
    "    parser.add_argument('--lambda_once', type=float,                default=10,   help='')\n",
    "    parser.add_argument('--lambda_GP', type=float,                  default=1,   help='WGANGP pentalty')\n",
    "    parser.add_argument('--smoothing', type=float,                  default=0.5,    help='labelsmoothing')\n",
    "\n",
    "    parser.add_argument('--load_D', type=int,                       default=0,      help='load pretrained D')\n",
    "    parser.add_argument('--load_G', type=int,                       default=0,      help='load pretrained D')\n",
    "    parser.add_argument('--num_workers', type=int,                  default=0,      help='num_workers')\n",
    "    parser.add_argument('--valid_begin', type=int,                  default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--train_G', type=int,                      default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--train_D', type=int,                      default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--D_pretrain_iter', type=int,              default=100,      help='whether valid before train')\n",
    "    parser.add_argument('--poolsize', type=int,                     default=1,      help='whether valid before train')\n",
    "\n",
    "\n",
    "    args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "    args.test_iter = args.test_iter//args.batch_size * args.batch_size\n",
    "    args.rep_iter = args.rep_iter//args.batch_size * args.batch_size\n",
    "    print('args.test_iter',args.test_iter)\n",
    "    print('args.rep_iter',args.rep_iter)#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\GitCode\\cycleMT\\wandb\\run-20220510_172234-fzwtycrc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/CYCLEGAN/runs/fzwtycrc\" target=\"_blank\">CYCLE!</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/CYCLEGAN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/CYCLEGAN/runs/fzwtycrc?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1e3bfa0eca0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_API_KEY']='a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME']=args.exp_name\n",
    "wandb.init(project=\"CYCLEGAN\",config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/10 05:22:42 PM |\t  Namespace(D_A_model_name='t5-small', D_B_model_name='Onlydrinkwater/T5-small-de-en', D_gamma=1, D_grad_clip=0.01, D_lr=5e-05, D_pretrain_iter=100, D_weight_decay=0.001, G_AB_model_name='t5-small', G_BA_model_name='Onlydrinkwater/T5-small-de-en', G_gamma=0.9, G_grad_clip=1, G_lr=5e-05, G_weight_decay=0.001, batch_size=4, epochs=50, exp_name='CYCLE!', gpu=0, lambda_A=0, lambda_B=0, lambda_GP=0.1, lambda_identity=0.5, lambda_once=1, load_D=0, load_G=0, max_length=128, num_workers=0, poolsize=1, rep_iter=100, smoothing=0.5, test_iter=500, train_D=1, train_G=1, train_num_points=500, valid_begin=1, valid_num_points=100)\n"
     ]
    }
   ],
   "source": [
    "#logging file\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'),'w',encoding = \"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "logging.info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/10 05:22:47 PM |\t  Gmodelsize:60.506624MB\n",
      "05/10 05:22:47 PM |\t  Dmodelsize:60.506624MB\n"
     ]
    }
   ],
   "source": [
    "GABmodelname = args.G_AB_model_name\n",
    "GBAmodelname = args.G_BA_model_name\n",
    "DAmodelname = args.D_A_model_name\n",
    "DBmodelname = args.D_B_model_name\n",
    "GABpretrained  =  AutoModelForSeq2SeqLM.from_pretrained(GABmodelname)\n",
    "GBApretrained  =  AutoModelForSeq2SeqLM.from_pretrained(GBAmodelname)\n",
    "DApretrained  =  AutoModelForSeq2SeqLM.from_pretrained(DAmodelname)\n",
    "DBpretrained  =  AutoModelForSeq2SeqLM.from_pretrained(DBmodelname)\n",
    "logging.info(f'Gmodelsize:{count_parameters_in_MB(GABpretrained)}MB')\n",
    "logging.info(f'Dmodelsize:{count_parameters_in_MB(DApretrained)}MB')\n",
    "tokenizer = AutoTokenizer.from_pretrained(GABmodelname)\n",
    "# tokenizerBA = AutoTokenizer.from_pretrained(GBAmodelname)#its the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/10 05:22:50 PM |\t  Using custom data configuration de-en-lang1=de,lang2=en\n",
      "05/10 05:22:50 PM |\t  Reusing dataset bible_para (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\bible_para\\de-en-lang1=de,lang2=en\\0.0.0\\b6cc20bcbfb0299beeba1dcc80a8420b975938ca0eef75b3ed30b50df7d950b1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 250.71it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"bible_para\", lang1=\"de\", lang2=\"en\")#load_dataset('wmt16',language+'-en')\n",
    "train = dataset['train']['translation'][:args.train_num_points]\n",
    "valid = dataset['train']['translation'][-args.valid_num_points:]#TODO:\n",
    "\n",
    "\n",
    "train_data = get_Dataset_chaos(train, tokenizer,max_length=args.max_length)\n",
    "train_dataloader = DataLoader(train_data, sampler= RandomSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "valid_data = get_Dataset(valid, tokenizer,max_length=args.max_length)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycleGAN = CycleGAN(args,GABpretrained,GBApretrained,DApretrained,DBpretrained,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/10 05:22:58 PM |\t  DB_a_: 0.069,  0.067,  0.147,  0.103,  \n",
      "05/10 05:22:58 PM |\t  DB_pred_dis: 0.096,  0.071,  0.163,  0.129,  \n",
      "05/10 05:22:58 PM |\t  DA_b: -0.06,  -0.05,  -0.07,  -0.04,  \n",
      "05/10 05:22:58 PM |\t  DA_pred_dis: -0.03,  -0.04,  -0.06,  -0.05,  \n",
      "05/10 05:22:58 PM |\t  GABloss:\t7.778103351593018\n",
      "05/10 05:22:58 PM |\t  GBAloss:\t9.578798294067383\n",
      "05/10 05:22:58 PM |\t  a_decoded[:2]:['And the kings of the earth, who have committed fornication and lived deliciously with her, shall bewail her, and lament for her, when they shall see the smoke of her burning,', 'Standing afar off for the fear of her torment, saying, Alas, alas, that great city Babylon, that mighty city! for in one hour is thy judgment come.', 'And the merchants of the earth shall weep and mourn over her; for no man buyeth their merchandise any more:', 'The merchandise of gold, and silver, and precious stones, and of pearls, and fine linen, and purple, and silk, and scarlet, and all thyine wood, and all manner vessels of ivory, and all manner vessels of most precious wood, and of brass, and iron, and marble,']\n",
      "05/10 05:22:58 PM |\t  pred_b_decoded[:2]:['They are being denied and they are blaming the kings on the earth who have hurried and driven by courage when they see the smoke from their fire;', \"It is a great city, the strong city, and it has come to justice in one hour's time.\", 'Those who buy on the planet will be ill and suffering because no one else will buy their goods,', 'The War of gold and silver and precious stones and the perl and eastern leeway and purpur and sand and sand and sand and sand and sand and sand and sand and sand and sand and sand and sand and sand and sand and sand and sand, and all the well-intentioned wood and all the well-intentioned wood and sand and marble,']\n",
      "05/10 05:22:58 PM |\t  b_decoded[:2]:['Und es werden sie beweinen und sie beklagen die Könige auf Erden, die mit ihr gehurt und Mutwillen getrieben haben, wenn sie sehen werden den Rauch von ihrem Brand;', 'und werden von ferne stehen vor Furcht ihrer Qual und sprechen: Weh, weh, die große Stadt Babylon, die starke Stadt! In einer Stunde ist ihr Gericht gekommen.', 'Und die Kaufleute auf Erden werden weinen und Leid tragen über sie, weil ihre Ware niemand mehr kaufen wird,', 'die Ware des Goldes und Silbers und Edelgesteins und die Perlen und köstliche Leinwand und Purpur und Seide und Scharlach und allerlei wohlriechendes Holz und allerlei Gefäß von Elfenbein und allerlei Gefäß von köstlichem Holz und von Erz und von Eisen und von Marmor,']\n",
      "05/10 05:22:58 PM |\t  pred_a_decoded[:2]:['Und die Könige der Erde, die sich mit ihr schmecken und köstlich gelebt haben, werden sie wüten und für sie beklagen, wenn sie den Rauch ihrer Verbrennung sehen werden,', 'Und es ist es, daß es sich um eine ganze Stunde handelt, denn es ist es, daß es sich um eine große Stadt Babylon handelt.', 'Und die Händler der Erde werden sich über sie weinen und trauern; denn niemand kauft ihre Waren mehr:', 'Die Waren aus Gold und Silber, und aus Edelsteinen, und aus Perlen und lila, und Seide, und alle thyinen Holz, und alle Arten von Elfenbein, und alle Arten von Schiffen aus dem kostbaren Holz, und aus Messing, und Eisen und Marmor,']\n",
      "05/10 05:23:38 PM |\t  computing score...\n",
      "05/10 05:23:38 PM |\t  G_AB GAB sacreBLEU : 19.831500\n",
      "05/10 05:23:38 PM |\t  G_BA GBA sacreBLEU : 11.326494\n",
      "05/10 05:23:38 PM |\t  G_AB GAB test loss : 7.740147\n",
      "05/10 05:23:38 PM |\t  G_BA GBA test loss : 10.451045\n",
      "05/10 05:23:38 PM |\t  \n",
      "\n",
      "  ----------------epoch:0----------------\n",
      "05/10 05:23:38 PM |\t  total iter:[0] \t G_lr:5e-05 \t DA_lr:5e-05 \t DB_lr:5e-05\n",
      "05/10 05:24:49 PM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.056747544556856155, 'GBA_once_meter': -0.039818622171878815, 'DA_meter': -0.0005450084060430526, 'DB_meter': -0.022244747653603555, 'DA_GP_meter': 0.07705596566200257, 'DB_GP_meter': 0.10042955368757248}\n",
      "05/10 05:24:49 PM |\t  19.2%\n",
      "05/10 05:26:05 PM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.052460173442959786, 'GBA_once_meter': -0.07503709733486176, 'DA_meter': -0.008550540637224912, 'DB_meter': -0.015057584568858147, 'DA_GP_meter': 0.10818269327282906, 'DB_GP_meter': 0.07940211415290832}\n",
      "05/10 05:26:05 PM |\t  39.2%\n",
      "05/10 05:27:20 PM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.04861260026693344, 'GBA_once_meter': -0.07947483643889428, 'DA_meter': -0.010976399555802345, 'DB_meter': -0.016627055779099464, 'DA_GP_meter': 0.07819300204515457, 'DB_GP_meter': 0.11502638548612594}\n",
      "05/10 05:27:20 PM |\t  59.199999999999996%\n",
      "05/10 05:28:49 PM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.05148069404065609, 'GBA_once_meter': -0.08943549752235412, 'DA_meter': -0.019548531621694565, 'DB_meter': -0.007290616407990455, 'DA_GP_meter': 0.07788236707448959, 'DB_GP_meter': 0.07975061014294624}\n",
      "05/10 05:28:49 PM |\t  79.2%\n",
      "05/10 05:31:15 PM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.05276660598814487, 'GBA_once_meter': -0.08799467086791993, 'DA_meter': -0.030075096525251865, 'DB_meter': -0.0035890892148017885, 'DA_GP_meter': 0.09961246147751808, 'DB_GP_meter': 0.08232765227556228}\n",
      "05/10 05:31:15 PM |\t  99.2%\n",
      "05/10 05:31:25 PM |\t  DB_a_: 0.096,  0.071,  0.172,  0.096,  \n",
      "05/10 05:31:25 PM |\t  DB_pred_dis: 0.104,  0.077,  0.059,  0.080,  \n",
      "05/10 05:31:25 PM |\t  DA_b: -0.00,  -0.01,  -0.03,  0.025,  \n",
      "05/10 05:31:25 PM |\t  DA_pred_dis: -0.00,  -0.04,  -0.03,  -0.02,  \n",
      "05/10 05:31:25 PM |\t  GABloss:\t6.839828014373779\n",
      "05/10 05:31:25 PM |\t  GBAloss:\t14.511137008666992\n",
      "05/10 05:31:25 PM |\t  a_decoded[:2]:['And the kings of the earth, who have committed fornication and lived deliciously with her, shall bewail her, and lament for her, when they shall see the smoke of her burning,', 'Standing afar off for the fear of her torment, saying, Alas, alas, that great city Babylon, that mighty city! for in one hour is thy judgment come.', 'And the merchants of the earth shall weep and mourn over her; for no man buyeth their merchandise any more:', 'The merchandise of gold, and silver, and precious stones, and of pearls, and fine linen, and purple, and silk, and scarlet, and all thyine wood, and all manner vessels of ivory, and all manner vessels of most precious wood, and of brass, and iron, and marble,']\n",
      "05/10 05:31:25 PM |\t  pred_b_decoded[:2]:['They are being savaged by the kingdoms of heavens who have hurled their spirits and hurried their spirits when they see their fires; they are being tortured; they are being tortured; they are being tortured; they are being tortured; they are being tortured; and they are being tortured; they are being tortured; they are being tortured; they are being tortured; they are being tortured; they are being tortured; they are being tortured; they are being tortured by their fires; they are savaged; they', 'They are afraid of their curses, and they are afraid of Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Babylon, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, Jerusalem, in one hour’s silence.', 'Those who buy their goods are dying and dying, because no one else will buy their goods, they are dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, dying, and dying, dying, dying, and dying, dying, dying, and dying, dying, and dying, dying, and they will no longer buy their goods;', 'Gold and silver silver and silver silver and silver gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold gold and marsh, gold and silver silver silver and silver gold gold gold']\n",
      "05/10 05:31:25 PM |\t  b_decoded[:2]:['Und es werden sie beweinen und sie beklagen die Könige auf Erden, die mit ihr gehurt und Mutwillen getrieben haben, wenn sie sehen werden den Rauch von ihrem Brand;', 'und werden von ferne stehen vor Furcht ihrer Qual und sprechen: Weh, weh, die große Stadt Babylon, die starke Stadt! In einer Stunde ist ihr Gericht gekommen.', 'Und die Kaufleute auf Erden werden weinen und Leid tragen über sie, weil ihre Ware niemand mehr kaufen wird,', 'die Ware des Goldes und Silbers und Edelgesteins und die Perlen und köstliche Leinwand und Purpur und Seide und Scharlach und allerlei wohlriechendes Holz und allerlei Gefäß von Elfenbein und allerlei Gefäß von köstlichem Holz und von Erz und von Eisen und von Marmor,']\n",
      "05/10 05:31:25 PM |\t  pred_a_decoded[:2]:['Und die Könige der Erde, die sich für die Vernichtung engagiert haben und köstlich mit ihr gelebt haben, werden sie wüten und für sie beklagen, wenn sie den Rauch ihrer Verbrennung sehen werden, wenn sie sehen werden, wie ihr Feuer brennen wird, wenn sie sehen werden, wenn sie den Rauch ihrer Verbrennung sehen,', 'Und es hat sich erwiesen, daß es sich um eine ganze Stunde handelt, denn in einer Stunde kommt dein Urteil.', 'Und die Händler der Erde werden über sie weinen und trauern; denn kein Mensch kauft ihre Waren mehr:', 'Die Waren aus Gold und Silber, und Edelsteine, und von Perlen, und lila, und Seide, und Schneiden, und alle Thyine Holz, und alle Arten von Elfenbein, und alle Arten Schiffe aus dem wertvollsten Holz, und von Messing, und Eisen, und Marmor, und alle Arten von Schiffen aus dem wertvollsten Holz, und alle Arten von Schiffen aus dem wertvollsten Holz, und von Messing']\n",
      "05/10 05:32:22 PM |\t  computing score...\n",
      "05/10 05:32:22 PM |\t  G_AB GAB sacreBLEU : 20.216327\n",
      "05/10 05:32:22 PM |\t  G_BA GBA sacreBLEU : 2.853768\n",
      "05/10 05:32:22 PM |\t  G_AB GAB test loss : 6.075680\n",
      "05/10 05:32:22 PM |\t  G_BA GBA test loss : 13.363660\n",
      "05/10 05:32:22 PM |\t  \n",
      "\n",
      "  ----------------epoch:1----------------\n",
      "05/10 05:32:22 PM |\t  total iter:[500] \t G_lr:4.05e-05 \t DA_lr:5e-05 \t DB_lr:5e-05\n",
      "05/10 05:34:50 PM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.0467685323394835, 'GBA_once_meter': -0.06501165837049484, 'DA_meter': -0.027837147824466228, 'DB_meter': -0.030129474736750126, 'DA_GP_meter': 0.07725175783038139, 'DB_GP_meter': 2.306772528886795}\n",
      "05/10 05:34:50 PM |\t  19.2%\n"
     ]
    }
   ],
   "source": [
    "if(args.valid_begin==1):\n",
    "    my_test(valid_dataloader,cycleGAN,tokenizer,logging,wandb)\n",
    "total_iter = [0]  \n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch}----------------\")\n",
    "    my_train(train_dataloader,cycleGAN,total_iter,args,logging,valid_dataloader,tokenizer,wandb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d33c3b0ef123e851f98887a8750ca7da758e4ff258891935cfe6ff9c0394387"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
