{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.test_iter 498\n",
      "args.rep_iter 99\n"
     ]
    }
   ],
   "source": [
    "if(True):\n",
    "    parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "    parser.add_argument('--valid_num_points', type=int,             default = 100, help='validation data number')\n",
    "    parser.add_argument('--train_num_points', type=int,             default = 500, help='train data number')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int,                   default=3,     help='Batch size')\n",
    "    parser.add_argument('--max_length', type=int,                   default=512,     help='max_length')\n",
    "\n",
    "    parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "    parser.add_argument('--G_AB_model_name', type=str,              default='t5-small',      help='model_name')\n",
    "    parser.add_argument('--G_BA_model_name', type=str,              default='Onlydrinkwater/T5-small-de-en',      help='model_name')\n",
    "    parser.add_argument('--D_A_model_name', type=str,               default='t5-small',      help='model_name')\n",
    "    parser.add_argument('--D_B_model_name', type=str,               default='Onlydrinkwater/T5-small-de-en',      help='model_name')\n",
    "    parser.add_argument('--exp_name', type=str,                     default='CYCLE!',      help='experiment name')\n",
    "    parser.add_argument('--rep_num', type=int,                      default=25,      help='report times for 1 epoch')\n",
    "    parser.add_argument('--rep_iter', type=int,                     default=100,      help='report times for 1 epoch')\n",
    "    parser.add_argument('--test_iter', type=int,                    default=500,      help='report times for 1 epoch')\n",
    "    parser.add_argument('--test_num', type=int,                     default=4,      help='test times for 1 epoch')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int,                       default=50,     help='num of training epochs')\n",
    "\n",
    "    parser.add_argument('--G_lr', type=float,                       default=0.0001,   help='learning rate for G')\n",
    "    parser.add_argument('--G_weight_decay', type=float,             default=1e-3,   help='learning de for G')\n",
    "    parser.add_argument('--D_lr', type=float,                       default=0.000001,   help='learning rate for D')\n",
    "    parser.add_argument('--D_weight_decay', type=float,             default=1e-3,   help='learning de for D')\n",
    "    parser.add_argument('--lambda_identity', type=float,            default=0.5,   help='')\n",
    "    parser.add_argument('--lambda_A', type=float,                   default=0,   help='')\n",
    "    parser.add_argument('--lambda_B', type=float,                   default=0,   help='')\n",
    "    parser.add_argument('--lambda_once', type=float,                default=1,   help='')\n",
    "    parser.add_argument('--smoothing', type=float,                  default=0.1,    help='labelsmoothing')\n",
    "\n",
    "\n",
    "    parser.add_argument('--load_D', type=int,                       default=0,      help='load pretrained D')\n",
    "    parser.add_argument('--valid_begin', type=int,                  default=0,      help='whether valid before train')\n",
    "    parser.add_argument('--train_G', type=int,                      default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--train_D', type=int,                      default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--D_pretrain_iter', type=int,              default=0,      help='whether valid before train')\n",
    "\n",
    "\n",
    "    args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "    args.test_iter = args.test_iter//args.batch_size * args.batch_size\n",
    "    args.rep_iter = args.rep_iter//args.batch_size * args.batch_size\n",
    "    print('args.test_iter',args.test_iter)\n",
    "    print('args.rep_iter',args.rep_iter)#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\GitCode\\cycleMT\\wandb\\run-20220428_010131-g9u63rfq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/CYCLEGAN/runs/g9u63rfq\" target=\"_blank\">CYCLE!</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/CYCLEGAN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/CYCLEGAN/runs/g9u63rfq?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x164e800baf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_API_KEY']='a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME']=args.exp_name\n",
    "wandb.init(project=\"CYCLEGAN\",config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/28 01:01:38 AM |\t  Namespace(D_A_model_name='t5-small', D_B_model_name='Onlydrinkwater/T5-small-de-en', D_lr=1e-05, D_pretrain_iter=0, D_weight_decay=0.001, G_AB_model_name='t5-small', G_BA_model_name='Onlydrinkwater/T5-small-de-en', G_lr=0.0001, G_weight_decay=0.001, batch_size=3, epochs=50, exp_name='CYCLE!', gpu=0, lambda_A=0, lambda_B=0, lambda_identity=0.5, lambda_once=1, load_D=0, max_length=512, rep_iter=99, rep_num=25, smoothing=0.1, test_iter=498, test_num=4, train_D=1, train_G=1, train_num_points=500, valid_begin=0, valid_num_points=100)\n"
     ]
    }
   ],
   "source": [
    "#logging file\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'),'w',encoding = \"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "logging.info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/28 01:01:43 AM |\t  Gmodelsize:60.506624MB\n",
      "04/28 01:01:43 AM |\t  Dmodelsize:60.506624MB\n"
     ]
    }
   ],
   "source": [
    "GABmodelname = args.G_AB_model_name\n",
    "GBAmodelname = args.G_BA_model_name\n",
    "DAmodelname = args.D_A_model_name\n",
    "DBmodelname = args.D_B_model_name\n",
    "GABpretrained  =  AutoModelForSeq2SeqLM.from_pretrained(GABmodelname)\n",
    "GBApretrained  =  AutoModelForSeq2SeqLM.from_pretrained(GBAmodelname)\n",
    "DApretrained  =  AutoModelForSeq2SeqLM.from_pretrained(DAmodelname)\n",
    "DBpretrained  =  AutoModelForSeq2SeqLM.from_pretrained(DBmodelname)\n",
    "logging.info(f'Gmodelsize:{count_parameters_in_MB(GABpretrained)}MB')\n",
    "logging.info(f'Dmodelsize:{count_parameters_in_MB(DApretrained)}MB')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(GABmodelname)\n",
    "# tokenizerBA = AutoTokenizer.from_pretrained(GBAmodelname)#its the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/28 01:01:47 AM |\t  Reusing dataset wmt16 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt16\\de-en\\1.0.0\\0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 32.36it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('wmt16','de-en')\n",
    "train = dataset['train']['translation'][:args.train_num_points]\n",
    "valid = dataset['train']['translation'][args.train_num_points:(args.train_num_points+args.valid_num_points)]\n",
    "\n",
    "\n",
    "train_data = get_Dataset_chaos(train, tokenizer,max_length=args.max_length)\n",
    "train_dataloader = DataLoader(train_data, sampler= RandomSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=True, num_workers=2)\n",
    "valid_data = get_Dataset(valid, tokenizer,max_length=args.max_length)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=args.batch_size, pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycleGAN = CycleGAN(args,GABpretrained,GBApretrained,DApretrained,DBpretrained,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/28 01:02:49 AM |\t  \n",
      "\n",
      "  ----------------epoch:0----------------\n",
      "04/28 01:02:49 AM |\t  total iter:[0]\n",
      "04/28 01:03:38 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 1.0574363018527175, 'GBA_once_meter': 0.9650687611464298, 'DA_meter': 0.5049331224325931, 'DB_meter': 0.4438437178279414}\n",
      "04/28 01:03:38 AM |\t  19.16167664670659%\n",
      "04/28 01:04:21 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 1.0186138767184634, 'GBA_once_meter': 0.9449522043719436, 'DA_meter': 0.44446715531927167, 'DB_meter': 0.393058284665599}\n",
      "04/28 01:04:21 AM |\t  38.92215568862276%\n",
      "04/28 01:05:05 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 1.0009713570276897, 'GBA_once_meter': 0.9212123372338035, 'DA_meter': 0.3905667716806585, 'DB_meter': 0.35428065874359826}\n",
      "04/28 01:05:05 AM |\t  58.68263473053892%\n",
      "04/28 01:05:48 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.9765893564079747, 'GBA_once_meter': 0.8741942138382883, 'DA_meter': 0.34157898660862085, 'DB_meter': 0.31664165222283563}\n",
      "04/28 01:05:48 AM |\t  78.44311377245509%\n",
      "04/28 01:06:32 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.9570961088845225, 'GBA_once_meter': 0.8251623944802717, 'DA_meter': 0.29775967561837396, 'DB_meter': 0.296845254572955}\n",
      "04/28 01:06:32 AM |\t  98.20359281437125%\n",
      "04/28 01:06:40 AM |\t  GABloss:\t6.908642292022705\n",
      "04/28 01:06:40 AM |\t  GBAloss:\t13.968677520751953\n",
      "04/28 01:06:40 AM |\t  a_decoded[:2]:['Past experience dictates that, as the elected representatives of the European taxpayer, we should, and indeed must, demand financial probity and transparency in the disbursement and auditing of this money, hence our amendments and additions relate to achieving what are known as \"value for money\" indicators in the grant-giving process.', 'Next, we all too often see vast sums of money being spent on projects whose outcomes will necessarily be unclear at the start of the programme period.']\n",
      "04/28 01:06:40 AM |\t  pred_b_decoded[:2]:['. (FR) The experience of the past shows that we, as the elected representatives of the European taxpayers, should demand financial redundancy and transparency in the payment of these funds and the associated auditing, and with our amendments and quotas, we want to achieve what can be achieved by granting grants as an indicator of the most economically favourable solution.', 'Secondly, all too often, huge sums are being spent on projects, whose results are simply not yet clear at the beginning of the programme.']\n",
      "04/28 01:06:40 AM |\t  b_decoded[:2]:['Die Erfahrungen der Vergangenheit zeigen, daß wir als die gewählten Vertreter der europäischen Steuerzahler finanzielle Redlichkeit und Transparenz bei der Auszahlung dieser Gelder und der damit verbundenen Rechnungsprüfung fordern sollten, ja müssen. Mit unseren nderungen und Zusätzen wollen wir das erreichen, was bei der Gewährung von Zuschüssen als Indikator für die wirtschaftlich günstigste Lösung dienen kann.', 'Zweitens fließen nur allzu oft riesige Summen in Projekte, deren Ergebnisse sich zu Beginn des Programmzeitraums einfach noch nicht klar abschätzen lassen.']\n",
      "04/28 01:06:40 AM |\t  pred_a_decoded[:2]:['Wir haben es mit unserer Erfahrung zu tun - wir haben es mit unserer Erfahrung zu tun -, wir haben es mit unserer Erfahrung zu tun - wir haben es mit unserer Erfahrung zu tun - wir haben es mit unserer Erfahrung zu tun - wir haben es mit unserer Erfahrung zu tun - wir haben es mit unserer Erfahrung zu tun - wir haben es mit unserer Erfahrung zu tun - wir haben es mit unserer', 'Wir müssen uns bemühen lassen lassen lassen lassen lassen lassen lassen.']\n",
      "04/28 01:07:59 AM |\t  computing score...\n",
      "04/28 01:07:59 AM |\t  G_AB GAB sacreBLEU : 3.902123\n",
      "04/28 01:07:59 AM |\t  G_BA GBA sacreBLEU : 21.717091\n",
      "04/28 01:07:59 AM |\t  G_AB GAB test loss : 8.078814\n",
      "04/28 01:07:59 AM |\t  G_BA GBA test loss : 13.423636\n",
      "04/28 01:08:01 AM |\t  \n",
      "\n",
      "  ----------------epoch:1----------------\n",
      "04/28 01:08:01 AM |\t  total iter:[501]\n",
      "04/28 01:08:44 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.871256658525178, 'GBA_once_meter': 0.7946602395086577, 'DA_meter': 0.2540579000205705, 'DB_meter': 0.2652862537087816}\n",
      "04/28 01:08:44 AM |\t  17.964071856287426%\n",
      "04/28 01:09:28 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.809733018730626, 'GBA_once_meter': 0.8382837844617439, 'DA_meter': 0.2256879869735602, 'DB_meter': 0.23367170312187888}\n",
      "04/28 01:09:28 AM |\t  37.72455089820359%\n",
      "04/28 01:10:11 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.8444077697667208, 'GBA_once_meter': 0.807511871511286, 'DA_meter': 0.19402991461031366, 'DB_meter': 0.2142566523768685}\n",
      "04/28 01:10:11 AM |\t  57.48502994011976%\n",
      "04/28 01:10:55 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.8569840608221112, 'GBA_once_meter': 0.7971658074494564, 'DA_meter': 0.1689018476189989, 'DB_meter': 0.19645459362954804}\n",
      "04/28 01:10:55 AM |\t  77.24550898203593%\n",
      "04/28 01:11:38 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.7859065695242449, 'GBA_once_meter': 0.821541013139667, 'DA_meter': 0.14327157808072638, 'DB_meter': 0.17030461222836465}\n",
      "04/28 01:11:38 AM |\t  97.0059880239521%\n",
      "04/28 01:11:54 AM |\t  GABloss:\t7.402412414550781\n",
      "04/28 01:11:54 AM |\t  GBAloss:\t15.226232528686523\n",
      "04/28 01:11:54 AM |\t  a_decoded[:2]:['Past experience dictates that, as the elected representatives of the European taxpayer, we should, and indeed must, demand financial probity and transparency in the disbursement and auditing of this money, hence our amendments and additions relate to achieving what are known as \"value for money\" indicators in the grant-giving process.', 'Next, we all too often see vast sums of money being spent on projects whose outcomes will necessarily be unclear at the start of the programme period.']\n",
      "04/28 01:11:54 AM |\t  pred_b_decoded[:2]:['.. we, as the elected representatives of the European taxpayers, we, as the elected representatives of the European taxpayers, should, we, the, our, our, our, our, our, our, our, our, our, our, our, our, our, our, our, our,,, our,,,,,,,,,,,, a securing grants, the most economically favourable.', '.........................................................']\n",
      "04/28 01:11:54 AM |\t  b_decoded[:2]:['Die Erfahrungen der Vergangenheit zeigen, daß wir als die gewählten Vertreter der europäischen Steuerzahler finanzielle Redlichkeit und Transparenz bei der Auszahlung dieser Gelder und der damit verbundenen Rechnungsprüfung fordern sollten, ja müssen. Mit unseren nderungen und Zusätzen wollen wir das erreichen, was bei der Gewährung von Zuschüssen als Indikator für die wirtschaftlich günstigste Lösung dienen kann.', 'Zweitens fließen nur allzu oft riesige Summen in Projekte, deren Ergebnisse sich zu Beginn des Programmzeitraums einfach noch nicht klar abschätzen lassen.']\n",
      "04/28 01:11:54 AM |\t  pred_a_decoded[:2]:['Wir freuen uns uns Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir', 'Wir freuen uns uns Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir Wir']\n"
     ]
    }
   ],
   "source": [
    "if(args.valid_begin==1):\n",
    "    my_test(valid_dataloader,cycleGAN,tokenizer,logging,wandb)\n",
    "total_iter = [0]  \n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch}----------------\")\n",
    "    my_train(train_dataloader,cycleGAN,total_iter,args,logging,valid_dataloader,tokenizer,wandb)\n",
    "    # my_test(valid_dataloader,cycleGAN,tokenizer,logging,wandb)\n",
    "    #TODO:cycgan.savemodel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
