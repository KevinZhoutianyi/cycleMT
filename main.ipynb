{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "from parameter import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.test_iter 496\n",
      "args.rep_iter 96\n"
     ]
    }
   ],
   "source": [
    "if(True):\n",
    "    parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "    parser.add_argument('--valid_num_points', type=int,             default = 100, help='validation data number')\n",
    "    parser.add_argument('--train_num_points', type=int,             default = 500, help='train data number')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int,                   default=8,     help='Batch size')\n",
    "    parser.add_argument('--max_length', type=int,                   default=128,     help='max_length')\n",
    "    parser.add_argument('--num_beam', type=int,                     default=1,     help='num_beam')\n",
    "\n",
    "    parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "    parser.add_argument('--G_AB_model_name', type=str,              default='t5-small',      help='model_name')\n",
    "    parser.add_argument('--G_BA_model_name', type=str,              default='Onlydrinkwater/T5-small-de-en',      help='model_name')\n",
    "    parser.add_argument('--D_A_model_name', type=str,               default='Onlydrinkwater/T5-small-de-en',      help='model_name')\n",
    "    parser.add_argument('--D_B_model_name', type=str,               default='t5-small',      help='model_name')\n",
    "    parser.add_argument('--exp_name', type=str,                     default='CYCLE!',      help='experiment name')\n",
    "    parser.add_argument('--rep_iter', type=int,                     default=100,      help='report times for 1 epoch')\n",
    "    parser.add_argument('--test_iter', type=int,                    default=500,      help='report times for 1 epoch')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int,                       default=50,     help='num of training epochs')\n",
    "\n",
    "    parser.add_argument('--G_lr', type=float,                       default=5e-6,   help='learning rate for G')\n",
    "    parser.add_argument('--G_weight_decay', type=float,             default=1e-3,   help='learning de for G')\n",
    "    parser.add_argument('--G_gamma', type=float,                    default=1,    help='lr*gamma after each test')\n",
    "    parser.add_argument('--G_grad_clip', type=float,                default=1,   help='grad_clip')\n",
    "    parser.add_argument('--D_lr', type=float,                       default=5e-5,   help='learning rate for D')\n",
    "    parser.add_argument('--D_weight_decay', type=float,             default=1e-3,   help='learning de for D')\n",
    "    parser.add_argument('--D_gamma', type=float,                    default=1,    help='lr*gamma after each test')\n",
    "    parser.add_argument('--D_grad_clip', type=float,                default=1e-2,   help='grad_clip')\n",
    "    parser.add_argument('--lambda_identity', type=float,            default=0.5,   help='')\n",
    "    parser.add_argument('--lambda_A', type=float,                   default=1,   help='')\n",
    "    parser.add_argument('--lambda_B', type=float,                   default=1,   help='')\n",
    "    parser.add_argument('--lambda_once', type=float,                default=0,   help='')\n",
    "    parser.add_argument('--lambda_GP', type=float,                  default=10,   help='WGANGP pentalty')\n",
    "    parser.add_argument('--DperG', type=int,                        default=2,    help='n_critc')\n",
    "    parser.add_argument('--GperD', type=int,                        default=2,    help='n_g')\n",
    "    parser.add_argument('--smoothing', type=float,                  default=0.5,    help='labelsmoothing')\n",
    "\n",
    "    parser.add_argument('--load_D', type=int,                       default=0,      help='load pretrained D')\n",
    "    parser.add_argument('--load_G', type=int,                       default=0,      help='load pretrained D')\n",
    "    parser.add_argument('--num_workers', type=int,                  default=0,      help='num_workers')\n",
    "    parser.add_argument('--valid_begin', type=int,                  default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--train_G', type=int,                      default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--train_D', type=int,                      default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--D_pretrain_iter', type=int,              default=0,      help='whether valid before train')\n",
    "    parser.add_argument('--poolsize', type=int,                     default=1,      help='whether valid before train')\n",
    "\n",
    "\n",
    "    args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "    args.test_iter = args.test_iter//args.batch_size * args.batch_size\n",
    "    args.rep_iter = args.rep_iter//args.batch_size * args.batch_size\n",
    "    print('args.test_iter',args.test_iter)\n",
    "    print('args.rep_iter',args.rep_iter)#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\GitCode\\cycleMT\\wandb\\run-20220531_004822-26o184v9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/cycleWMT/runs/26o184v9\" target=\"_blank\">CYCLE!</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/cycleWMT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/cycleWMT/runs/26o184v9?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2228ea63e50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_API_KEY']='a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME']=args.exp_name\n",
    "wandb.init(project=\"dsc291\",config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/31 12:48:26 AM |\t  Namespace(D_A_model_name='Onlydrinkwater/T5-small-de-en', D_B_model_name='t5-small', D_gamma=1, D_grad_clip=0.01, D_lr=5e-05, D_pretrain_iter=0, D_weight_decay=0.001, DperG=2, G_AB_model_name='t5-small', G_BA_model_name='Onlydrinkwater/T5-small-de-en', G_gamma=1, G_grad_clip=1, G_lr=5e-06, G_weight_decay=0.001, GperD=2, batch_size=8, epochs=50, exp_name='CYCLE!', gpu=0, lambda_A=1, lambda_B=1, lambda_GP=10, lambda_identity=0.5, lambda_once=0, load_D=0, load_G=0, max_length=128, num_beam=1, num_workers=0, poolsize=1, rep_iter=96, smoothing=0.5, test_iter=496, train_D=1, train_G=1, train_num_points=500, valid_begin=1, valid_num_points=100)\n"
     ]
    }
   ],
   "source": [
    "#logging file\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'),'w',encoding = \"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "logging.info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/31 12:48:31 AM |\t  Gmodelsize:60.506624MB\n",
      "05/31 12:48:31 AM |\t  Dmodelsize:60.506624MB\n"
     ]
    }
   ],
   "source": [
    "GABmodelname = args.G_AB_model_name\n",
    "GBAmodelname = args.G_BA_model_name\n",
    "DAmodelname = args.D_A_model_name\n",
    "DBmodelname = args.D_B_model_name\n",
    "GABpretrained  =  AutoModelForSeq2SeqLM.from_pretrained(GABmodelname)\n",
    "GBApretrained  =  AutoModelForSeq2SeqLM.from_pretrained(GBAmodelname)\n",
    "DApretrained  =  AutoModelForSeq2SeqLM.from_pretrained(DAmodelname)\n",
    "DBpretrained  =  AutoModelForSeq2SeqLM.from_pretrained(DBmodelname)\n",
    "logging.info(f'Gmodelsize:{count_parameters_in_MB(GABpretrained)}MB')\n",
    "logging.info(f'Dmodelsize:{count_parameters_in_MB(DApretrained)}MB')\n",
    "tokenizer = AutoTokenizer.from_pretrained(GABmodelname)\n",
    "# tokenizerBA = AutoTokenizer.from_pretrained(GBAmodelname)#its the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/31 12:48:35 AM |\t  Reusing dataset wmt16 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt16\\de-en\\1.0.0\\9e0038fe4cc117bd474d2774032cc133e355146ed0a47021b2040ca9db4645c0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 25.95it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('wmt16',language+'-en')#load_dataset(\"bible_para\", lang1=\"de\", lang2=\"en\")\n",
    "train = dataset['train']['translation'][:args.train_num_points]\n",
    "valid = dataset['validation']['translation'][-args.valid_num_points:]#TODO:\n",
    "\n",
    "\n",
    "train_data = get_Dataset_chaos(train, tokenizer,max_length=args.max_length)\n",
    "train_dataloader = DataLoader(train_data, sampler= RandomSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "valid_data = get_Dataset(valid, tokenizer,max_length=args.max_length)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycleGAN = CycleGAN(args,GABpretrained,GBApretrained,DApretrained,DBpretrained,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/31 12:49:13 AM |\t  DB_a_: 0.162,  0.140,  0.157,  0.179,  0.182,  0.151,  0.158,  0.165,  \n",
      "05/31 12:49:13 AM |\t  DB_pred_dis: 0.168,  0.190,  0.193,  0.165,  0.159,  0.146,  0.157,  0.140,  \n",
      "05/31 12:49:13 AM |\t  DA_b: 0.155,  0.097,  0.017,  0.083,  0.168,  0.109,  0.168,  0.117,  \n",
      "05/31 12:49:13 AM |\t  DA_pred_dis: 0.157,  0.085,  0.038,  0.103,  0.126,  0.134,  0.174,  0.128,  \n",
      "05/31 12:49:13 AM |\t  GABloss:\t5.591159343719482\n",
      "05/31 12:49:13 AM |\t  GBAloss:\t9.856866836547852\n",
      "05/31 12:49:13 AM |\t  a_decoded[:2]:['Prestige Cruises, also based in Miami, operates under the Oceania and Regent brands, which together have eight cruise ships traveling to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'It posted revenues of $1.2 billion in 2013, up 6 percent from the year earlier.', 'The $29 billion cruise industry is expected to benefit in the coming years from the rise of the middle class in emerging economies such as China and India.', 'Companies are racing to position themselves as the cruise operators of choice for these new customers.', 'Prestige Cruises registered with U.S. regulators for an initial public offering in January 2014.', \"Apollo has been the company's majority shareholder following an $850 million deal in 2007.\", 'Norwegian Cruise was created in its current form in 2000 through a merger with a cruise operator owned by Genting Bhd (GENT.KL), the leisure and casino conglomerate controlled by Malaysian billionaire Lim Kok Thay.', 'Apollo made a $1 billion investment in Norwegian Cruise in 2008.']\n",
      "05/31 12:49:13 AM |\t  pred_b_decoded[:2]:['The Prestige cruises, which was also based in Miami, is owned by Oceania and Regent, which are the joint operators of eight cross-roads to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'Revenue for 2013 was reported to be USD 1.2 billion, an increase of 6% compared with last year.', 'In the next few years, the cross-border sector will be expected to be around USD 29 billion, and it will be expected to be in the next few years, due to the strengthening of the middle class in the swine countries such as China and India.', 'The companies are trying to position themselves as the best choice for these new customers in the cross-border operators.', 'In January 2014, Prestige Cruises requested the stock exchanges with the US regulatory authorities.', 'Apollo has been the majority shareholder in the company since a $ 850 million deal in 2007.', 'Norwegian cruise was able to escape its current form in 2000 by combining with a cruise operator, GENT.KL, a leisure and Caribbean cruiser under the control of the Malaysian billionaire Lim Kok Thay.', 'Apollo invested USD 1 billion in Norwegian cruise in 2008.']\n",
      "05/31 12:49:13 AM |\t  b_decoded[:2]:['Prestige Cruises, ebenfalls in Miami angesiedelt, agiert unter den Markennamen Oceania und Regent, die gemeinsam acht Kreuzfahrschiffe auf Routen nach Skandinavien, Russland, dem Mittelmeer, Nordamerika, Asien, Afrika und Südamerika betreiben.', 'Es meldete Einnahmen für 2013 von 1,2 Milliarden $, ein Anstieg von 6 Prozent im Vergleich zum Vorjahr.', 'Die Kreuzfahrtbranche liegt insgesamt bei etwa 29 Milliarden $ und in den nächsten Jahren wird für sie aufgrund der Stärkung der Mittelklasse in Schwellenländern wie China und Indien erwartet.', 'Die Unternehmen versuchen sich für diese neuen Kunden als die beste Wahl bei den Kreuzfahrtanbietern zu positionieren.', 'Prestige Cruises beantragte bei den US-Regulierungsbehörden im Januar 2014 den Börsengang.', 'Apollo ist der Mehrheitsaktionär bei dem Unternehmen seit einem 850 Millionen $ Deal 2007.', 'Norwegian Cruise entstand in seiner derzeitigen Form im Jahre 2000 durch eine Fusion mit einem Kreuzfahrtanbieter im Besitz von Genting Bhd (GENT.KL), einem Freizeit- und Kasinomischkonzern unter der Kontrolle des malaysischen Milliardärs Lim Kok Thay.', 'Apollo investierte 2008 1 Milliarde $ in Norwegian Cruise.']\n",
      "05/31 12:49:13 AM |\t  pred_a_decoded[:2]:['Prestige Cruises, auch in Miami mit Sitz, hat acht Kreuzfahrtschiffe, die nach Skandinavien, Russland, dem Mittelmeerraum, Nordamerika, Asien, Afrika und Südamerika reisen.', 'Das Unternehmen erzielte einen Umsatz von 1,2 Milliarden Dollar im Jahr 2013 gegenüber dem Vorjahr.', 'Es wird erwartet, dass die Reiseindustrie in den kommenden Jahren von dem Aufstieg der Mittelschicht in Schwellenländern wie China und Indien profitieren wird.', 'Die Unternehmen setzen sich für diese neuen Kunden als die bevorzugten Kreuzfahrtbetreiber ein.', 'Im Januar 2014 registrierte Prestige Cruises für ein erstes öffentliches Angebot.', 'Apollo war nach einem 850 Millionen Dollar-Vertrag im Jahr 2007 der Mehrheitsaktionär des Unternehmens.', 'Norwegian Cruise wurde im Jahr 2000 durch eine Fusion mit einem Kreuzfahrtunternehmen im Besitz von Genting Bhd (GENT.KL), dem Freizeit- und Casinokonzern, der von Malaysian Milliardär Lim Kok Thay kontrolliert wird, gegründet.', 'Im Jahr 2008 hat Apollo eine Milliarde Dollar in Norwegian Cruise investiert.']\n",
      "05/31 12:49:37 AM |\t  computing score...\n",
      "05/31 12:49:37 AM |\t  G_AB GAB sacreBLEU : 22.655786\n",
      "05/31 12:49:37 AM |\t  G_BA GBA sacreBLEU : 20.518123\n",
      "05/31 12:49:38 AM |\t  G_AB GAB test loss : 4.988615\n",
      "05/31 12:49:38 AM |\t  G_BA GBA test loss : 8.722591\n",
      "05/31 12:49:38 AM |\t  D_A DA test loss : -0.005615\n",
      "05/31 12:49:38 AM |\t  D_B DB test loss : 0.002703\n",
      "05/31 12:49:38 AM |\t  D_A DA test accuracy : 0.586538\n",
      "05/31 12:49:38 AM |\t  D_B DB test accuracy : 0.471154\n",
      "05/31 12:49:38 AM |\t  \n",
      "\n",
      "  ----------------epoch:0----------------\n",
      "05/31 12:49:38 AM |\t  total iter:[0] \t G_lr:5e-06 \t DA_lr:5e-05 \t DB_lr:5e-05\n",
      "05/31 12:50:35 AM |\t  {'GB_cycle_meter': 10.278919792175293, 'GA_cycle_meter': 10.215650749206542, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.002761552886416515, 'DB_meter': -0.00632886999907593, 'DA_GP_meter': 0.04199213503549496, 'DB_GP_meter': 0.042639475082978606}\n",
      "05/31 12:50:35 AM |\t  17.46031746031746%\n",
      "05/31 12:51:32 AM |\t  {'GB_cycle_meter': 10.27978515625, 'GA_cycle_meter': 10.247404257456461, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.007937021708736816, 'DB_meter': 0.005246844841167331, 'DA_GP_meter': 0.0204420640754203, 'DB_GP_meter': 0.03515520831570029}\n",
      "05/31 12:51:32 AM |\t  36.507936507936506%\n",
      "05/31 12:52:25 AM |\t  {'GB_cycle_meter': 10.285468419392904, 'GA_cycle_meter': 10.244673728942871, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.01846540082866947, 'DB_meter': -0.004398348663623135, 'DA_GP_meter': 0.019576602848246694, 'DB_GP_meter': 0.037503934310128294}\n",
      "05/31 12:52:25 AM |\t  55.55555555555556%\n",
      "05/31 12:53:20 AM |\t  {'GB_cycle_meter': 10.264051596323648, 'GA_cycle_meter': 10.275945822397867, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.008028187168141207, 'DB_meter': -0.006421793873111407, 'DA_GP_meter': 0.01723266455034415, 'DB_GP_meter': 0.02023056320225199}\n",
      "05/31 12:53:20 AM |\t  74.60317460317461%\n",
      "05/31 12:54:19 AM |\t  {'GB_cycle_meter': 10.255085309346518, 'GA_cycle_meter': 10.276927789052328, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.004826730582863092, 'DB_meter': 0.007296291451590757, 'DA_GP_meter': 0.01576533238403499, 'DB_GP_meter': 0.01530999131500721}\n",
      "05/31 12:54:19 AM |\t  93.65079365079364%\n",
      "05/31 12:54:36 AM |\t  DB_a_: 0.141,  0.138,  0.140,  0.159,  0.144,  0.096,  0.150,  0.075,  \n",
      "05/31 12:54:36 AM |\t  DB_pred_dis: 0.143,  0.150,  0.136,  0.160,  0.130,  0.135,  0.131,  0.058,  \n",
      "05/31 12:54:36 AM |\t  DA_b: 0.134,  0.106,  0.066,  0.110,  0.144,  0.114,  0.160,  0.117,  \n",
      "05/31 12:54:36 AM |\t  DA_pred_dis: 0.137,  0.095,  0.072,  0.106,  0.125,  0.124,  0.156,  0.127,  \n",
      "05/31 12:54:36 AM |\t  GABloss:\t5.742043495178223\n",
      "05/31 12:54:36 AM |\t  GBAloss:\t10.06333065032959\n",
      "05/31 12:54:36 AM |\t  a_decoded[:2]:['Prestige Cruises, also based in Miami, operates under the Oceania and Regent brands, which together have eight cruise ships traveling to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'It posted revenues of $1.2 billion in 2013, up 6 percent from the year earlier.', 'The $29 billion cruise industry is expected to benefit in the coming years from the rise of the middle class in emerging economies such as China and India.', 'Companies are racing to position themselves as the cruise operators of choice for these new customers.', 'Prestige Cruises registered with U.S. regulators for an initial public offering in January 2014.', \"Apollo has been the company's majority shareholder following an $850 million deal in 2007.\", 'Norwegian Cruise was created in its current form in 2000 through a merger with a cruise operator owned by Genting Bhd (GENT.KL), the leisure and casino conglomerate controlled by Malaysian billionaire Lim Kok Thay.', 'Apollo made a $1 billion investment in Norwegian Cruise in 2008.']\n",
      "05/31 12:54:36 AM |\t  pred_b_decoded[:2]:['The Prestige cruises, which was also in Miami, is based on the brand name Oceania and Regent, which jointly operate eight cross-border ships on routes to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'Revenue for 2013 was reported at USD 1.2 billion, an increase of 6% compared with last year.', 'In the next few years, the cross-border sector will be estimated at around USD 29 billion, and it will be expected in the light of the strengthening of the middle class in the swine countries such as China and India.', 'The companies are trying to position themselves as the best choice for these new customers in the cross-border operators.', 'In January 2014, Prestige Cruises requested the stock exchanges with the US regulatory authorities.', 'Apollo has been the majority shareholder in the company since a $ 850 million deal in 2007.', 'Norwegian cruise was able to escape its current form in 2000 by combining with a cruise operator, GENT.KL, a leisure and Caribbean cruiser under the control of the Malaysian billionaire Lim Kok Thay.', 'Apollo invested USD 1 billion in Norwegian cruise in 2008.']\n",
      "05/31 12:54:36 AM |\t  b_decoded[:2]:['Prestige Cruises, ebenfalls in Miami angesiedelt, agiert unter den Markennamen Oceania und Regent, die gemeinsam acht Kreuzfahrschiffe auf Routen nach Skandinavien, Russland, dem Mittelmeer, Nordamerika, Asien, Afrika und Südamerika betreiben.', 'Es meldete Einnahmen für 2013 von 1,2 Milliarden $, ein Anstieg von 6 Prozent im Vergleich zum Vorjahr.', 'Die Kreuzfahrtbranche liegt insgesamt bei etwa 29 Milliarden $ und in den nächsten Jahren wird für sie aufgrund der Stärkung der Mittelklasse in Schwellenländern wie China und Indien erwartet.', 'Die Unternehmen versuchen sich für diese neuen Kunden als die beste Wahl bei den Kreuzfahrtanbietern zu positionieren.', 'Prestige Cruises beantragte bei den US-Regulierungsbehörden im Januar 2014 den Börsengang.', 'Apollo ist der Mehrheitsaktionär bei dem Unternehmen seit einem 850 Millionen $ Deal 2007.', 'Norwegian Cruise entstand in seiner derzeitigen Form im Jahre 2000 durch eine Fusion mit einem Kreuzfahrtanbieter im Besitz von Genting Bhd (GENT.KL), einem Freizeit- und Kasinomischkonzern unter der Kontrolle des malaysischen Milliardärs Lim Kok Thay.', 'Apollo investierte 2008 1 Milliarde $ in Norwegian Cruise.']\n",
      "05/31 12:54:36 AM |\t  pred_a_decoded[:2]:['Prestige Cruises, auch mit Sitz in Miami, ist unter den Marken Oceania und Regent tätig, die zusammen acht Kreuzfahrtschiffe nach Skandinavien, Russland, dem Mittelmeerraum, Nordamerika, Asien, Afrika und Südamerika fahren.', 'Im Jahr 2013 erzielte es einen Umsatz von 1,2 Mrd. USD, was 6 Prozent höher ist als im Vorjahr.', 'Es wird erwartet, dass die Reiseindustrie in den kommenden Jahren von dem Aufstieg der Mittelschicht in Schwellenländern wie China und Indien profitieren wird.', 'Die Unternehmen setzen sich für diese neuen Kunden als die bevorzugten Kreuzfahrtbetreiber ein.', 'Im Januar 2014 registrierte Prestige Cruises für ein erstes öffentliches Angebot.', 'Apollo ist nach einem 850 Mio. USD-Vertrag im Jahr 2007 der Mehrheitsaktionär des Unternehmens.', 'Norwegian Cruise wurde im Jahr 2000 durch eine Fusion mit einem Kreuzfahrtunternehmen im Besitz von Genting Bhd (GENT.KL), dem Freizeit- und Casinokonzern, der von Malaysian Milliardär Lim Kok Thay kontrolliert wird.', 'Im Jahr 2008 hat Apollo eine Milliarde Dollar in Norwegian Cruise investiert.']\n",
      "05/31 12:55:01 AM |\t  computing score...\n",
      "05/31 12:55:01 AM |\t  G_AB GAB sacreBLEU : 23.243362\n",
      "05/31 12:55:01 AM |\t  G_BA GBA sacreBLEU : 20.667536\n",
      "05/31 12:55:01 AM |\t  G_AB GAB test loss : 5.121208\n",
      "05/31 12:55:01 AM |\t  G_BA GBA test loss : 8.945163\n",
      "05/31 12:55:01 AM |\t  D_A DA test loss : -0.003278\n",
      "05/31 12:55:01 AM |\t  D_B DB test loss : 0.000785\n",
      "05/31 12:55:01 AM |\t  D_A DA test accuracy : 0.615385\n",
      "05/31 12:55:01 AM |\t  D_B DB test accuracy : 0.461538\n",
      "05/31 12:55:05 AM |\t  \n",
      "\n",
      "  ----------------epoch:1----------------\n",
      "05/31 12:55:05 AM |\t  total iter:[504] \t G_lr:5e-06 \t DA_lr:5e-05 \t DB_lr:5e-05\n",
      "05/31 12:55:49 AM |\t  {'GB_cycle_meter': 10.285168965657553, 'GA_cycle_meter': 10.241054058074951, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.005861106212250888, 'DB_meter': 0.00495804528084894, 'DA_GP_meter': 0.013560932905723652, 'DB_GP_meter': 0.015441796975210309}\n",
      "05/31 12:55:49 AM |\t  12.698412698412698%\n",
      "05/31 12:56:47 AM |\t  {'GB_cycle_meter': 10.275767008463541, 'GA_cycle_meter': 10.272391637166342, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.0017058071680366993, 'DB_meter': -0.00203314326548328, 'DA_GP_meter': 0.010821919733037552, 'DB_GP_meter': 0.02876952321579059}\n",
      "05/31 12:56:47 AM |\t  31.746031746031743%\n",
      "05/31 12:57:42 AM |\t  {'GB_cycle_meter': 10.288142363230387, 'GA_cycle_meter': 10.232495466868082, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.01623622541471074, 'DB_meter': -0.00024028902407735586, 'DA_GP_meter': 0.010775593536285063, 'DB_GP_meter': 0.010382719182719788}\n",
      "05/31 12:57:42 AM |\t  50.79365079365079%\n",
      "05/31 12:58:40 AM |\t  {'GB_cycle_meter': 10.274340470631918, 'GA_cycle_meter': 10.249265829722086, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.0006446520953128735, 'DB_meter': -0.0013264463050290942, 'DA_GP_meter': 0.009815780678763986, 'DB_GP_meter': 0.007936862132434422}\n",
      "05/31 12:58:40 AM |\t  69.84126984126983%\n",
      "05/31 12:59:40 AM |\t  {'GB_cycle_meter': 10.291474342346191, 'GA_cycle_meter': 10.222499052683512, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.003533255308866501, 'DB_meter': 0.0020021539918767908, 'DA_GP_meter': 0.010480720472211639, 'DB_GP_meter': 0.0034695832582656294}\n",
      "05/31 12:59:40 AM |\t  88.88888888888889%\n",
      "05/31 01:00:04 AM |\t  DB_a_: 0.032,  0.035,  0.033,  0.032,  0.035,  0.033,  0.032,  0.036,  \n",
      "05/31 01:00:04 AM |\t  DB_pred_dis: 0.030,  0.035,  0.028,  0.031,  0.034,  0.033,  0.031,  0.037,  \n",
      "05/31 01:00:04 AM |\t  DA_b: 0.111,  0.083,  0.076,  0.090,  0.116,  0.096,  0.137,  0.093,  \n",
      "05/31 01:00:04 AM |\t  DA_pred_dis: 0.110,  0.077,  0.055,  0.083,  0.104,  0.104,  0.132,  0.105,  \n",
      "05/31 01:00:04 AM |\t  GABloss:\t5.935170650482178\n",
      "05/31 01:00:04 AM |\t  GBAloss:\t10.300734519958496\n",
      "05/31 01:00:04 AM |\t  a_decoded[:2]:['Prestige Cruises, also based in Miami, operates under the Oceania and Regent brands, which together have eight cruise ships traveling to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'It posted revenues of $1.2 billion in 2013, up 6 percent from the year earlier.', 'The $29 billion cruise industry is expected to benefit in the coming years from the rise of the middle class in emerging economies such as China and India.', 'Companies are racing to position themselves as the cruise operators of choice for these new customers.', 'Prestige Cruises registered with U.S. regulators for an initial public offering in January 2014.', \"Apollo has been the company's majority shareholder following an $850 million deal in 2007.\", 'Norwegian Cruise was created in its current form in 2000 through a merger with a cruise operator owned by Genting Bhd (GENT.KL), the leisure and casino conglomerate controlled by Malaysian billionaire Lim Kok Thay.', 'Apollo made a $1 billion investment in Norwegian Cruise in 2008.']\n",
      "05/31 01:00:04 AM |\t  pred_b_decoded[:2]:['Prestige Cruises, also based in Miami, is based on the brand name Oceania and Regent, which jointly operate eight cross-border ships on routes to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'Revenue for 2013 was reported to be USD 1.2 billion, an increase of 6% compared with last year.', 'In the next few years, the cross-border sector will be estimated at around USD 29 billion, and it will be expected in the light of the strengthening of the mid-class in countries such as China and India.', 'The companies are trying to position themselves as the best choice for these new customers in the cross-border operators.', 'Prestige Cruises requested the stock exchange in January 2014.', 'Apollo has been the majority shareholder in the company since a $ 850 million deal in 2007.', 'Norwegian cruise was able to take off in its current form in 2000 by combining with a cruise operator, GENT.KL, a leisure and Caribbean cruiser under the control of the Malaysian billionaire Lim Kok Thay.', 'Apollo invested USD 1 billion in Norwegian cruise in 2008.']\n",
      "05/31 01:00:04 AM |\t  b_decoded[:2]:['Prestige Cruises, ebenfalls in Miami angesiedelt, agiert unter den Markennamen Oceania und Regent, die gemeinsam acht Kreuzfahrschiffe auf Routen nach Skandinavien, Russland, dem Mittelmeer, Nordamerika, Asien, Afrika und Südamerika betreiben.', 'Es meldete Einnahmen für 2013 von 1,2 Milliarden $, ein Anstieg von 6 Prozent im Vergleich zum Vorjahr.', 'Die Kreuzfahrtbranche liegt insgesamt bei etwa 29 Milliarden $ und in den nächsten Jahren wird für sie aufgrund der Stärkung der Mittelklasse in Schwellenländern wie China und Indien erwartet.', 'Die Unternehmen versuchen sich für diese neuen Kunden als die beste Wahl bei den Kreuzfahrtanbietern zu positionieren.', 'Prestige Cruises beantragte bei den US-Regulierungsbehörden im Januar 2014 den Börsengang.', 'Apollo ist der Mehrheitsaktionär bei dem Unternehmen seit einem 850 Millionen $ Deal 2007.', 'Norwegian Cruise entstand in seiner derzeitigen Form im Jahre 2000 durch eine Fusion mit einem Kreuzfahrtanbieter im Besitz von Genting Bhd (GENT.KL), einem Freizeit- und Kasinomischkonzern unter der Kontrolle des malaysischen Milliardärs Lim Kok Thay.', 'Apollo investierte 2008 1 Milliarde $ in Norwegian Cruise.']\n",
      "05/31 01:00:04 AM |\t  pred_a_decoded[:2]:['Prestige Cruises, auch mit Sitz in Miami, ist unter den Marken Oceania und Regent tätig, die zusammen acht Kreuzfahrtschiffe nach Skandinavien, Russland, dem Mittelmeerraum, Nordamerika, Asien, Afrika und Südamerika fahren.', 'Im Jahr 2013 erzielte es einen Umsatz von 1,2 Mrd. USD, was 6 Prozent höher ist als im Vorjahr.', 'Es wird erwartet, dass die Reiseindustrie in den kommenden Jahren von dem Aufstieg der Mittelschicht in Schwellenländern wie China und Indien profitieren wird.', 'Die Unternehmen setzen sich für diese neuen Kunden als die bevorzugten Kreuzfahrtbetreiber ein.', 'Im Januar 2014 hat Prestige Cruises bei den US-Regulierungsbehörden ein erstes öffentliches Angebot angekündigt.', 'Apollo ist nach einem 850 Mio. USD-Vertrag im Jahr 2007 der Mehrheitsaktionär des Unternehmens.', 'Norwegian Cruise wurde im Jahr 2000 durch eine Fusion mit einem Kreuzfahrtunternehmen im Besitz von Genting Bhd (GENT.KL), dem Freizeit- und Kasinokonzern, der von Malaysian Milliardär Lim Kok Thay kontrolliert wird.', 'Im Jahr 2008 hat Apollo eine Milliarde Dollar in Norwegian Cruise investiert.']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\GitCode\\cycleMT\\main.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/GitCode/cycleMT/main.ipynb#ch0000007?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mepochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/GitCode/cycleMT/main.ipynb#ch0000007?line=5'>6</a>\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m  ----------------epoch:\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m----------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/GitCode/cycleMT/main.ipynb#ch0000007?line=6'>7</a>\u001b[0m     my_train(train_dataloader,cycleGAN,total_iter,args,logging,valid_dataloader,tokenizer,wandb)\n",
      "File \u001b[1;32mg:\\GitCode\\cycleMT\\train.py:60\u001b[0m, in \u001b[0;36mmy_train\u001b[1;34m(loader, model, total_iter, args, logging, valid_loader, tokenizer, wandb)\u001b[0m\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/train.py?line=56'>57</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mG_BA,os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(wandb\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mdir, \u001b[39m\"\u001b[39m\u001b[39mG_BA.pt\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/train.py?line=57'>58</a>\u001b[0m wandb\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39m./files/*.pt\u001b[39m\u001b[39m\"\u001b[39m, base_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./files\u001b[39m\u001b[39m\"\u001b[39m, policy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlive\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='file:///g%3A/GitCode/cycleMT/train.py?line=59'>60</a>\u001b[0m my_test(valid_loader,model,tokenizer,logging,wandb)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mg:\\GitCode\\cycleMT\\test.py:37\u001b[0m, in \u001b[0;36mmy_test\u001b[1;34m(loader, model, tokenizer, logging, wandb)\u001b[0m\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/test.py?line=32'>33</a>\u001b[0m b \u001b[39m=\u001b[39m Variable(batch[\u001b[39m2\u001b[39m], requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mto(device, non_blocking\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m#de    \u001b[39;00m\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/test.py?line=33'>34</a>\u001b[0m b_attn \u001b[39m=\u001b[39m Variable(batch[\u001b[39m3\u001b[39m], requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mto(device, non_blocking\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> <a href='file:///g%3A/GitCode/cycleMT/test.py?line=36'>37</a>\u001b[0m a_generate \u001b[39m=\u001b[39m GAB\u001b[39m.\u001b[39;49mtest_generate(a)[:,\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/test.py?line=37'>38</a>\u001b[0m b_generate  \u001b[39m=\u001b[39m GBA\u001b[39m.\u001b[39mtest_generate(b)[:,\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/test.py?line=39'>40</a>\u001b[0m GAB_loss \u001b[39m=\u001b[39m GAB\u001b[39m.\u001b[39mforward(a,a_attn,b,b_attn)\u001b[39m.\u001b[39mloss\n",
      "File \u001b[1;32mg:\\GitCode\\cycleMT\\basic_model.py:172\u001b[0m, in \u001b[0;36mG.test_generate\u001b[1;34m(self, x, num_beams, max_length)\u001b[0m\n\u001b[0;32m    <a href='file:///g%3A/GitCode/cycleMT/basic_model.py?line=169'>170</a>\u001b[0m prefix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenzied_prefix\u001b[39m.\u001b[39mrepeat(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39m1\u001b[39m)\u001b[39m#.cuda()\u001b[39;00m\n\u001b[0;32m    <a href='file:///g%3A/GitCode/cycleMT/basic_model.py?line=170'>171</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mhstack((prefix,x))\n\u001b[1;32m--> <a href='file:///g%3A/GitCode/cycleMT/basic_model.py?line=171'>172</a>\u001b[0m output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate( input_ids \u001b[39m=\u001b[39;49m x, num_beams \u001b[39m=\u001b[39;49m num_beams, early_stopping \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, max_length \u001b[39m=\u001b[39;49m max_length, length_penalty \u001b[39m=\u001b[39;49m\u001b[39m0.6\u001b[39;49m, repetition_penalty \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    <a href='file:///g%3A/GitCode/cycleMT/basic_model.py?line=172'>173</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output_ids\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\transformers\\generation_utils.py:1315\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1310'>1311</a>\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1311'>1312</a>\u001b[0m         input_ids, expand_size\u001b[39m=\u001b[39mnum_beams, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1312'>1313</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1313'>1314</a>\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1314'>1315</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1315'>1316</a>\u001b[0m         input_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1316'>1317</a>\u001b[0m         beam_scorer,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1317'>1318</a>\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1318'>1319</a>\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1319'>1320</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1320'>1321</a>\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1321'>1322</a>\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1322'>1323</a>\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1323'>1324</a>\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1324'>1325</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1325'>1326</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1327'>1328</a>\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1328'>1329</a>\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1329'>1330</a>\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1330'>1331</a>\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, typical_p\u001b[39m=\u001b[39mtypical_p, temperature\u001b[39m=\u001b[39mtemperature, num_beams\u001b[39m=\u001b[39mnum_beams\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1331'>1332</a>\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\transformers\\generation_utils.py:2158\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2153'>2154</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2155'>2156</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2157'>2158</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2158'>2159</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2159'>2160</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2160'>2161</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2161'>2162</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2162'>2163</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2164'>2165</a>\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2165'>2166</a>\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1638\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1634'>1635</a>\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1636'>1637</a>\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1637'>1638</a>\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1638'>1639</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1639'>1640</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1640'>1641</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1641'>1642</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1642'>1643</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1643'>1644</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1644'>1645</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1645'>1646</a>\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1646'>1647</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1647'>1648</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1648'>1649</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1649'>1650</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1650'>1651</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1652'>1653</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1654'>1655</a>\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1033\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1019'>1020</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1020'>1021</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1021'>1022</a>\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1029'>1030</a>\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1030'>1031</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1031'>1032</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1032'>1033</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1033'>1034</a>\u001b[0m         hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1034'>1035</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1035'>1036</a>\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1036'>1037</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1037'>1038</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1038'>1039</a>\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1039'>1040</a>\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1040'>1041</a>\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1041'>1042</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1042'>1043</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1043'>1044</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1044'>1045</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1046'>1047</a>\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1047'>1048</a>\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:720\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=716'>717</a>\u001b[0m     attention_outputs \u001b[39m=\u001b[39m attention_outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m2\u001b[39m:]\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=718'>719</a>\u001b[0m \u001b[39m# Apply Feed Forward layer\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=719'>720</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_states)\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=721'>722</a>\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=722'>723</a>\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16 \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39misinf(hidden_states)\u001b[39m.\u001b[39many():\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:329\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=326'>327</a>\u001b[0m forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=327'>328</a>\u001b[0m forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDenseReluDense(forwarded_states)\n\u001b[1;32m--> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=328'>329</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(forwarded_states)\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=329'>330</a>\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if(args.valid_begin==1):\n",
    "    my_test(valid_dataloader,cycleGAN,tokenizer,logging,wandb)\n",
    "total_iter = [0]  \n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch}----------------\")\n",
    "    my_train(train_dataloader,cycleGAN,total_iter,args,logging,valid_dataloader,tokenizer,wandb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2a958cf2c5c0cd3cfa7593bb1f22c814db6e88adb2853f1b0eeb9f68d33d3cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cycle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
