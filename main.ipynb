{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.test_iter 498\n",
      "args.rep_iter 99\n"
     ]
    }
   ],
   "source": [
    "if(True):\n",
    "    parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "    parser.add_argument('--valid_num_points', type=int,             default = 100, help='validation data number')\n",
    "    parser.add_argument('--train_num_points', type=int,             default = 500, help='train data number')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int,                   default=3,     help='Batch size')\n",
    "    parser.add_argument('--max_length', type=int,                   default=512,     help='max_length')\n",
    "\n",
    "    parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "    parser.add_argument('--G_AB_model_name', type=str,              default='t5-small',      help='model_name')\n",
    "    parser.add_argument('--G_BA_model_name', type=str,              default='Onlydrinkwater/T5-small-de-en',      help='model_name')\n",
    "    parser.add_argument('--D_A_model_name', type=str,               default='t5-small',      help='model_name')\n",
    "    parser.add_argument('--D_B_model_name', type=str,               default='Onlydrinkwater/T5-small-de-en',      help='model_name')\n",
    "    parser.add_argument('--exp_name', type=str,                     default='CYCLE!',      help='experiment name')\n",
    "    parser.add_argument('--rep_iter', type=int,                     default=100,      help='report times for 1 epoch')\n",
    "    parser.add_argument('--test_iter', type=int,                    default=500,      help='report times for 1 epoch')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int,                       default=50,     help='num of training epochs')\n",
    "\n",
    "    parser.add_argument('--G_lr', type=float,                       default=0.00001,   help='learning rate for G')\n",
    "    parser.add_argument('--G_weight_decay', type=float,             default=1e-3,   help='learning de for G')\n",
    "    parser.add_argument('--D_lr', type=float,                       default=0.0001,   help='learning rate for D')\n",
    "    parser.add_argument('--D_weight_decay', type=float,             default=1e-3,   help='learning de for D')\n",
    "    parser.add_argument('--lambda_identity', type=float,            default=0.5,   help='')\n",
    "    parser.add_argument('--lambda_A', type=float,                   default=0,   help='')\n",
    "    parser.add_argument('--lambda_B', type=float,                   default=0,   help='')\n",
    "    parser.add_argument('--lambda_once', type=float,                default=1,   help='')\n",
    "    parser.add_argument('--smoothing', type=float,                  default=0.1,    help='labelsmoothing')\n",
    "\n",
    "\n",
    "    parser.add_argument('--load_D', type=int,                       default=1,      help='load pretrained D')\n",
    "    parser.add_argument('--valid_begin', type=int,                  default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--train_G', type=int,                      default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--train_D', type=int,                      default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--D_pretrain_iter', type=int,              default=0,      help='whether valid before train')\n",
    "\n",
    "\n",
    "    args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "    args.test_iter = args.test_iter//args.batch_size * args.batch_size\n",
    "    args.rep_iter = args.rep_iter//args.batch_size * args.batch_size\n",
    "    print('args.test_iter',args.test_iter)\n",
    "    print('args.rep_iter',args.rep_iter)#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1nkqc4sh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99906a1eaaf343ed9a316de7603d321f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>DA_meter</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>DB_meter</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GAB sacreBLEU</td><td>██▂▁▁</td></tr><tr><td>GAB test loss</td><td>█▃▁▂▂</td></tr><tr><td>GAB_once_meter</td><td>█████████▇▆▄▃▃▂▂▂▂▃▂▂▂▂▁▁</td></tr><tr><td>GA_cycle_meter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GBA sacreBLEU</td><td>███▃▁</td></tr><tr><td>GBA test loss</td><td>▃▁▂▁█</td></tr><tr><td>GBA_once_meter</td><td>▇▆▅▆▅▅▂▇▁▆▄▇▅▄▄▄▁▇▆▆▆▅▆▆█</td></tr><tr><td>GB_cycle_meter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>DA_meter</td><td>0</td></tr><tr><td>DB_meter</td><td>0</td></tr><tr><td>GAB sacreBLEU</td><td>0.01651</td></tr><tr><td>GAB test loss</td><td>6.74105</td></tr><tr><td>GAB_once_meter</td><td>0.49979</td></tr><tr><td>GA_cycle_meter</td><td>0.0</td></tr><tr><td>GBA sacreBLEU</td><td>0.08352</td></tr><tr><td>GBA test loss</td><td>14.56278</td></tr><tr><td>GBA_once_meter</td><td>0.96583</td></tr><tr><td>GB_cycle_meter</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">CYCLE!</strong>: <a href=\"https://wandb.ai/onlydrinkwater/CYCLEGAN/runs/1nkqc4sh\" target=\"_blank\">https://wandb.ai/onlydrinkwater/CYCLEGAN/runs/1nkqc4sh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220428_112244-1nkqc4sh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1nkqc4sh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\GitCode\\cycleMT\\wandb\\run-20220428_120418-2pfbq7by</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/CYCLEGAN/runs/2pfbq7by\" target=\"_blank\">CYCLE!</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/CYCLEGAN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/CYCLEGAN/runs/2pfbq7by?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x24177ec0d90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_API_KEY']='a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME']=args.exp_name\n",
    "wandb.init(project=\"CYCLEGAN\",config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/28 12:04:36 PM |\t  Namespace(D_A_model_name='t5-small', D_B_model_name='Onlydrinkwater/T5-small-de-en', D_lr=0.0001, D_pretrain_iter=1000, D_weight_decay=0.001, G_AB_model_name='t5-small', G_BA_model_name='Onlydrinkwater/T5-small-de-en', G_lr=1e-05, G_weight_decay=0.001, batch_size=3, epochs=50, exp_name='CYCLE!', gpu=0, lambda_A=0, lambda_B=0, lambda_identity=0.5, lambda_once=1, load_D=0, max_length=512, rep_iter=99, smoothing=0.1, test_iter=498, train_D=1, train_G=1, train_num_points=500, valid_begin=1, valid_num_points=100)\n"
     ]
    }
   ],
   "source": [
    "#logging file\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'),'w',encoding = \"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "logging.info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/28 12:04:42 PM |\t  Gmodelsize:60.506624MB\n",
      "04/28 12:04:42 PM |\t  Dmodelsize:60.506624MB\n"
     ]
    }
   ],
   "source": [
    "GABmodelname = args.G_AB_model_name\n",
    "GBAmodelname = args.G_BA_model_name\n",
    "DAmodelname = args.D_A_model_name\n",
    "DBmodelname = args.D_B_model_name\n",
    "GABpretrained  =  AutoModelForSeq2SeqLM.from_pretrained(GABmodelname)\n",
    "GBApretrained  =  AutoModelForSeq2SeqLM.from_pretrained(GBAmodelname)\n",
    "DApretrained  =  AutoModelForSeq2SeqLM.from_pretrained(DAmodelname)\n",
    "DBpretrained  =  AutoModelForSeq2SeqLM.from_pretrained(DBmodelname)\n",
    "logging.info(f'Gmodelsize:{count_parameters_in_MB(GABpretrained)}MB')\n",
    "logging.info(f'Dmodelsize:{count_parameters_in_MB(DApretrained)}MB')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(GABmodelname)\n",
    "# tokenizerBA = AutoTokenizer.from_pretrained(GBAmodelname)#its the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/28 12:04:46 PM |\t  Reusing dataset wmt16 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt16\\de-en\\1.0.0\\0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 14.39it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('wmt16','de-en')\n",
    "train = dataset['train']['translation'][:args.train_num_points]\n",
    "valid = dataset['train']['translation'][args.train_num_points:(args.train_num_points+args.valid_num_points)]\n",
    "\n",
    "\n",
    "train_data = get_Dataset_chaos(train, tokenizer,max_length=args.max_length)\n",
    "train_dataloader = DataLoader(train_data, sampler= RandomSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=True, num_workers=2)\n",
    "valid_data = get_Dataset(valid, tokenizer,max_length=args.max_length)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=args.batch_size, pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycleGAN = CycleGAN(args,GABpretrained,GBApretrained,DApretrained,DBpretrained,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/28 11:24:06 AM |\t  DA_a: tensor([0.], device='cuda:0')tensor([0.0092], device='cuda:0')tensor([0.], device='cuda:0')\n",
      "04/28 11:24:06 AM |\t  DB_pred_dis: tensor([0.], device='cuda:0')tensor([0.0361], device='cuda:0')tensor([0.0351], device='cuda:0')\n",
      "04/28 11:24:06 AM |\t  DB_b: tensor([0.], device='cuda:0')tensor([0.0361], device='cuda:0')tensor([0.0351], device='cuda:0')\n",
      "04/28 11:24:06 AM |\t  DA_pred_dis: tensor([0.0095], device='cuda:0')tensor([0.], device='cuda:0')tensor([0.], device='cuda:0')\n",
      "04/28 11:24:06 AM |\t  GABloss:\t7.180871486663818\n",
      "04/28 11:24:06 AM |\t  GBAloss:\t12.750386238098145\n",
      "04/28 11:24:06 AM |\t  a_decoded[:2]:['Past experience dictates that, as the elected representatives of the European taxpayer, we should, and indeed must, demand financial probity and transparency in the disbursement and auditing of this money, hence our amendments and additions relate to achieving what are known as \"value for money\" indicators in the grant-giving process.', 'Next, we all too often see vast sums of money being spent on projects whose outcomes will necessarily be unclear at the start of the programme period.']\n",
      "04/28 11:24:06 AM |\t  pred_b_decoded[:2]:['The experience of the past shows that we, as elected representatives of the European taxpayers, should demand financial redundancy and transparency in the payment of these funds and the auditing that is involved, and with our amendments and contributions we want to achieve what can be used as an indicator of the most economically favourable solution in the grant of subsidies.', 'Secondly, all too often, huge sums are being spent on projects whose results are simply not clearly assessed at the beginning of the programme period.']\n",
      "04/28 11:24:06 AM |\t  b_decoded[:2]:['Die Erfahrungen der Vergangenheit zeigen, daß wir als die gewählten Vertreter der europäischen Steuerzahler finanzielle Redlichkeit und Transparenz bei der Auszahlung dieser Gelder und der damit verbundenen Rechnungsprüfung fordern sollten, ja müssen. Mit unseren nderungen und Zusätzen wollen wir das erreichen, was bei der Gewährung von Zuschüssen als Indikator für die wirtschaftlich günstigste Lösung dienen kann.', 'Zweitens fließen nur allzu oft riesige Summen in Projekte, deren Ergebnisse sich zu Beginn des Programmzeitraums einfach noch nicht klar abschätzen lassen.']\n",
      "04/28 11:24:06 AM |\t  pred_a_decoded[:2]:['Die Erfahrung der Vergangenheit legt fest, dass wir als gewählte Vertreter des europäischen Steuerzahlers finanzielle Besonnenheit und Transparenz bei der Auszahlung und Prüfung dieses Geldes fordern sollten und müssen. Deshalb beziehen sich unsere nderungsanträge und Ergänzungen auf die Erreichung der so genannten \"Wert-für-Geld-Indikatoren\" im Förderverfahren.', 'Als nächstes sehen wir allzu oft riesige Summen für Projekte ausgegeben, deren Ergebnisse zu Beginn des Programmzeitraums notwendigerweise unklar sein werden.']\n",
      "04/28 11:25:14 AM |\t  computing score...\n",
      "04/28 11:25:14 AM |\t  G_AB GAB sacreBLEU : 18.128650\n",
      "04/28 11:25:14 AM |\t  G_BA GBA sacreBLEU : 22.972780\n",
      "04/28 11:25:14 AM |\t  G_AB GAB test loss : 8.635276\n",
      "04/28 11:25:14 AM |\t  G_BA GBA test loss : 11.878961\n",
      "04/28 11:25:14 AM |\t  \n",
      "\n",
      "  ----------------epoch:0----------------\n",
      "04/28 11:25:14 AM |\t  total iter:[0]\n",
      "04/28 11:25:57 AM |\t  {'DA_meter': 0.3192941438068043, 'DB_meter': 0.2906152168006608}\n",
      "04/28 11:25:57 AM |\t  19.16167664670659%\n",
      "04/28 11:26:40 AM |\t  {'DA_meter': 0.06703538216198936, 'DB_meter': 0.11267963390458714}\n",
      "04/28 11:26:40 AM |\t  38.92215568862276%\n",
      "04/28 11:27:25 AM |\t  {'DA_meter': 0.03228381444050959, 'DB_meter': 0.06046007330896276}\n",
      "04/28 11:27:25 AM |\t  58.68263473053892%\n",
      "04/28 11:28:12 AM |\t  {'DA_meter': 0.009822185324138087, 'DB_meter': 0.034411475245812624}\n",
      "04/28 11:28:12 AM |\t  78.44311377245509%\n",
      "04/28 11:28:58 AM |\t  {'DA_meter': 0.014138611139390956, 'DB_meter': 0.023460226013522708}\n",
      "04/28 11:28:58 AM |\t  98.20359281437125%\n",
      "04/28 11:29:00 AM |\t  \n",
      "\n",
      "  ----------------epoch:1----------------\n",
      "04/28 11:29:00 AM |\t  total iter:[501]\n",
      "04/28 11:29:41 AM |\t  {'DA_meter': 0.005888696518289207, 'DB_meter': 0.019607513081141267}\n",
      "04/28 11:29:41 AM |\t  17.964071856287426%\n",
      "04/28 11:30:23 AM |\t  {'DA_meter': 0.005819323801640139, 'DB_meter': 0.016153786165611535}\n",
      "04/28 11:30:23 AM |\t  37.72455089820359%\n",
      "04/28 11:31:06 AM |\t  {'DA_meter': 0.00408719303650838, 'DB_meter': 0.010681811827728807}\n",
      "04/28 11:31:06 AM |\t  57.48502994011976%\n",
      "04/28 11:31:50 AM |\t  {'DA_meter': 0.006498001116790075, 'DB_meter': 0.008771657020340419}\n",
      "04/28 11:31:50 AM |\t  77.24550898203593%\n",
      "04/28 11:32:33 AM |\t  {'DA_meter': 0.001735534394604408, 'DB_meter': 0.01379747592815847}\n",
      "04/28 11:32:33 AM |\t  97.0059880239521%\n",
      "04/28 11:32:39 AM |\t  \n",
      "\n",
      "  ----------------epoch:2----------------\n",
      "04/28 11:32:39 AM |\t  total iter:[1002]\n",
      "04/28 11:33:20 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.9793747385342916, 'GBA_once_meter': 0.9607283194859823, 'DA_meter': 0.005198383393387, 'DB_meter': 0.004358888138085604}\n",
      "04/28 11:33:20 AM |\t  16.766467065868262%\n",
      "04/28 11:34:06 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.9750400185585022, 'GBA_once_meter': 0.9549162658778104, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:34:06 AM |\t  36.52694610778443%\n",
      "04/28 11:34:51 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.9784995046528903, 'GBA_once_meter': 0.9451212395321239, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:34:51 AM |\t  56.287425149700596%\n",
      "04/28 11:35:36 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.9803947192249876, 'GBA_once_meter': 0.9541110288013112, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:35:36 AM |\t  76.04790419161677%\n",
      "04/28 11:36:22 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.9773285063830289, 'GBA_once_meter': 0.9450388659130443, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:36:22 AM |\t  95.80838323353294%\n",
      "04/28 11:36:33 AM |\t  DA_a: tensor([0.8490], device='cuda:0')tensor([0.9236], device='cuda:0')tensor([0.9590], device='cuda:0')\n",
      "04/28 11:36:33 AM |\t  DB_pred_dis: tensor([0.1873], device='cuda:0')tensor([0.0094], device='cuda:0')tensor([0.9478], device='cuda:0')\n",
      "04/28 11:36:33 AM |\t  DB_b: tensor([0.1873], device='cuda:0')tensor([0.0094], device='cuda:0')tensor([0.9478], device='cuda:0')\n",
      "04/28 11:36:33 AM |\t  DA_pred_dis: tensor([0.5566], device='cuda:0')tensor([0.2726], device='cuda:0')tensor([0.9484], device='cuda:0')\n",
      "04/28 11:36:33 AM |\t  GABloss:\t6.6747918128967285\n",
      "04/28 11:36:33 AM |\t  GBAloss:\t11.621590614318848\n",
      "04/28 11:36:33 AM |\t  a_decoded[:2]:['Past experience dictates that, as the elected representatives of the European taxpayer, we should, and indeed must, demand financial probity and transparency in the disbursement and auditing of this money, hence our amendments and additions relate to achieving what are known as \"value for money\" indicators in the grant-giving process.', 'Next, we all too often see vast sums of money being spent on projects whose outcomes will necessarily be unclear at the start of the programme period.']\n",
      "04/28 11:36:33 AM |\t  pred_b_decoded[:2]:['The experience of the past shows that we, as elected representatives of the European taxpayers, should demand financial redundancy and transparency in the payment of these funds and the associated auditing, and, with our amendments and contributions, we want to achieve what can be achieved by granting grants as an indicator of the most economically favourable solution.', 'Secondly, all too often, huge sums are spent on projects whose results, at the beginning of the programme period, are simply not yet clear.']\n",
      "04/28 11:36:33 AM |\t  b_decoded[:2]:['Die Erfahrungen der Vergangenheit zeigen, daß wir als die gewählten Vertreter der europäischen Steuerzahler finanzielle Redlichkeit und Transparenz bei der Auszahlung dieser Gelder und der damit verbundenen Rechnungsprüfung fordern sollten, ja müssen. Mit unseren nderungen und Zusätzen wollen wir das erreichen, was bei der Gewährung von Zuschüssen als Indikator für die wirtschaftlich günstigste Lösung dienen kann.', 'Zweitens fließen nur allzu oft riesige Summen in Projekte, deren Ergebnisse sich zu Beginn des Programmzeitraums einfach noch nicht klar abschätzen lassen.']\n",
      "04/28 11:36:33 AM |\t  pred_a_decoded[:2]:['Die Erfahrung der Vergangenheit legt fest, daß wir als gewählte Vertreter des europäischen Steuerzahlers finanzielle Besonnenheit und Transparenz bei der Auszahlung und Prüfung dieses Geldes fordern sollten und müssen. Deshalb haben wir unsere nderungsanträge und Ergänzungen zur Erreichung der sogenannten \"Wert-für-Geld-Indikatoren\" im Rahmen des Zuschussverfahrens vorgelegt.', 'Wir haben es nämlich gesehen, daß wir es nämlich nicht schaffen können, daß wir es nicht schaffen können, daß wir es nicht schaffen können, daß wir es nicht schaffen können, daß wir es nicht schaffen können, daß wir es nicht schaffen können, daß wir es nicht schaffen können, daß wir es schaffen können, daß wir']\n",
      "04/28 11:37:43 AM |\t  computing score...\n",
      "04/28 11:37:43 AM |\t  G_AB GAB sacreBLEU : 17.173943\n",
      "04/28 11:37:43 AM |\t  G_BA GBA sacreBLEU : 24.487756\n",
      "04/28 11:37:43 AM |\t  G_AB GAB test loss : 7.212710\n",
      "04/28 11:37:43 AM |\t  G_BA GBA test loss : 11.066635\n",
      "04/28 11:37:47 AM |\t  \n",
      "\n",
      "  ----------------epoch:3----------------\n",
      "04/28 11:37:47 AM |\t  total iter:[1503]\n",
      "04/28 11:38:26 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.991920946222363, 'GBA_once_meter': 0.9468039671579996, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:38:26 AM |\t  15.568862275449103%\n",
      "04/28 11:39:11 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.990258048881184, 'GBA_once_meter': 0.9280492157647104, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:39:11 AM |\t  35.32934131736527%\n",
      "04/28 11:39:57 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.9780734217528141, 'GBA_once_meter': 0.959181157025424, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:39:57 AM |\t  55.08982035928144%\n",
      "04/28 11:40:42 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.9741110223712344, 'GBA_once_meter': 0.9217482552383885, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:40:42 AM |\t  74.8502994011976%\n",
      "04/28 11:41:28 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.9539170445817889, 'GBA_once_meter': 0.9557250087911432, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:41:28 AM |\t  94.61077844311377%\n",
      "04/28 11:41:41 AM |\t  DA_a: tensor([0.8735], device='cuda:0')tensor([0.9632], device='cuda:0')tensor([0.9250], device='cuda:0')\n",
      "04/28 11:41:41 AM |\t  DB_pred_dis: tensor([0.0217], device='cuda:0')tensor([0.0200], device='cuda:0')tensor([0.0351], device='cuda:0')\n",
      "04/28 11:41:41 AM |\t  DB_b: tensor([0.0217], device='cuda:0')tensor([0.0200], device='cuda:0')tensor([0.0351], device='cuda:0')\n",
      "04/28 11:41:41 AM |\t  DA_pred_dis: tensor([0.3807], device='cuda:0')tensor([0.2011], device='cuda:0')tensor([0.1889], device='cuda:0')\n",
      "04/28 11:41:41 AM |\t  GABloss:\t5.738551139831543\n",
      "04/28 11:41:41 AM |\t  GBAloss:\t12.330987930297852\n",
      "04/28 11:41:41 AM |\t  a_decoded[:2]:['Past experience dictates that, as the elected representatives of the European taxpayer, we should, and indeed must, demand financial probity and transparency in the disbursement and auditing of this money, hence our amendments and additions relate to achieving what are known as \"value for money\" indicators in the grant-giving process.', 'Next, we all too often see vast sums of money being spent on projects whose outcomes will necessarily be unclear at the start of the programme period.']\n",
      "04/28 11:41:41 AM |\t  pred_b_decoded[:2]:['The experience of the past shows that we, as the elected representatives of the European taxpayers, should be able to demand financial redundancy and transparency in the payment of these funds and the corresponding auditing, and, with our amendments and quotas, we want to achieve what can be achieved by granting grants as an indicator of the most economically favourable solution.', 'Secondly, all too often, huge sums are spent on projects whose results, at the beginning of the programming period, are simply not clear.']\n",
      "04/28 11:41:41 AM |\t  b_decoded[:2]:['Die Erfahrungen der Vergangenheit zeigen, daß wir als die gewählten Vertreter der europäischen Steuerzahler finanzielle Redlichkeit und Transparenz bei der Auszahlung dieser Gelder und der damit verbundenen Rechnungsprüfung fordern sollten, ja müssen. Mit unseren nderungen und Zusätzen wollen wir das erreichen, was bei der Gewährung von Zuschüssen als Indikator für die wirtschaftlich günstigste Lösung dienen kann.', 'Zweitens fließen nur allzu oft riesige Summen in Projekte, deren Ergebnisse sich zu Beginn des Programmzeitraums einfach noch nicht klar abschätzen lassen.']\n",
      "04/28 11:41:41 AM |\t  pred_a_decoded[:2]:['Wir wissen, daß wir wissen müssen, daß wir wissen müssen, daß wir wissen müssen, daß wir wissen müssen, daß wir wissen müssen, daß wir wissen müssen, daß wir uns bemühen müssen, daß wir uns bemühen müssen, daß wir uns bemühen müssen, daß wir uns bemühen müssen, daß wir uns bemühen müssen, daß wir uns bemühen müssen, daß wir uns bemühen müssen, daß wir uns', 'Wir werden uns freuen freuen, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden, dass wir uns freuen werden']\n",
      "04/28 11:43:04 AM |\t  computing score...\n",
      "04/28 11:43:04 AM |\t  G_AB GAB sacreBLEU : 1.481615\n",
      "04/28 11:43:04 AM |\t  G_BA GBA sacreBLEU : 24.250082\n",
      "04/28 11:43:04 AM |\t  G_AB GAB test loss : 6.559374\n",
      "04/28 11:43:04 AM |\t  G_BA GBA test loss : 11.495312\n",
      "04/28 11:43:09 AM |\t  \n",
      "\n",
      "  ----------------epoch:4----------------\n",
      "04/28 11:43:09 AM |\t  total iter:[2004]\n",
      "04/28 11:43:45 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.8417724641886625, 'GBA_once_meter': 0.9401715506206859, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:43:45 AM |\t  14.37125748502994%\n",
      "04/28 11:44:31 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.678148890986587, 'GBA_once_meter': 0.9595860333153696, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:44:31 AM |\t  34.13173652694611%\n",
      "04/28 11:45:16 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.6471531662074003, 'GBA_once_meter': 0.9439751090425433, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:45:16 AM |\t  53.89221556886228%\n",
      "04/28 11:46:00 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.6058927268692942, 'GBA_once_meter': 0.9419825329925074, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:46:00 AM |\t  73.65269461077844%\n",
      "04/28 11:46:46 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.589700951720729, 'GBA_once_meter': 0.9395659602049625, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:46:46 AM |\t  93.41317365269461%\n",
      "04/28 11:47:06 AM |\t  DA_a: tensor([0.8005], device='cuda:0')tensor([0.9338], device='cuda:0')tensor([0.9191], device='cuda:0')\n",
      "04/28 11:47:06 AM |\t  DB_pred_dis: tensor([0.], device='cuda:0')tensor([0.0319], device='cuda:0')tensor([0.], device='cuda:0')\n",
      "04/28 11:47:06 AM |\t  DB_b: tensor([0.], device='cuda:0')tensor([0.0319], device='cuda:0')tensor([0.], device='cuda:0')\n",
      "04/28 11:47:06 AM |\t  DA_pred_dis: tensor([0.3223], device='cuda:0')tensor([0.4132], device='cuda:0')tensor([0.3191], device='cuda:0')\n",
      "04/28 11:47:06 AM |\t  GABloss:\t6.187473297119141\n",
      "04/28 11:47:06 AM |\t  GBAloss:\t11.432674407958984\n",
      "04/28 11:47:06 AM |\t  a_decoded[:2]:['Past experience dictates that, as the elected representatives of the European taxpayer, we should, and indeed must, demand financial probity and transparency in the disbursement and auditing of this money, hence our amendments and additions relate to achieving what are known as \"value for money\" indicators in the grant-giving process.', 'Next, we all too often see vast sums of money being spent on projects whose outcomes will necessarily be unclear at the start of the programme period.']\n",
      "04/28 11:47:06 AM |\t  pred_b_decoded[:2]:['The experience of the past shows that, as the elected representatives of the European taxpayers, we, the European taxpayers, are, of course, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a securing a securing a securing the cheapest, is, with our amendments and the – the – the – the – the – the – – the – the – the – the – the – the – the – the – the – the – the – the – the – the – the – the – the – the – the – the – the – the – the – the – the – – the – – the – – the – – the – the – – – – the – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –', 'Secondly, all too often, huge sums of money are spent on projects, whose results, at the beginning of the programme, are simply not able to be able to be able to be able to be able to be able to be able to apologise.']\n",
      "04/28 11:47:06 AM |\t  b_decoded[:2]:['Die Erfahrungen der Vergangenheit zeigen, daß wir als die gewählten Vertreter der europäischen Steuerzahler finanzielle Redlichkeit und Transparenz bei der Auszahlung dieser Gelder und der damit verbundenen Rechnungsprüfung fordern sollten, ja müssen. Mit unseren nderungen und Zusätzen wollen wir das erreichen, was bei der Gewährung von Zuschüssen als Indikator für die wirtschaftlich günstigste Lösung dienen kann.', 'Zweitens fließen nur allzu oft riesige Summen in Projekte, deren Ergebnisse sich zu Beginn des Programmzeitraums einfach noch nicht klar abschätzen lassen.']\n",
      "04/28 11:47:06 AM |\t  pred_a_decoded[:2]:['Wir freuen freuenheißenheißenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuen freuenheißenheißenheißenheißen freuen freuen freuenheißenheißenheißenheißen freuen freuen freuenheißenheißenheißenheißen freuen freuen freuenheißenheißenheißenheißen freuen freuen freuenheißenheißenheißenheißen freuen freuen freuenheißenheißenheißenheißen freuen freuen freuenheißenheißenheißenheißen freuen freuen freuenheißenheißenheißenheißen freuen freuen freuenheißenheißenheißenheißenheißenheißen freuen freuenheißenheißenheißenheißen', 'Wir freuen freuenheißenheißenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißenheißen freuen freuenheißenheißenheißenheißen freuen freuenheißenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißenheißenheißen freuen freuenheißen']\n",
      "04/28 11:50:31 AM |\t  computing score...\n",
      "04/28 11:50:31 AM |\t  G_AB GAB sacreBLEU : 0.019659\n",
      "04/28 11:50:31 AM |\t  G_BA GBA sacreBLEU : 6.745557\n",
      "04/28 11:50:31 AM |\t  G_AB GAB test loss : 6.963777\n",
      "04/28 11:50:31 AM |\t  G_BA GBA test loss : 11.301238\n",
      "04/28 11:50:37 AM |\t  \n",
      "\n",
      "  ----------------epoch:5----------------\n",
      "04/28 11:50:37 AM |\t  total iter:[2505]\n",
      "04/28 11:51:10 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.5953132474061215, 'GBA_once_meter': 0.9384095903598901, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:51:10 AM |\t  13.17365269461078%\n",
      "04/28 11:51:56 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.5895465052489078, 'GBA_once_meter': 0.9196541237108635, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:51:56 AM |\t  32.93413173652694%\n",
      "04/28 11:52:41 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.5826727964661338, 'GBA_once_meter': 0.9572871363524235, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:52:41 AM |\t  52.69461077844312%\n",
      "04/28 11:53:26 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.6070690552393595, 'GBA_once_meter': 0.9535171299269705, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:53:26 AM |\t  72.45508982035929%\n",
      "04/28 11:54:13 AM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.584141259843653, 'GBA_once_meter': 0.9537606438000997, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 11:54:13 AM |\t  92.21556886227546%\n",
      "04/28 11:54:34 AM |\t  DA_a: tensor([0.8192], device='cuda:0')tensor([0.8776], device='cuda:0')tensor([0.9401], device='cuda:0')\n",
      "04/28 11:54:34 AM |\t  DB_pred_dis: tensor([0.], device='cuda:0')tensor([0.], device='cuda:0')tensor([0.], device='cuda:0')\n",
      "04/28 11:54:34 AM |\t  DB_b: tensor([0.], device='cuda:0')tensor([0.], device='cuda:0')tensor([0.], device='cuda:0')\n",
      "04/28 11:54:34 AM |\t  DA_pred_dis: tensor([0.3725], device='cuda:0')tensor([0.4007], device='cuda:0')tensor([0.4412], device='cuda:0')\n",
      "04/28 11:54:34 AM |\t  GABloss:\t6.246008396148682\n",
      "04/28 11:54:34 AM |\t  GBAloss:\t12.929682731628418\n",
      "04/28 11:54:34 AM |\t  a_decoded[:2]:['Past experience dictates that, as the elected representatives of the European taxpayer, we should, and indeed must, demand financial probity and transparency in the disbursement and auditing of this money, hence our amendments and additions relate to achieving what are known as \"value for money\" indicators in the grant-giving process.', 'Next, we all too often see vast sums of money being spent on projects whose outcomes will necessarily be unclear at the start of the programme period.']\n",
      "04/28 11:54:34 AM |\t  pred_b_decoded[:2]:['The ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad', 'The ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad ad']\n",
      "04/28 11:54:34 AM |\t  b_decoded[:2]:['Die Erfahrungen der Vergangenheit zeigen, daß wir als die gewählten Vertreter der europäischen Steuerzahler finanzielle Redlichkeit und Transparenz bei der Auszahlung dieser Gelder und der damit verbundenen Rechnungsprüfung fordern sollten, ja müssen. Mit unseren nderungen und Zusätzen wollen wir das erreichen, was bei der Gewährung von Zuschüssen als Indikator für die wirtschaftlich günstigste Lösung dienen kann.', 'Zweitens fließen nur allzu oft riesige Summen in Projekte, deren Ergebnisse sich zu Beginn des Programmzeitraums einfach noch nicht klar abschätzen lassen.']\n",
      "04/28 11:54:34 AM |\t  pred_a_decoded[:2]:['Wir freuen freuenheißenheißenheißenheißenheißen freuenheißenheißen freuenheißenheißen freuenheißenheißen freuenheißenheißen freuenheißen freuenheißenheißen freuenheißen freuenheißenheißen freuenheißen freuenheißenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißen', 'Wir freuen freuenheißenheißenheißenheißenheißen freuenheißenheißen freuenheißenheißen freuenheißenheißen freuenheißenheißen freuenheißenheißen freuenheißen freuenheißenheißen freuenheißen freuenheißen freuenheißenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißenheißen freuen freuenheißen']\n",
      "04/28 11:59:35 AM |\t  computing score...\n",
      "04/28 11:59:35 AM |\t  G_AB GAB sacreBLEU : 0.016510\n",
      "04/28 11:59:35 AM |\t  G_BA GBA sacreBLEU : 0.083516\n",
      "04/28 11:59:35 AM |\t  G_AB GAB test loss : 6.741048\n",
      "04/28 11:59:35 AM |\t  G_BA GBA test loss : 14.562776\n",
      "04/28 11:59:43 AM |\t  \n",
      "\n",
      "  ----------------epoch:6----------------\n",
      "04/28 11:59:43 AM |\t  total iter:[3006]\n",
      "04/28 12:00:13 PM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.5944863991303877, 'GBA_once_meter': 0.9501899733687892, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 12:00:13 PM |\t  11.976047904191617%\n",
      "04/28 12:00:59 PM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.5841792550953951, 'GBA_once_meter': 0.9462835734540765, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 12:00:59 PM |\t  31.736526946107784%\n",
      "04/28 12:01:45 PM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.5426338459506179, 'GBA_once_meter': 0.9500702077692206, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 12:01:45 PM |\t  51.49700598802395%\n",
      "04/28 12:02:30 PM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.5032424700982643, 'GBA_once_meter': 0.9533850677085646, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 12:02:30 PM |\t  71.25748502994011%\n",
      "04/28 12:03:14 PM |\t  {'GB_cycle_meter': 0.0, 'GA_cycle_meter': 0.0, 'GAB_once_meter': 0.49979307615395746, 'GBA_once_meter': 0.9658331329172308, 'DA_meter': 0, 'DB_meter': 0}\n",
      "04/28 12:03:14 PM |\t  91.01796407185628%\n",
      "04/28 12:03:37 PM |\t  DA_a: tensor([0.8378], device='cuda:0')tensor([0.9608], device='cuda:0')tensor([0.8266], device='cuda:0')\n",
      "04/28 12:03:37 PM |\t  DB_pred_dis: tensor([0.], device='cuda:0')tensor([0.], device='cuda:0')tensor([0.], device='cuda:0')\n",
      "04/28 12:03:37 PM |\t  DB_b: tensor([0.], device='cuda:0')tensor([0.], device='cuda:0')tensor([0.], device='cuda:0')\n",
      "04/28 12:03:37 PM |\t  DA_pred_dis: tensor([0.3165], device='cuda:0')tensor([0.3761], device='cuda:0')tensor([0.4844], device='cuda:0')\n",
      "04/28 12:03:37 PM |\t  GABloss:\t6.461639404296875\n",
      "04/28 12:03:37 PM |\t  GBAloss:\t12.332457542419434\n",
      "04/28 12:03:37 PM |\t  a_decoded[:2]:['Past experience dictates that, as the elected representatives of the European taxpayer, we should, and indeed must, demand financial probity and transparency in the disbursement and auditing of this money, hence our amendments and additions relate to achieving what are known as \"value for money\" indicators in the grant-giving process.', 'Next, we all too often see vast sums of money being spent on projects whose outcomes will necessarily be unclear at the start of the programme period.']\n",
      "04/28 12:03:37 PM |\t  pred_b_decoded[:2]:['aficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficfic', 'aficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficficfic']\n",
      "04/28 12:03:37 PM |\t  b_decoded[:2]:['Die Erfahrungen der Vergangenheit zeigen, daß wir als die gewählten Vertreter der europäischen Steuerzahler finanzielle Redlichkeit und Transparenz bei der Auszahlung dieser Gelder und der damit verbundenen Rechnungsprüfung fordern sollten, ja müssen. Mit unseren nderungen und Zusätzen wollen wir das erreichen, was bei der Gewährung von Zuschüssen als Indikator für die wirtschaftlich günstigste Lösung dienen kann.', 'Zweitens fließen nur allzu oft riesige Summen in Projekte, deren Ergebnisse sich zu Beginn des Programmzeitraums einfach noch nicht klar abschätzen lassen.']\n",
      "04/28 12:03:37 PM |\t  pred_a_decoded[:2]:['We freuen begrüßheißenheißen we freuenheißenheißen we freuenheißenheißen freuen freuenheißenheißen freuen freuenheißen freuen freuenheißen freuen freuen freuenheißen freuen freuen freuenheißen freuen freuen freuenheißen freuen freuen freuen freuenheißen freuen freuen freuen freuen freuenheißen freuen freuen freuen freuen freuen freuen freuen freuenheißen freuen freuen freuen freuenheißen freuen freuen freuen freuen freuen freuen freuenheißen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuenheißen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen', 'We freuen weheißenheißen we freuenheißenheißen we freuenheißenheißenheißen freuen freuenheißenheißen freuen freuenheißen freuen freuenheißen freuen freuenheißen freuen freuen freuenheißen freuen freuen freuenheißen freuen freuen freuen freuenheißen freuen freuen freuen freuenheißen freuen freuen freuen freuen freuenheißen freuen freuen freuen freuen freuenheißen freuen freuen freuen freuen freuenheißen freuen freuen freuen freuen freuen freuenheißen freuen freuen freuen freuen freuen freuen freuenheißen freuen freuen freuen freuen freuen freuen freuen freuen freuenheißen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen freuen']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_239628/2913806740.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\n\\n  ----------------epoch:{epoch}----------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mmy_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcycleGAN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotal_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwandb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\GitCode\\cycleMT\\train.py\u001b[0m in \u001b[0;36mmy_train\u001b[1;34m(loader, model, total_iter, args, logging, valid_loader, tokenizer, wandb)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mD_A\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'./checkpoint/D_A.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mD_B\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'./checkpoint/D_B.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mmy_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwandb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\GitCode\\cycleMT\\test.py\u001b[0m in \u001b[0;36mmy_test\u001b[1;34m(loader, model, tokenizer, logging, wandb)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0ma_generate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGAB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_generate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mb_generate\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mGBA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_generate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\GitCode\\cycleMT\\basic_model.py\u001b[0m in \u001b[0;36mtest_generate\u001b[1;34m(self, x, num_beams, max_length)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenzied_prefix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0moutput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength_penalty\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_attn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_attn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[0;32m   1313\u001b[0m             )\n\u001b[0;32m   1314\u001b[0m             \u001b[1;31m# 12. run beam search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1315\u001b[1;33m             return self.beam_search(\n\u001b[0m\u001b[0;32m   1316\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2156\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2158\u001b[1;33m             outputs = self(\n\u001b[0m\u001b[0;32m   2159\u001b[0m                 \u001b[1;33m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2160\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1637\u001b[0m         \u001b[1;31m# Decode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1638\u001b[1;33m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[0;32m   1639\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1640\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1031\u001b[0m                 )\n\u001b[0;32m   1032\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1033\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m   1034\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[0mself_attn_past_key_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[0;32m    669\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    572\u001b[0m     ):\n\u001b[0;32m    573\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[0;32m    575\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m             \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# get query states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[0mquery_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;31m# get key/value states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if(args.valid_begin==1):\n",
    "    my_test(valid_dataloader,cycleGAN,tokenizer,logging,wandb)\n",
    "total_iter = [0]  \n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch}----------------\")\n",
    "    my_train(train_dataloader,cycleGAN,total_iter,args,logging,valid_dataloader,tokenizer,wandb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
