{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "from parameter import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.test_iter 500\n",
      "args.rep_iter 100\n"
     ]
    }
   ],
   "source": [
    "if(True):\n",
    "    parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "    parser.add_argument('--valid_num_points', type=int,             default = 100, help='validation data number')\n",
    "    parser.add_argument('--train_num_points', type=int,             default = 500, help='train data number')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int,                   default=4,     help='Batch size')\n",
    "    parser.add_argument('--max_length', type=int,                   default=128,     help='max_length')\n",
    "    parser.add_argument('--num_beam', type=int,                     default=2,     help='num_beam')\n",
    "\n",
    "    parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "    parser.add_argument('--G_AB_model_name', type=str,              default='t5-small',      help='model_name')\n",
    "    parser.add_argument('--G_BA_model_name', type=str,              default='Onlydrinkwater/T5-small-de-en',      help='model_name')\n",
    "    parser.add_argument('--D_A_model_name', type=str,               default='Onlydrinkwater/T5-small-de-en',      help='model_name')\n",
    "    parser.add_argument('--D_B_model_name', type=str,               default='t5-small',      help='model_name')\n",
    "    parser.add_argument('--exp_name', type=str,                     default='CYCLE!',      help='experiment name')\n",
    "    parser.add_argument('--rep_iter', type=int,                     default=100,      help='report times for 1 epoch')\n",
    "    parser.add_argument('--test_iter', type=int,                    default=500,      help='report times for 1 epoch')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int,                       default=50,     help='num of training epochs')\n",
    "\n",
    "    parser.add_argument('--G_lr', type=float,                       default=5e-6,   help='learning rate for G')\n",
    "    parser.add_argument('--G_weight_decay', type=float,             default=1e-3,   help='learning de for G')\n",
    "    parser.add_argument('--G_gamma', type=float,                    default=1,    help='lr*gamma after each test')\n",
    "    parser.add_argument('--G_grad_clip', type=float,                default=1,   help='grad_clip')\n",
    "    parser.add_argument('--D_lr', type=float,                       default=5e-5,   help='learning rate for D')\n",
    "    parser.add_argument('--D_weight_decay', type=float,             default=1e-3,   help='learning de for D')\n",
    "    parser.add_argument('--D_gamma', type=float,                    default=1,    help='lr*gamma after each test')\n",
    "    parser.add_argument('--D_grad_clip', type=float,                default=1e-2,   help='grad_clip')\n",
    "    parser.add_argument('--lambda_identity', type=float,            default=0.5,   help='')\n",
    "    parser.add_argument('--lambda_A', type=float,                   default=1,   help='')\n",
    "    parser.add_argument('--lambda_B', type=float,                   default=1,   help='')\n",
    "    parser.add_argument('--lambda_once', type=float,                default=0,   help='')\n",
    "    parser.add_argument('--lambda_GP', type=float,                  default=10,   help='WGANGP pentalty')\n",
    "    parser.add_argument('--DperG', type=int,                        default=2,    help='n_critc')\n",
    "    parser.add_argument('--GperD', type=int,                        default=2,    help='n_g')\n",
    "    parser.add_argument('--smoothing', type=float,                  default=0.5,    help='labelsmoothing')\n",
    "\n",
    "    parser.add_argument('--load_D', type=int,                       default=0,      help='load pretrained D')\n",
    "    parser.add_argument('--load_G', type=int,                       default=0,      help='load pretrained D')\n",
    "    parser.add_argument('--num_workers', type=int,                  default=0,      help='num_workers')\n",
    "    parser.add_argument('--valid_begin', type=int,                  default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--train_G', type=int,                      default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--train_D', type=int,                      default=1,      help='whether valid before train')\n",
    "    parser.add_argument('--D_pretrain_iter', type=int,              default=0,      help='whether valid before train')\n",
    "    parser.add_argument('--poolsize', type=int,                     default=1,      help='whether valid before train')\n",
    "\n",
    "\n",
    "    args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "    args.test_iter = args.test_iter//args.batch_size * args.batch_size\n",
    "    args.rep_iter = args.rep_iter//args.batch_size * args.batch_size\n",
    "    print('args.test_iter',args.test_iter)\n",
    "    print('args.rep_iter',args.rep_iter)#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\GitCode\\cycleMT\\wandb\\run-20220530_193025-25wmgrsx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/cycleWMT/runs/25wmgrsx\" target=\"_blank\">CYCLE!</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/cycleWMT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/cycleWMT/runs/25wmgrsx?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1592ea04eb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_API_KEY']='a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME']=args.exp_name\n",
    "wandb.init(project=\"cycleWMT\",config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 07:30:28 PM |\t  Namespace(D_A_model_name='Onlydrinkwater/T5-small-de-en', D_B_model_name='t5-small', D_gamma=1, D_grad_clip=0.01, D_lr=5e-05, D_pretrain_iter=0, D_weight_decay=0.001, DperG=2, G_AB_model_name='t5-small', G_BA_model_name='Onlydrinkwater/T5-small-de-en', G_gamma=1, G_grad_clip=1, G_lr=5e-06, G_weight_decay=0.001, GperD=2, batch_size=4, epochs=50, exp_name='CYCLE!', gpu=0, lambda_A=1, lambda_B=1, lambda_GP=10, lambda_identity=0.5, lambda_once=0, load_D=0, load_G=0, max_length=128, num_beam=2, num_workers=0, poolsize=1, rep_iter=100, smoothing=0.5, test_iter=500, train_D=1, train_G=1, train_num_points=500, valid_begin=1, valid_num_points=100)\n"
     ]
    }
   ],
   "source": [
    "#logging file\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'),'w',encoding = \"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "logging.info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 07:30:33 PM |\t  Gmodelsize:60.506624MB\n",
      "05/30 07:30:33 PM |\t  Dmodelsize:60.506624MB\n"
     ]
    }
   ],
   "source": [
    "GABmodelname = args.G_AB_model_name\n",
    "GBAmodelname = args.G_BA_model_name\n",
    "DAmodelname = args.D_A_model_name\n",
    "DBmodelname = args.D_B_model_name\n",
    "GABpretrained  =  AutoModelForSeq2SeqLM.from_pretrained(GABmodelname)\n",
    "GBApretrained  =  AutoModelForSeq2SeqLM.from_pretrained(GBAmodelname)\n",
    "DApretrained  =  AutoModelForSeq2SeqLM.from_pretrained(DAmodelname)\n",
    "DBpretrained  =  AutoModelForSeq2SeqLM.from_pretrained(DBmodelname)\n",
    "logging.info(f'Gmodelsize:{count_parameters_in_MB(GABpretrained)}MB')\n",
    "logging.info(f'Dmodelsize:{count_parameters_in_MB(DApretrained)}MB')\n",
    "tokenizer = AutoTokenizer.from_pretrained(GABmodelname)\n",
    "# tokenizerBA = AutoTokenizer.from_pretrained(GBAmodelname)#its the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 07:30:37 PM |\t  Reusing dataset wmt16 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt16\\de-en\\1.0.0\\9e0038fe4cc117bd474d2774032cc133e355146ed0a47021b2040ca9db4645c0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 24.89it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('wmt16',language+'-en')#load_dataset(\"bible_para\", lang1=\"de\", lang2=\"en\")\n",
    "train = dataset['train']['translation'][:args.train_num_points]\n",
    "valid = dataset['validation']['translation'][-args.valid_num_points:]#TODO:\n",
    "\n",
    "\n",
    "train_data = get_Dataset_chaos(train, tokenizer,max_length=args.max_length)\n",
    "train_dataloader = DataLoader(train_data, sampler= RandomSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "valid_data = get_Dataset(valid, tokenizer,max_length=args.max_length)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycleGAN = CycleGAN(args,GABpretrained,GBApretrained,DApretrained,DBpretrained,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 07:31:08 PM |\t  \n",
      "\n",
      "  ----------------epoch:0----------------\n",
      "05/30 07:31:08 PM |\t  total iter:[0] \t G_lr:5e-06 \t DA_lr:5e-05 \t DB_lr:5e-05\n",
      "05/30 07:32:56 PM |\t  {'GB_cycle_meter': 10.288027366002401, 'GA_cycle_meter': 10.20786706606547, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.00900066701695323, 'DB_meter': -0.004490953236818314, 'DA_GP_meter': 0.06232021939009428, 'DB_GP_meter': 0.028895174451172352}\n",
      "05/30 07:32:56 PM |\t  19.2%\n",
      "05/30 07:34:37 PM |\t  {'GB_cycle_meter': 10.256957451502482, 'GA_cycle_meter': 10.2678275903066, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.0031093112379312515, 'DB_meter': 0.002440945766866207, 'DA_GP_meter': 0.024880402758717536, 'DB_GP_meter': 0.020370380431413652}\n",
      "05/30 07:34:37 PM |\t  39.2%\n",
      "05/30 07:36:19 PM |\t  {'GB_cycle_meter': 10.282902790949894, 'GA_cycle_meter': 10.227728330172026, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.0055402122437953946, 'DB_meter': -0.004721790626645088, 'DA_GP_meter': 0.01815946415066719, 'DB_GP_meter': 0.011497445907443761}\n",
      "05/30 07:36:19 PM |\t  59.199999999999996%\n",
      "05/30 07:38:06 PM |\t  {'GB_cycle_meter': 10.301541010538736, 'GA_cycle_meter': 10.212521235148111, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.004701846390962601, 'DB_meter': -0.005745469145476818, 'DA_GP_meter': 0.01672174945473671, 'DB_GP_meter': 0.008060531355440617}\n",
      "05/30 07:38:06 PM |\t  79.2%\n",
      "05/30 07:39:52 PM |\t  {'GB_cycle_meter': 10.277893653282753, 'GA_cycle_meter': 10.237827741182768, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.0030783646553754805, 'DB_meter': -0.006573219522833824, 'DA_GP_meter': 0.013467870876193047, 'DB_GP_meter': 0.007447431068867445}\n",
      "05/30 07:39:52 PM |\t  99.2%\n",
      "05/30 07:40:00 PM |\t  DB_a_: 0.153,  0.123,  0.134,  0.147,  \n",
      "05/30 07:40:00 PM |\t  DB_pred_dis: 0.145,  0.134,  0.130,  0.145,  \n",
      "05/30 07:40:00 PM |\t  DA_b: 0.132,  0.098,  0.055,  0.092,  \n",
      "05/30 07:40:00 PM |\t  DA_pred_dis: 0.129,  0.080,  0.063,  0.102,  \n",
      "05/30 07:40:00 PM |\t  GABloss:\t5.756576061248779\n",
      "05/30 07:40:00 PM |\t  GBAloss:\t10.20509147644043\n",
      "05/30 07:40:00 PM |\t  a_decoded[:2]:['Prestige Cruises, also based in Miami, operates under the Oceania and Regent brands, which together have eight cruise ships traveling to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'It posted revenues of $1.2 billion in 2013, up 6 percent from the year earlier.', 'The $29 billion cruise industry is expected to benefit in the coming years from the rise of the middle class in emerging economies such as China and India.', 'Companies are racing to position themselves as the cruise operators of choice for these new customers.']\n",
      "05/30 07:40:00 PM |\t  pred_b_decoded[:2]:['Prestige Cruises, also based in Miami, is based on the brand name Oceania and Regent, which jointly operate eight cross-border ships on routes to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'Revenue for 2013 was reported to be USD 1.2 billion, an increase of 6% compared with last year.', 'In the next few years, the cross-border sector will be estimated at around USD 29 billion, and it will be expected in the light of the strengthening of the middle class in the swine countries such as China and India.', 'The companies are trying to position themselves as the best choice for these new customers in the cross-border operators.']\n",
      "05/30 07:40:00 PM |\t  b_decoded[:2]:['Prestige Cruises, ebenfalls in Miami angesiedelt, agiert unter den Markennamen Oceania und Regent, die gemeinsam acht Kreuzfahrschiffe auf Routen nach Skandinavien, Russland, dem Mittelmeer, Nordamerika, Asien, Afrika und Südamerika betreiben.', 'Es meldete Einnahmen für 2013 von 1,2 Milliarden $, ein Anstieg von 6 Prozent im Vergleich zum Vorjahr.', 'Die Kreuzfahrtbranche liegt insgesamt bei etwa 29 Milliarden $ und in den nächsten Jahren wird für sie aufgrund der Stärkung der Mittelklasse in Schwellenländern wie China und Indien erwartet.', 'Die Unternehmen versuchen sich für diese neuen Kunden als die beste Wahl bei den Kreuzfahrtanbietern zu positionieren.']\n",
      "05/30 07:40:00 PM |\t  pred_a_decoded[:2]:['Prestige Cruises, auch mit Sitz in Miami, ist unter den Marken Oceania und Regent tätig, die zusammen acht Kreuzfahrtschiffe nach Skandinavien, Russland, dem Mittelmeerraum, Nordamerika, Asien, Afrika und Südamerika fahren.', 'Im Jahr 2013 erzielte es einen Umsatz von 1,2 Mrd. USD, was 6 Prozent höher ist als im Vorjahr.', 'Es wird erwartet, dass die Reiseindustrie in den kommenden Jahren von dem Aufstieg der Mittelschicht in Schwellenländern wie China und Indien profitieren wird.', 'Die Unternehmen setzen sich für diese neuen Kunden als die bevorzugten Kreuzfahrtbetreiber ein.']\n",
      "05/30 07:40:40 PM |\t  computing score...\n",
      "05/30 07:40:40 PM |\t  G_AB GAB sacreBLEU : 23.666810\n",
      "05/30 07:40:40 PM |\t  G_BA GBA sacreBLEU : 21.214271\n",
      "05/30 07:40:40 PM |\t  G_AB GAB test loss : 5.172800\n",
      "05/30 07:40:40 PM |\t  G_BA GBA test loss : 9.041624\n",
      "05/30 07:40:40 PM |\t  D_A DA test loss : -0.004116\n",
      "05/30 07:40:40 PM |\t  D_B DB test loss : -0.000293\n",
      "05/30 07:40:40 PM |\t  D_A DA test accuracy : 0.610000\n",
      "05/30 07:40:40 PM |\t  D_B DB test accuracy : 0.500000\n",
      "05/30 07:40:40 PM |\t  \n",
      "\n",
      "  ----------------epoch:1----------------\n",
      "05/30 07:40:40 PM |\t  total iter:[500] \t G_lr:5e-06 \t DA_lr:5e-05 \t DB_lr:5e-05\n",
      "05/30 07:42:29 PM |\t  {'GB_cycle_meter': 10.271867911020914, 'GA_cycle_meter': 10.246761322021484, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.0026144738495349883, 'DB_meter': -0.004224586151540279, 'DA_GP_meter': 0.011830505393445491, 'DB_GP_meter': 0.006947512319311499}\n",
      "05/30 07:42:29 PM |\t  19.2%\n",
      "05/30 07:44:10 PM |\t  {'GB_cycle_meter': 10.271742820739746, 'GA_cycle_meter': 10.22061308224996, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.0035440437868237496, 'DB_meter': 0.0012110478058457374, 'DA_GP_meter': 0.009595597274601459, 'DB_GP_meter': 0.01564467215910554}\n",
      "05/30 07:44:10 PM |\t  39.2%\n",
      "05/30 07:46:01 PM |\t  {'GB_cycle_meter': 10.29376037304218, 'GA_cycle_meter': 10.216671283428486, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.005908528454601765, 'DB_meter': -0.00624555841088295, 'DA_GP_meter': 0.009831594061106444, 'DB_GP_meter': 0.0052995753940194845}\n",
      "05/30 07:46:01 PM |\t  59.199999999999996%\n",
      "05/30 07:47:44 PM |\t  {'GB_cycle_meter': 10.295184135437012, 'GA_cycle_meter': 10.225796620051065, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.00027954785153269765, 'DB_meter': 0.0037207846343517304, 'DA_GP_meter': 0.007325724232941866, 'DB_GP_meter': 0.005937439072877169}\n",
      "05/30 07:47:44 PM |\t  79.2%\n",
      "05/30 07:49:28 PM |\t  {'GB_cycle_meter': 10.281576596773588, 'GA_cycle_meter': 10.245852543757511, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.006974594183266163, 'DB_meter': -0.0006072817742824554, 'DA_GP_meter': 0.005847418559715152, 'DB_GP_meter': 0.004121191594749689}\n",
      "05/30 07:49:28 PM |\t  99.2%\n",
      "05/30 07:49:36 PM |\t  DB_a_: 0.137,  0.113,  0.112,  0.131,  \n",
      "05/30 07:49:36 PM |\t  DB_pred_dis: 0.133,  0.117,  0.106,  0.133,  \n",
      "05/30 07:49:36 PM |\t  DA_b: 0.127,  0.096,  0.090,  0.111,  \n",
      "05/30 07:49:36 PM |\t  DA_pred_dis: 0.123,  0.073,  0.077,  0.104,  \n",
      "05/30 07:49:36 PM |\t  GABloss:\t6.223456382751465\n",
      "05/30 07:49:36 PM |\t  GBAloss:\t10.710319519042969\n",
      "05/30 07:49:36 PM |\t  a_decoded[:2]:['Prestige Cruises, also based in Miami, operates under the Oceania and Regent brands, which together have eight cruise ships traveling to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'It posted revenues of $1.2 billion in 2013, up 6 percent from the year earlier.', 'The $29 billion cruise industry is expected to benefit in the coming years from the rise of the middle class in emerging economies such as China and India.', 'Companies are racing to position themselves as the cruise operators of choice for these new customers.']\n",
      "05/30 07:49:36 PM |\t  pred_b_decoded[:2]:['Prestige Cruises, also based in Miami, is based on the brand name Oceania and Regent, which jointly operate eight cross-border ships on routes to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'Revenue for 2013 was reported to be USD 1.2 billion, an increase of 6% compared with last year.', 'In the next few years, the cross-border sector will be estimated at around USD 29 billion, and it will be expected in the light of the strengthening of the mid-class in heavy countries such as China and India.', 'The companies are trying to position themselves as the best choice for these new customers in the cross-border operators.']\n",
      "05/30 07:49:36 PM |\t  b_decoded[:2]:['Prestige Cruises, ebenfalls in Miami angesiedelt, agiert unter den Markennamen Oceania und Regent, die gemeinsam acht Kreuzfahrschiffe auf Routen nach Skandinavien, Russland, dem Mittelmeer, Nordamerika, Asien, Afrika und Südamerika betreiben.', 'Es meldete Einnahmen für 2013 von 1,2 Milliarden $, ein Anstieg von 6 Prozent im Vergleich zum Vorjahr.', 'Die Kreuzfahrtbranche liegt insgesamt bei etwa 29 Milliarden $ und in den nächsten Jahren wird für sie aufgrund der Stärkung der Mittelklasse in Schwellenländern wie China und Indien erwartet.', 'Die Unternehmen versuchen sich für diese neuen Kunden als die beste Wahl bei den Kreuzfahrtanbietern zu positionieren.']\n",
      "05/30 07:49:36 PM |\t  pred_a_decoded[:2]:['Prestige Cruises, mit Sitz in Miami, ist unter den Marken Oceania und Regent tätig, die zusammen acht Kreuzfahrtschiffe nach Skandinavien, Russland, dem Mittelmeerraum, Nordamerika, Asien, Afrika und Südamerika fahren.', 'Im Jahr 2013 erzielte es einen Umsatz von 1,2 Mrd. USD, wobei es um 6 Prozent gegenüber dem Vorjahreszeitraum ging.', 'Es wird erwartet, dass die Reiseindustrie in den kommenden Jahren von dem Aufstieg der Mittelschicht in Schwellenländern wie China und Indien profitieren wird.', 'Die Unternehmen setzen sich für diese neuen Kunden als die bevorzugten Kreuzfahrtbetreiber ein.']\n",
      "05/30 07:50:17 PM |\t  computing score...\n",
      "05/30 07:50:17 PM |\t  G_AB GAB sacreBLEU : 23.906778\n",
      "05/30 07:50:17 PM |\t  G_BA GBA sacreBLEU : 21.382682\n",
      "05/30 07:50:17 PM |\t  G_AB GAB test loss : 5.611436\n",
      "05/30 07:50:17 PM |\t  G_BA GBA test loss : 9.494787\n",
      "05/30 07:50:17 PM |\t  D_A DA test loss : -0.005133\n",
      "05/30 07:50:17 PM |\t  D_B DB test loss : -0.002649\n",
      "05/30 07:50:17 PM |\t  D_A DA test accuracy : 0.610000\n",
      "05/30 07:50:17 PM |\t  D_B DB test accuracy : 0.620000\n",
      "05/30 07:50:17 PM |\t  \n",
      "\n",
      "  ----------------epoch:2----------------\n",
      "05/30 07:50:17 PM |\t  total iter:[1000] \t G_lr:5e-06 \t DA_lr:5e-05 \t DB_lr:5e-05\n",
      "05/30 07:52:01 PM |\t  {'GB_cycle_meter': 10.277164220809937, 'GA_cycle_meter': 10.210715452829996, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.0043994981795549394, 'DB_meter': -0.0019416149333119392, 'DA_GP_meter': 0.004701112955808639, 'DB_GP_meter': 0.00399986264295876}\n",
      "05/30 07:52:01 PM |\t  19.2%\n",
      "05/30 07:53:49 PM |\t  {'GB_cycle_meter': 10.265422185262045, 'GA_cycle_meter': 10.244590361913046, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': 0.0005741404369473457, 'DB_meter': -0.01472509179264307, 'DA_GP_meter': 0.004501560628414154, 'DB_GP_meter': 0.0069868665421381594}\n",
      "05/30 07:53:49 PM |\t  39.2%\n",
      "05/30 07:55:29 PM |\t  {'GB_cycle_meter': 10.231107051555927, 'GA_cycle_meter': 10.18633820460393, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.011784550957381726, 'DB_meter': 0.00027635157108306884, 'DA_GP_meter': 0.004254644801840186, 'DB_GP_meter': 0.0021063122642226515}\n",
      "05/30 07:55:29 PM |\t  59.199999999999996%\n",
      "05/30 07:57:25 PM |\t  {'GB_cycle_meter': 10.26043152809143, 'GA_cycle_meter': 10.227939287821451, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.0036461204290390015, 'DB_meter': -0.002790936715900898, 'DA_GP_meter': 0.003726313170045614, 'DB_GP_meter': 0.0011612946561217541}\n",
      "05/30 07:57:25 PM |\t  79.2%\n",
      "05/30 07:59:12 PM |\t  {'GB_cycle_meter': 10.279196078960712, 'GA_cycle_meter': 10.223758110633263, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.0050437355414032934, 'DB_meter': -0.0014004521816968919, 'DA_GP_meter': 0.0036598666664212944, 'DB_GP_meter': 0.0004270520968566416}\n",
      "05/30 07:59:12 PM |\t  99.2%\n",
      "05/30 07:59:21 PM |\t  DB_a_: 0.032,  0.030,  0.031,  0.031,  \n",
      "05/30 07:59:21 PM |\t  DB_pred_dis: 0.032,  0.030,  0.030,  0.030,  \n",
      "05/30 07:59:21 PM |\t  DA_b: 0.118,  0.094,  0.102,  0.097,  \n",
      "05/30 07:59:21 PM |\t  DA_pred_dis: 0.112,  0.069,  0.077,  0.096,  \n",
      "05/30 07:59:21 PM |\t  GABloss:\t6.903481960296631\n",
      "05/30 07:59:21 PM |\t  GBAloss:\t11.20413875579834\n",
      "05/30 07:59:21 PM |\t  a_decoded[:2]:['Prestige Cruises, also based in Miami, operates under the Oceania and Regent brands, which together have eight cruise ships traveling to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'It posted revenues of $1.2 billion in 2013, up 6 percent from the year earlier.', 'The $29 billion cruise industry is expected to benefit in the coming years from the rise of the middle class in emerging economies such as China and India.', 'Companies are racing to position themselves as the cruise operators of choice for these new customers.']\n",
      "05/30 07:59:21 PM |\t  pred_b_decoded[:2]:['Prestige Cruises, also based in Miami, is operated under the brand name Oceania and Regent, which jointly operate eight cross-border ships on routes to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'It reported revenue for 2013 of USD 1.2 billion, an increase of 6% compared with last year.', 'In the next few years, the cross-border sector will be estimated at around USD 29 billion, and it will be expected to do so in the light of the strengthening of the mid-class in heavy countries such as China and India.', 'The companies are trying to position themselves as the best choice for these new customers in the cross-border operators.']\n",
      "05/30 07:59:21 PM |\t  b_decoded[:2]:['Prestige Cruises, ebenfalls in Miami angesiedelt, agiert unter den Markennamen Oceania und Regent, die gemeinsam acht Kreuzfahrschiffe auf Routen nach Skandinavien, Russland, dem Mittelmeer, Nordamerika, Asien, Afrika und Südamerika betreiben.', 'Es meldete Einnahmen für 2013 von 1,2 Milliarden $, ein Anstieg von 6 Prozent im Vergleich zum Vorjahr.', 'Die Kreuzfahrtbranche liegt insgesamt bei etwa 29 Milliarden $ und in den nächsten Jahren wird für sie aufgrund der Stärkung der Mittelklasse in Schwellenländern wie China und Indien erwartet.', 'Die Unternehmen versuchen sich für diese neuen Kunden als die beste Wahl bei den Kreuzfahrtanbietern zu positionieren.']\n",
      "05/30 07:59:21 PM |\t  pred_a_decoded[:2]:['Prestige Cruises, auch in Miami, ist unter den Marken Oceania und Regent tätig, die zusammen acht Kreuzfahrtschiffe nach Skandinavien, Russland, dem Mittelmeerraum, Nordamerika, Asien, Afrika und Südamerika fahren.', 'Im Jahr 2013 erzielte es einen Umsatz von 1,2 Mrd. USD, wobei es um 6 Prozent gegenüber dem Vorjahreszeitraum ging.', 'Es wird erwartet, dass die Reiseindustrie in den kommenden Jahren von dem Aufstieg der Mittelschicht in Schwellenländern wie China und Indien profitieren wird.', 'Die Unternehmen setzen sich für diese neuen Kunden als die bevorzugten Kreuzfahrtbetreiber ein.']\n",
      "05/30 08:00:01 PM |\t  computing score...\n",
      "05/30 08:00:01 PM |\t  G_AB GAB sacreBLEU : 24.326188\n",
      "05/30 08:00:01 PM |\t  G_BA GBA sacreBLEU : 21.351867\n",
      "05/30 08:00:01 PM |\t  G_AB GAB test loss : 6.249340\n",
      "05/30 08:00:01 PM |\t  G_BA GBA test loss : 9.928893\n",
      "05/30 08:00:01 PM |\t  D_A DA test loss : -0.006832\n",
      "05/30 08:00:01 PM |\t  D_B DB test loss : -0.000449\n",
      "05/30 08:00:01 PM |\t  D_A DA test accuracy : 0.700000\n",
      "05/30 08:00:01 PM |\t  D_B DB test accuracy : 0.720000\n",
      "05/30 08:00:01 PM |\t  \n",
      "\n",
      "  ----------------epoch:3----------------\n",
      "05/30 08:00:01 PM |\t  total iter:[1500] \t G_lr:5e-06 \t DA_lr:5e-05 \t DB_lr:5e-05\n",
      "05/30 08:01:51 PM |\t  {'GB_cycle_meter': 10.239019473393759, 'GA_cycle_meter': 10.238989035288492, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.005762370433658361, 'DB_meter': -0.0016001351550221444, 'DA_GP_meter': 0.003778317552059889, 'DB_GP_meter': 9.757075982633978e-05}\n",
      "05/30 08:01:51 PM |\t  19.2%\n",
      "05/30 08:03:40 PM |\t  {'GB_cycle_meter': 10.269721269607544, 'GA_cycle_meter': 10.238832314809164, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.012680374756455422, 'DB_meter': 0.0002613039966672659, 'DA_GP_meter': 0.0038252225611358883, 'DB_GP_meter': 5.42108180525247e-05}\n",
      "05/30 08:03:40 PM |\t  39.2%\n",
      "05/30 08:05:23 PM |\t  {'GB_cycle_meter': 10.248295783996582, 'GA_cycle_meter': 10.16697568159837, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.006435537841171026, 'DB_meter': -0.0008954060263931751, 'DA_GP_meter': 0.004328953055664897, 'DB_GP_meter': 6.961363207665272e-05}\n",
      "05/30 08:05:23 PM |\t  59.199999999999996%\n",
      "05/30 08:07:11 PM |\t  {'GB_cycle_meter': 10.268133242925009, 'GA_cycle_meter': 10.216081460316977, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.01691299840807915, 'DB_meter': -0.0017781513556838035, 'DA_GP_meter': 0.004940722063183784, 'DB_GP_meter': 8.922767941839994e-05}\n",
      "05/30 08:07:11 PM |\t  79.2%\n",
      "05/30 08:09:01 PM |\t  {'GB_cycle_meter': 10.2468780370859, 'GA_cycle_meter': 10.19604858985314, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.010163190346211194, 'DB_meter': -0.0019878734461963176, 'DA_GP_meter': 0.004699963368475437, 'DB_GP_meter': 8.993409472168423e-05}\n",
      "05/30 08:09:01 PM |\t  99.2%\n",
      "05/30 08:09:08 PM |\t  DB_a_: 0.033,  0.032,  0.032,  0.032,  \n",
      "05/30 08:09:08 PM |\t  DB_pred_dis: 0.033,  0.031,  0.033,  0.031,  \n",
      "05/30 08:09:08 PM |\t  DA_b: 0.098,  0.070,  0.090,  0.070,  \n",
      "05/30 08:09:08 PM |\t  DA_pred_dis: 0.088,  0.033,  0.061,  0.081,  \n",
      "05/30 08:09:08 PM |\t  GABloss:\t7.823338031768799\n",
      "05/30 08:09:08 PM |\t  GBAloss:\t11.646291732788086\n",
      "05/30 08:09:08 PM |\t  a_decoded[:2]:['Prestige Cruises, also based in Miami, operates under the Oceania and Regent brands, which together have eight cruise ships traveling to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'It posted revenues of $1.2 billion in 2013, up 6 percent from the year earlier.', 'The $29 billion cruise industry is expected to benefit in the coming years from the rise of the middle class in emerging economies such as China and India.', 'Companies are racing to position themselves as the cruise operators of choice for these new customers.']\n",
      "05/30 08:09:08 PM |\t  pred_b_decoded[:2]:['Prestige Cruises, also based in Miami, is operated under the brand name Oceania and Regent, which jointly operate eight cross-border ships on routes to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'It reported revenue for 2013 of USD 1.2 billion, an increase of 6% compared with last year.', 'The cross-border industry is estimated to be around USD 29 billion as a whole, and in the coming years it will be expected to do so because of the strengthening of the mid-class in heavy countries such as China and India.', 'The companies are trying to position themselves as the best choice for these new customers in the cross-border operators.']\n",
      "05/30 08:09:08 PM |\t  b_decoded[:2]:['Prestige Cruises, ebenfalls in Miami angesiedelt, agiert unter den Markennamen Oceania und Regent, die gemeinsam acht Kreuzfahrschiffe auf Routen nach Skandinavien, Russland, dem Mittelmeer, Nordamerika, Asien, Afrika und Südamerika betreiben.', 'Es meldete Einnahmen für 2013 von 1,2 Milliarden $, ein Anstieg von 6 Prozent im Vergleich zum Vorjahr.', 'Die Kreuzfahrtbranche liegt insgesamt bei etwa 29 Milliarden $ und in den nächsten Jahren wird für sie aufgrund der Stärkung der Mittelklasse in Schwellenländern wie China und Indien erwartet.', 'Die Unternehmen versuchen sich für diese neuen Kunden als die beste Wahl bei den Kreuzfahrtanbietern zu positionieren.']\n",
      "05/30 08:09:08 PM |\t  pred_a_decoded[:2]:['Prestige Cruises, auch in Miami, ist unter den Marken Oceania und Regent tätig, die zusammen acht Kreuzfahrtschiffe nach Skandinavien, Russland, dem Mittelmeerraum, Nordamerika, Asien, Afrika und Südamerika fahren.', 'Im Jahr 2013 erzielte es einen Umsatz von 1,2 Mrd. USD, wobei es um 6 Prozent gegenüber dem Vorjahreszeitraum ging.', 'Die Reiseindustrie in Höhe von 29 Milliarden Dollar wird in den kommenden Jahren von dem Aufstieg der Mittelschicht in Schwellenländern wie China und Indien profitieren.', 'Die Unternehmen setzen sich für diese neuen Kunden als die bevorzugten Kreuzfahrtbetreiber ein.']\n",
      "05/30 08:09:49 PM |\t  computing score...\n",
      "05/30 08:09:49 PM |\t  G_AB GAB sacreBLEU : 24.469158\n",
      "05/30 08:09:49 PM |\t  G_BA GBA sacreBLEU : 21.855418\n",
      "05/30 08:09:49 PM |\t  G_AB GAB test loss : 7.105077\n",
      "05/30 08:09:49 PM |\t  G_BA GBA test loss : 10.369081\n",
      "05/30 08:09:49 PM |\t  D_A DA test loss : -0.010957\n",
      "05/30 08:09:49 PM |\t  D_B DB test loss : -0.000384\n",
      "05/30 08:09:49 PM |\t  D_A DA test accuracy : 0.680000\n",
      "05/30 08:09:49 PM |\t  D_B DB test accuracy : 0.670000\n",
      "05/30 08:09:49 PM |\t  \n",
      "\n",
      "  ----------------epoch:4----------------\n",
      "05/30 08:09:49 PM |\t  total iter:[2000] \t G_lr:5e-06 \t DA_lr:5e-05 \t DB_lr:5e-05\n",
      "05/30 08:11:32 PM |\t  {'GB_cycle_meter': 10.243653456370035, 'GA_cycle_meter': 10.21643861134847, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.013463508263230324, 'DB_meter': -0.0004061206988990307, 'DA_GP_meter': 0.005500276554375887, 'DB_GP_meter': 0.00011209144868189469}\n",
      "05/30 08:11:32 PM |\t  19.2%\n",
      "05/30 08:13:26 PM |\t  {'GB_cycle_meter': 10.264635483423868, 'GA_cycle_meter': 10.200777769088745, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.019345317389816045, 'DB_meter': 0.0017439878731966018, 'DA_GP_meter': 0.005659188590943814, 'DB_GP_meter': 0.00023304710324737244}\n",
      "05/30 08:13:26 PM |\t  39.2%\n",
      "05/30 08:15:13 PM |\t  {'GB_cycle_meter': 10.25394579080435, 'GA_cycle_meter': 10.212742658761831, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.022946715503931046, 'DB_meter': -0.0007793617807328701, 'DA_GP_meter': 0.005780845768749714, 'DB_GP_meter': 0.00010526669801038225}\n",
      "05/30 08:15:13 PM |\t  59.199999999999996%\n",
      "05/30 08:17:05 PM |\t  {'GB_cycle_meter': 10.255216519037882, 'GA_cycle_meter': 10.205772638320923, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.017233533449470997, 'DB_meter': -0.0012372904270887375, 'DA_GP_meter': 0.005984226185828448, 'DB_GP_meter': 7.0805463110446e-05}\n",
      "05/30 08:17:05 PM |\t  79.2%\n",
      "05/30 08:18:59 PM |\t  {'GB_cycle_meter': 10.250679162832407, 'GA_cycle_meter': 10.137255741999699, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.02663955807685852, 'DB_meter': 0.0008250185009092092, 'DA_GP_meter': 0.006741964193060994, 'DB_GP_meter': 6.016593466483755e-05}\n",
      "05/30 08:18:59 PM |\t  99.2%\n",
      "05/30 08:19:07 PM |\t  DB_a_: 0.035,  0.033,  0.034,  0.034,  \n",
      "05/30 08:19:07 PM |\t  DB_pred_dis: 0.035,  0.033,  0.034,  0.033,  \n",
      "05/30 08:19:07 PM |\t  DA_b: 0.116,  0.099,  0.124,  0.108,  \n",
      "05/30 08:19:07 PM |\t  DA_pred_dis: 0.106,  0.060,  0.078,  0.130,  \n",
      "05/30 08:19:07 PM |\t  GABloss:\t8.91854476928711\n",
      "05/30 08:19:07 PM |\t  GBAloss:\t12.188756942749023\n",
      "05/30 08:19:07 PM |\t  a_decoded[:2]:['Prestige Cruises, also based in Miami, operates under the Oceania and Regent brands, which together have eight cruise ships traveling to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'It posted revenues of $1.2 billion in 2013, up 6 percent from the year earlier.', 'The $29 billion cruise industry is expected to benefit in the coming years from the rise of the middle class in emerging economies such as China and India.', 'Companies are racing to position themselves as the cruise operators of choice for these new customers.']\n",
      "05/30 08:19:07 PM |\t  pred_b_decoded[:2]:['Prestige Cruises, also based in Miami, is based on the brand name Oceania and Regent, which jointly operate eight cross-roads on routes to Scandinavia, Russia, the Mediterranean, North America, Asia, Africa and South America.', 'It reported revenue for 2013 of USD 1.2 billion, an increase of 6% compared with last year.', 'The cross-border industry is estimated to be around USD 29 billion as a whole, and in the coming years it will be expected to do so because of the strengthening of the mid-class in heavy countries such as China and India.', 'The companies are trying to position themselves as the best choice for these new customers in the cross-border operators.']\n",
      "05/30 08:19:07 PM |\t  b_decoded[:2]:['Prestige Cruises, ebenfalls in Miami angesiedelt, agiert unter den Markennamen Oceania und Regent, die gemeinsam acht Kreuzfahrschiffe auf Routen nach Skandinavien, Russland, dem Mittelmeer, Nordamerika, Asien, Afrika und Südamerika betreiben.', 'Es meldete Einnahmen für 2013 von 1,2 Milliarden $, ein Anstieg von 6 Prozent im Vergleich zum Vorjahr.', 'Die Kreuzfahrtbranche liegt insgesamt bei etwa 29 Milliarden $ und in den nächsten Jahren wird für sie aufgrund der Stärkung der Mittelklasse in Schwellenländern wie China und Indien erwartet.', 'Die Unternehmen versuchen sich für diese neuen Kunden als die beste Wahl bei den Kreuzfahrtanbietern zu positionieren.']\n",
      "05/30 08:19:07 PM |\t  pred_a_decoded[:2]:['Prestige Cruises, ebenfalls in Miami, ist unter den Marken Oceania und Regent tätig, die zusammen acht Kreuzfahrtschiffe nach Skandinavien, Russland, dem Mittelmeerraum, Nordamerika, Asien, Afrika und Südamerika fahren.', 'Im Jahr 2013 erzielte es einen Umsatz von 1,2 Mrd. USD, was 6 Prozent höher ist als im Vorjahr.', 'Die Reiseindustrie in Höhe von 29 Milliarden Dollar wird in den kommenden Jahren von dem Aufstieg der Mittelschicht in Schwellenländern wie China und Indien profitieren.', 'Die Unternehmen setzen sich für diese neuen Kunden als die bevorzugten Kreuzfahrtbetreiber ein.']\n",
      "05/30 08:19:50 PM |\t  computing score...\n",
      "05/30 08:19:50 PM |\t  G_AB GAB sacreBLEU : 24.944447\n",
      "05/30 08:19:50 PM |\t  G_BA GBA sacreBLEU : 21.707555\n",
      "05/30 08:19:50 PM |\t  G_AB GAB test loss : 8.083587\n",
      "05/30 08:19:50 PM |\t  G_BA GBA test loss : 10.867443\n",
      "05/30 08:19:50 PM |\t  D_A DA test loss : -0.016447\n",
      "05/30 08:19:50 PM |\t  D_B DB test loss : -0.000414\n",
      "05/30 08:19:50 PM |\t  D_A DA test accuracy : 0.710000\n",
      "05/30 08:19:50 PM |\t  D_B DB test accuracy : 0.730000\n",
      "05/30 08:19:50 PM |\t  \n",
      "\n",
      "  ----------------epoch:5----------------\n",
      "05/30 08:19:50 PM |\t  total iter:[2500] \t G_lr:5e-06 \t DA_lr:5e-05 \t DB_lr:5e-05\n",
      "05/30 08:21:43 PM |\t  {'GB_cycle_meter': 10.220013856887817, 'GA_cycle_meter': 10.251599391301474, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.03679038293659687, 'DB_meter': -0.002325325608253479, 'DA_GP_meter': 0.008665556143969298, 'DB_GP_meter': 6.167673636809922e-05}\n",
      "05/30 08:21:43 PM |\t  19.2%\n",
      "05/30 08:23:44 PM |\t  {'GB_cycle_meter': 10.213937441507975, 'GA_cycle_meter': 10.200010379155477, 'GAB_once_meter': 0.0, 'GBA_once_meter': 0.0, 'DA_meter': -0.034309463910758495, 'DB_meter': 0.00034003349021077155, 'DA_GP_meter': 0.008275501262396574, 'DB_GP_meter': 4.587603652908001e-05}\n",
      "05/30 08:23:44 PM |\t  39.2%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\GitCode\\cycleMT\\main.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/GitCode/cycleMT/main.ipynb#ch0000007?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mepochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/GitCode/cycleMT/main.ipynb#ch0000007?line=5'>6</a>\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m  ----------------epoch:\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m----------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/GitCode/cycleMT/main.ipynb#ch0000007?line=6'>7</a>\u001b[0m     my_train(train_dataloader,cycleGAN,total_iter,args,logging,valid_dataloader,tokenizer,wandb)\n",
      "File \u001b[1;32mg:\\GitCode\\cycleMT\\train.py:26\u001b[0m, in \u001b[0;36mmy_train\u001b[1;34m(loader, model, total_iter, args, logging, valid_loader, tokenizer, wandb)\u001b[0m\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/train.py?line=23'>24</a>\u001b[0m         model\u001b[39m.\u001b[39moptimize_parameters(trainD\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mtrain_D,trainG\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/train.py?line=24'>25</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///g%3A/GitCode/cycleMT/train.py?line=25'>26</a>\u001b[0m         model\u001b[39m.\u001b[39;49moptimize_parameters(trainD\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mtrain_D,trainG\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/train.py?line=26'>27</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/train.py?line=27'>28</a>\u001b[0m \u001b[39m    if(step%args.GperD==0 and step!=0):\u001b[39;00m\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/train.py?line=28'>29</a>\u001b[0m \u001b[39m        model.optimize_parameters(trainD=True,trainG=True)\u001b[39;00m\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/train.py?line=29'>30</a>\u001b[0m \u001b[39m    else:\u001b[39;00m\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/train.py?line=30'>31</a>\u001b[0m \u001b[39m        model.optimize_parameters(trainD=False,trainG=True)\u001b[39;00m\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/train.py?line=31'>32</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/train.py?line=33'>34</a>\u001b[0m \u001b[39mif\u001b[39;00m(total_iter[\u001b[39m0\u001b[39m]\u001b[39m%\u001b[39margs\u001b[39m.\u001b[39mrep_iter \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "File \u001b[1;32mg:\\GitCode\\cycleMT\\cycle.py:108\u001b[0m, in \u001b[0;36mCycleGAN.optimize_parameters\u001b[1;34m(self, trainD, trainG)\u001b[0m\n\u001b[0;32m    <a href='file:///g%3A/GitCode/cycleMT/cycle.py?line=105'>106</a>\u001b[0m \u001b[39melif\u001b[39;00m(trainD):\n\u001b[0;32m    <a href='file:///g%3A/GitCode/cycleMT/cycle.py?line=106'>107</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_requires_grad([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mG_AB, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mG_BA], \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> <a href='file:///g%3A/GitCode/cycleMT/cycle.py?line=107'>108</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward()\n\u001b[0;32m    <a href='file:///g%3A/GitCode/cycleMT/cycle.py?line=108'>109</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_D_A\u001b[39m.\u001b[39mzero_grad()   \u001b[39m# set D_A and D_B's gradients to zero\u001b[39;00m\n\u001b[0;32m    <a href='file:///g%3A/GitCode/cycleMT/cycle.py?line=109'>110</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_D_B\u001b[39m.\u001b[39mzero_grad()   \u001b[39m# set D_A and D_B's gradients to zero\u001b[39;00m\n",
      "File \u001b[1;32mg:\\GitCode\\cycleMT\\cycle.py:76\u001b[0m, in \u001b[0;36mCycleGAN.forward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/cycle.py?line=73'>74</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_A,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_A_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mG_BA\u001b[39m.\u001b[39mgumbel_generate_soft(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfake_B,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfake_B_attn)   \u001b[39m# G_B(G_A(A)) (batchsize*numbeam,sentencelength,vocabsize)\u001b[39;00m\n\u001b[0;32m     <a href='file:///g%3A/GitCode/cycleMT/cycle.py?line=74'>75</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfake_A,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfake_A_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mG_BA\u001b[39m.\u001b[39mgumbel_generate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreal_B,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreal_B_attn)  \u001b[39m# G_B(B)\u001b[39;00m\n\u001b[1;32m---> <a href='file:///g%3A/GitCode/cycleMT/cycle.py?line=75'>76</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_B,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_B_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mG_AB\u001b[39m.\u001b[39;49mgumbel_generate_soft(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_A,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_A_attn)\n",
      "File \u001b[1;32mg:\\GitCode\\cycleMT\\basic_model.py:143\u001b[0m, in \u001b[0;36mG.gumbel_generate_soft\u001b[1;34m(self, x, x_attn)\u001b[0m\n\u001b[0;32m    <a href='file:///g%3A/GitCode/cycleMT/basic_model.py?line=140'>141</a>\u001b[0m \u001b[39mif\u001b[39;00m(\u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mshape)\u001b[39m==\u001b[39m\u001b[39m3\u001b[39m):\n\u001b[0;32m    <a href='file:///g%3A/GitCode/cycleMT/basic_model.py?line=141'>142</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(x,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m#change logit to index if needed\u001b[39;00m\n\u001b[1;32m--> <a href='file:///g%3A/GitCode/cycleMT/basic_model.py?line=142'>143</a>\u001b[0m generate_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(x,num_beams\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)[:,\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mcontiguous()\u001b[39m#get rid of start padding \u001b[39;00m\n\u001b[0;32m    <a href='file:///g%3A/GitCode/cycleMT/basic_model.py?line=143'>144</a>\u001b[0m att \u001b[39m=\u001b[39m (generate_id\u001b[39m>\u001b[39m\u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mlong()\n\u001b[0;32m    <a href='file:///g%3A/GitCode/cycleMT/basic_model.py?line=144'>145</a>\u001b[0m x_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x_)\n",
      "File \u001b[1;32mg:\\GitCode\\cycleMT\\basic_model.py:166\u001b[0m, in \u001b[0;36mG.generate\u001b[1;34m(self, input_ids, num_beams, max_length, num_return_sequences)\u001b[0m\n\u001b[0;32m    <a href='file:///g%3A/GitCode/cycleMT/basic_model.py?line=163'>164</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, input_ids, num_beams \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, num_return_sequences\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m):\u001b[39m#long training time!\u001b[39;00m\n\u001b[0;32m    <a href='file:///g%3A/GitCode/cycleMT/basic_model.py?line=164'>165</a>\u001b[0m     max_length \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mmax_length\n\u001b[1;32m--> <a href='file:///g%3A/GitCode/cycleMT/basic_model.py?line=165'>166</a>\u001b[0m     output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate( input_ids \u001b[39m=\u001b[39;49m input_ids, num_beams \u001b[39m=\u001b[39;49m num_beams, early_stopping \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, max_length \u001b[39m=\u001b[39;49m max_length, length_penalty \u001b[39m=\u001b[39;49m\u001b[39m0.6\u001b[39;49m, repetition_penalty \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,num_return_sequences\u001b[39m=\u001b[39;49mnum_return_sequences )\n\u001b[0;32m    <a href='file:///g%3A/GitCode/cycleMT/basic_model.py?line=166'>167</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output_ids\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\transformers\\generation_utils.py:1315\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1310'>1311</a>\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1311'>1312</a>\u001b[0m         input_ids, expand_size\u001b[39m=\u001b[39mnum_beams, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1312'>1313</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1313'>1314</a>\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1314'>1315</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1315'>1316</a>\u001b[0m         input_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1316'>1317</a>\u001b[0m         beam_scorer,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1317'>1318</a>\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1318'>1319</a>\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1319'>1320</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1320'>1321</a>\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1321'>1322</a>\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1322'>1323</a>\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1323'>1324</a>\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1324'>1325</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1325'>1326</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1327'>1328</a>\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1328'>1329</a>\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1329'>1330</a>\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1330'>1331</a>\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, typical_p\u001b[39m=\u001b[39mtypical_p, temperature\u001b[39m=\u001b[39mtemperature, num_beams\u001b[39m=\u001b[39mnum_beams\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=1331'>1332</a>\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\transformers\\generation_utils.py:2158\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2153'>2154</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2155'>2156</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2157'>2158</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2158'>2159</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2159'>2160</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2160'>2161</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2161'>2162</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2162'>2163</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2164'>2165</a>\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/generation_utils.py?line=2165'>2166</a>\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1638\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1634'>1635</a>\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1636'>1637</a>\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1637'>1638</a>\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1638'>1639</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1639'>1640</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1640'>1641</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1641'>1642</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1642'>1643</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1643'>1644</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1644'>1645</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1645'>1646</a>\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1646'>1647</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1647'>1648</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1648'>1649</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1649'>1650</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1650'>1651</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1652'>1653</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1654'>1655</a>\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1033\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1019'>1020</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1020'>1021</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1021'>1022</a>\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1029'>1030</a>\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1030'>1031</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1031'>1032</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1032'>1033</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1033'>1034</a>\u001b[0m         hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1034'>1035</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1035'>1036</a>\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1036'>1037</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1037'>1038</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1038'>1039</a>\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1039'>1040</a>\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1040'>1041</a>\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1041'>1042</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1042'>1043</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1043'>1044</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1044'>1045</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1046'>1047</a>\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1047'>1048</a>\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:668\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=664'>665</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=665'>666</a>\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=667'>668</a>\u001b[0m self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m0\u001b[39;49m](\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=668'>669</a>\u001b[0m     hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=669'>670</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=670'>671</a>\u001b[0m     position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=671'>672</a>\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=672'>673</a>\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=673'>674</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=674'>675</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=675'>676</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=676'>677</a>\u001b[0m hidden_states, present_key_value_state \u001b[39m=\u001b[39m self_attention_outputs[:\u001b[39m2\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=677'>678</a>\u001b[0m attention_outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:574\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=562'>563</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=563'>564</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=564'>565</a>\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=570'>571</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=571'>572</a>\u001b[0m ):\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=572'>573</a>\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=573'>574</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSelfAttention(\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=574'>575</a>\u001b[0m         normed_hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=575'>576</a>\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=576'>577</a>\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=577'>578</a>\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=578'>579</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=579'>580</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=580'>581</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=581'>582</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=582'>583</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=583'>584</a>\u001b[0m     outputs \u001b[39m=\u001b[39m (hidden_states,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:506\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=501'>502</a>\u001b[0m \u001b[39m# get key/value states\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=502'>503</a>\u001b[0m key_states \u001b[39m=\u001b[39m project(\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=503'>504</a>\u001b[0m     hidden_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk, key_value_states, past_key_value[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=504'>505</a>\u001b[0m )\n\u001b[1;32m--> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=505'>506</a>\u001b[0m value_states \u001b[39m=\u001b[39m project(\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=506'>507</a>\u001b[0m     hidden_states, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv, key_value_states, past_key_value[\u001b[39m1\u001b[39;49m] \u001b[39mif\u001b[39;49;00m past_key_value \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=507'>508</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=509'>510</a>\u001b[0m \u001b[39m# compute scores\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=510'>511</a>\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=511'>512</a>\u001b[0m     query_states, key_states\u001b[39m.\u001b[39mtranspose(\u001b[39m3\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=512'>513</a>\u001b[0m )  \u001b[39m# equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:483\u001b[0m, in \u001b[0;36mT5Attention.forward.<locals>.project\u001b[1;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=478'>479</a>\u001b[0m \u001b[39m\"\"\"projects hidden states correctly to key/query states\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=479'>480</a>\u001b[0m \u001b[39mif\u001b[39;00m key_value_states \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=480'>481</a>\u001b[0m     \u001b[39m# self-attn\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=481'>482</a>\u001b[0m     \u001b[39m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=482'>483</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m shape(proj_layer(hidden_states))\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=483'>484</a>\u001b[0m \u001b[39melif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=484'>485</a>\u001b[0m     \u001b[39m# cross-attn\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=485'>486</a>\u001b[0m     \u001b[39m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/transformers/models/t5/modeling_t5.py?line=486'>487</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m shape(proj_layer(key_value_states))\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\cycle\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/kevin/miniconda3/envs/cycle/lib/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if(args.valid_begin==1):\n",
    "    my_test(valid_dataloader,cycleGAN,tokenizer,logging,wandb)\n",
    "total_iter = [0]  \n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch}----------------\")\n",
    "    my_train(train_dataloader,cycleGAN,total_iter,args,logging,valid_dataloader,tokenizer,wandb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2a958cf2c5c0cd3cfa7593bb1f22c814db6e88adb2853f1b0eeb9f68d33d3cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cycle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
