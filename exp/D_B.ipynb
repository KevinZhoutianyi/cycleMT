{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "def d(l):\n",
    "    return tokenizer.batch_decode(l,skip_special_tokens=True)\n",
    "def en(l):\n",
    "    return tokenize(l,tokenizer,512,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./model/D_B.pt').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natürlich dieseen wurden je eine solche dieser Art vergetroffenn. Hier handelt Ausebenfalls en dergetroffen.  H  Eine B    diese Ver Genehmigung) Nutzen Wir Nutzen Maßnahmen Das Hand  Wie gegen Frau der auf dazu ebensomals  8 Mehr   Die ähnlich Gebiet Zu     über  Bristol darauf Es Mängel  Staat tätig erklärt Nutzen kostet. weiter Pie  Act  Das  In gefällt  Red klar In   Das Diese behandelt Dies Da Konsequenz bereits Ausnahme Afrika;',\n",
       " 'Also die danngesagt, daß wir alsoks ver richtiggemäß haben am damitsondernaß der keines aus holen haben. wenndaß wir ichs heute tuns haben, geradesondernaß neu Fres auch einmalKampagne haben, unsdaß  demanderfd, mitgeschafft hatten, sodaß wir Vas gut selbstgegeben haben, dasanimaß wir es  bietet haben, Siesondernaß wir vermutlichs amwagen haben, daß wir darüberents teilweise auchg haben, ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d([[14649,   637,    35,  3163,   528,   266,  6466,     3,  1878,  1261,\n",
    "           548, 25556,    29,     5,  3204, 11141,  1392,  7667,     3,    35,\n",
    "            74, 25556,     5,     3,   454,     1,     3,  2820,   272,     3,\n",
    "             3,     3,   637,   781, 31660,    61, 23986,  1185, 23986, 19876,\n",
    "         32075,   644,  2263,     3,  2739,  2995,  7672,    74,   219,  4042,\n",
    "         12424,  1982,     7,     3,   505,     1,     0, 11093,     3,     0,\n",
    "             3,   316,     1, 16284, 19358,  1811,     0,     3,     3,     3,\n",
    "             3,   510,     3, 17836,  6101,  1122, 31848,     3, 18122, 19709,\n",
    "         13411,     1, 23986, 22907,     0,     1,     5,  3067, 11591,     3,\n",
    "          1983,     3,   644,     3,    86, 25533,     3,     1,  1624,  8330,\n",
    "            86,     3,     3,   644,  2167, 24566,  9654,   878, 31391,  3360,\n",
    "         25385, 30615,   117,     1],\n",
    "        [ 1203,    67,  1352, 14668,     6,     3,    26,  7118,   558,    92,\n",
    "           157,     7,   548,  7716, 14002,   745,   183,  2889,  3296,  7118,\n",
    "            74,  4276,    15,     7,   403,     3, 16067,   745,     5,  1301,\n",
    "            26,  7118,   558,     3,   362,     7,  4270,  5240,     7,   745,\n",
    "             6,  6034,  3296,  7118,  5854,  6248,    15,     7,   319,  4909,\n",
    "         30249,   745,     6,  1149,    26,  7118,     3, 17199,    89,    26,\n",
    "             6,   181, 27551,  8827,     6,    78,    26,  7118,   558,   584,\n",
    "             9,     7,  1806,  2619,  6975,   745,     6,   211, 13607,  7118,\n",
    "           558,     3,    15,     7,     3,     3,  4236,   745,     6,   292,\n",
    "          3296,  7118,   558,     3, 29277,     7,   183, 11994,   745,     6,\n",
    "             3,    26,  7118,   558,  9924,   295,     7, 19815,   319,   122,\n",
    "           745,     6,     3,     1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label = en(['Mr President, Commissioner, I would like to thank Mrs Schroedter for an excellent report.', 'She has gone into the issue in some depth and in the committee debate she took account of many of the amendments that have been tabled regarding this report.'])\n",
    "pred = en(['Mr President, Commissioner President-in-Office of the Council, Commissioner Commissioner Commissioner, Commissioner Commissioner Commissioner Commissioner, Commissioner Commissioner Commissioner Commissioner, ladies and gentlemen, Commissioner Commissioner Commissioner Commissioner Commissioner Commissioner Commissioner Commissioner, ladies and gentlemen, Commissioner Commissioner Commissioner Commissioner, ladies and gentlemen, Commissioner Commissioner Commissioner Commissioner Commissioner, ladies and gentlemen, Commissioner Commissioner Commissioner Commissioner Commissioner Commissioner Commissioner, ladies and gentlemen, Commissioner Commissioner Commissioner Commissioner Commissioner, ladies and gentlemen, Commissioner Commissioner Commissioner Commissioner, ladies and gentlemen, Commissioner Commissioner Commissioner, ladies and gentlemen, Commissioner Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner, Commissioner Commissioner,', 'I also welcome the comments made on behalf of the Committee on Legal Affairs and the Internal Market and the Committee on Economic and Monetary Affairs.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.Tensor([[ 2167,  1392, 19107,    64,    67,     3,     2, 20181,    74,  2995,\n",
    "          7497,    52,  2880, 13896,   537,     3, 28180, 19333,  1149,    67,\n",
    "           493,  7733, 15635,   425,  9687,    49,   961,  6186, 14707,     6,\n",
    "            67, 26740,    74, 27579,  1324,     7,  5578,     7,   436,     5,\n",
    "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0],\n",
    "        [   86, 20878, 10993,  7852,     3,    15,     7,    16,   177,     3,\n",
    "          5802,  3093, 12834,   468,    74,    86, 18992,  8369,    74, 25886,\n",
    "         14080,     7,     6,     3, 21984,    23,    67,  6822, 12880,    16,\n",
    "           645, 13010,   115,    40,  5588,   324, 15833,   157,  6850,    15,\n",
    "             6,    16,    73,   172,    83,  7423,  2978,  4713,  1885, 19580,\n",
    "             3,  9096,     6,     3,    26,  7118,    67, 18209,    18,    64,\n",
    "         13848,  1639,  5348,    15,   219,     3, 20112,    49, 21621,   170,\n",
    "          4499, 25477,  3163,     5,     1,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1363,  1661,     6, 14595,     6,    27,   133,   114,    12,  2763,\n",
      "          8667,   180, 10363,    15,    26,   449,    21,    46,  1287,   934,\n",
      "             5,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  451,    65,  2767,   139,     8,   962,    16,   128,  4963,    11,\n",
      "            16,     8,  4492,  5054,   255,   808,   905,    13,   186,    13,\n",
      "             8, 12123,     7,    24,    43,   118,   953,    26,  1918,    48,\n",
      "           934,     5,     1]], device='cuda:0')\n",
      "torch.Size([2, 33])\n",
      "\t\t---------embedding\n",
      "torch.Size([2, 33, 512])\n",
      "tensor([[[-15.8327,   0.3196,   3.1946,  ...,  -4.6197,  27.3722,  17.6071],\n",
      "         [ -1.9148,  -6.4818,  28.3587,  ..., -56.7450, -71.9382,   8.4272],\n",
      "         [ 12.5716,   7.8596, -10.9473,  ...,   7.8750,  -6.7155,  -3.3432],\n",
      "         ...,\n",
      "         [ -1.9089,   0.1207,  -7.0306,  ...,  -0.4225,   2.6453,  -2.8076],\n",
      "         [ -1.9089,   0.1207,  -7.0306,  ...,  -0.4225,   2.6453,  -2.8076],\n",
      "         [ -1.9089,   0.1207,  -7.0306,  ...,  -0.4225,   2.6453,  -2.8076]],\n",
      "\n",
      "        [[ -0.0811,  -8.3459,  34.6410,  ..., -26.4998,  10.8664,  26.6754],\n",
      "         [ 17.7880,  -3.4346, -18.3111,  ...,  11.3356,  -3.1962,  14.4275],\n",
      "         [  6.6495, -23.3625, -36.0316,  ...,  -6.1322,  42.6921,   7.7975],\n",
      "         ...,\n",
      "         [  8.7908,   7.7483,   5.5777,  ...,  16.4545, -42.9668,   1.5239],\n",
      "         [ -4.2707,   7.3047, -14.5229,  ...,  12.9807,  -2.6582,  -3.0608],\n",
      "         [ 12.5838,   8.1357, -11.4688,  ...,   7.7555,  -7.2309,   0.8988]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([2, 33, 512])\n",
      "tensor([[[-0.0118,  0.1339, -0.1731,  ..., -0.2488,  0.2081, -0.2403],\n",
      "         [-0.1009,  0.0546, -0.1594,  ..., -0.2892, -0.0017, -0.1313],\n",
      "         [ 0.0106,  0.2509, -0.2410,  ..., -0.0882,  0.1153, -0.1659],\n",
      "         ...,\n",
      "         [ 0.1480, -0.0294, -0.0805,  ..., -0.2151,  0.0463, -0.0907],\n",
      "         [ 0.1547, -0.0507, -0.0865,  ..., -0.2170,  0.0372, -0.0760],\n",
      "         [ 0.1544, -0.0568, -0.0875,  ..., -0.2167,  0.0391, -0.0767]],\n",
      "\n",
      "        [[ 0.0778,  0.1718,  0.0940,  ..., -0.2421,  0.0687, -0.2019],\n",
      "         [ 0.0309,  0.2564, -0.2188,  ..., -0.1009,  0.0480, -0.1721],\n",
      "         [ 0.0662,  0.3321, -0.2699,  ..., -0.1721,  0.0460, -0.2028],\n",
      "         ...,\n",
      "         [ 0.0193,  0.0428, -0.0327,  ...,  0.0762, -0.0217, -0.3064],\n",
      "         [-0.1333,  0.0068,  0.0233,  ...,  0.1601, -0.0976, -0.0669],\n",
      "         [-0.0430,  0.0654, -0.1061,  ..., -0.0811, -0.0742,  0.0041]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([2, 33, 512])\n",
      "tensor([[[-0.0118,  0.1339, -0.1731,  ..., -0.2488,  0.2081, -0.2403],\n",
      "         [-0.1009,  0.0546, -0.1594,  ..., -0.2892, -0.0017, -0.1313],\n",
      "         [ 0.0106,  0.2509, -0.2410,  ..., -0.0882,  0.1153, -0.1659],\n",
      "         ...,\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "        [[ 0.0778,  0.1718,  0.0940,  ..., -0.2421,  0.0687, -0.2019],\n",
      "         [ 0.0309,  0.2564, -0.2188,  ..., -0.1009,  0.0480, -0.1721],\n",
      "         [ 0.0662,  0.3321, -0.2699,  ..., -0.1721,  0.0460, -0.2028],\n",
      "         ...,\n",
      "         [ 0.0193,  0.0428, -0.0327,  ...,  0.0762, -0.0217, -0.3064],\n",
      "         [-0.1333,  0.0068,  0.0233,  ...,  0.1601, -0.0976, -0.0669],\n",
      "         [-0.0430,  0.0654, -0.1061,  ..., -0.0811, -0.0742,  0.0041]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([2, 512])\n",
      "tensor([[-0.0185,  0.1983, -0.1390,  ..., -0.1002,  0.0827, -0.1432],\n",
      "        [ 0.0222,  0.2427, -0.0694,  ..., -0.1163,  0.0505, -0.1615]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[1.0234],\n",
      "        [0.9825]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.0234],\n",
      "        [0.9825]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = label[0].cuda()\n",
    "#delte 0 give huge diff\n",
    "       \n",
    "# x = torch.Tensor(x).long().cuda()\n",
    "print(x)\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn_unsq,1)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret_label =  model.classifier(distr)#(bs,1)\n",
    "print(ret_label)\n",
    "\n",
    "print(model(x,x_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256])\n",
      "\t\t---------embedding\n",
      "torch.Size([2, 256, 512])\n",
      "tensor([[[-15.8327,   0.3196,   3.1946,  ...,  -4.6197,  27.3722,  17.6071],\n",
      "         [ -1.9148,  -6.4818,  28.3587,  ..., -56.7450, -71.9382,   8.4272],\n",
      "         [ 12.5716,   7.8596, -10.9473,  ...,   7.8750,  -6.7155,  -3.3432],\n",
      "         ...,\n",
      "         [-27.5763, -20.2604,   2.1405,  ...,  -6.3479, -36.1910,  16.2589],\n",
      "         [ 12.5716,   7.8596, -10.9473,  ...,   7.8750,  -6.7155,  -3.3432],\n",
      "         [ 12.5838,   8.1357, -11.4688,  ...,   7.7555,  -7.2309,   0.8988]],\n",
      "\n",
      "        [[ 15.7629,   3.0036,  14.8671,  ..., -11.0151,  -4.3929,   5.6647],\n",
      "         [ 12.8145,   4.5959,  -1.3623,  ...,  -0.2104, -33.6627,  34.2502],\n",
      "         [ 24.0963,  -6.9128, -34.4636,  ..., -18.3673,  21.6795,   5.4308],\n",
      "         ...,\n",
      "         [ -1.9089,   0.1207,  -7.0306,  ...,  -0.4225,   2.6453,  -2.8076],\n",
      "         [ -1.9089,   0.1207,  -7.0306,  ...,  -0.4225,   2.6453,  -2.8076],\n",
      "         [ -1.9089,   0.1207,  -7.0306,  ...,  -0.4225,   2.6453,  -2.8076]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([2, 256, 512])\n",
      "tensor([[[-0.1410, -0.1578, -0.1062,  ..., -0.1847,  0.1169,  0.0778],\n",
      "         [-0.1093, -0.2353, -0.1192,  ..., -0.2556, -0.0998,  0.0912],\n",
      "         [ 0.0341,  0.1026, -0.1478,  ..., -0.0673,  0.0026, -0.0192],\n",
      "         ...,\n",
      "         [-0.1102, -0.1423, -0.1389,  ..., -0.1314,  0.0038, -0.0911],\n",
      "         [ 0.0058, -0.1019, -0.1731,  ..., -0.0923,  0.0469, -0.0137],\n",
      "         [-0.0383, -0.0309, -0.1796,  ..., -0.1425,  0.0274, -0.0105]],\n",
      "\n",
      "        [[ 0.0726,  0.2631, -0.1048,  ..., -0.2296,  0.0811, -0.1001],\n",
      "         [ 0.0715,  0.3671, -0.0412,  ..., -0.1660, -0.0241, -0.0797],\n",
      "         [-0.0038,  0.3695, -0.1200,  ..., -0.2381,  0.0669,  0.0897],\n",
      "         ...,\n",
      "         [ 0.0728, -0.0247, -0.0950,  ..., -0.2179, -0.0064, -0.2026],\n",
      "         [ 0.0728, -0.0247, -0.0950,  ..., -0.2179, -0.0064, -0.2026],\n",
      "         [ 0.0728, -0.0247, -0.0950,  ..., -0.2179, -0.0064, -0.2026]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([2, 256, 512])\n",
      "tensor([[[-0.1410, -0.1578, -0.1062,  ..., -0.1847,  0.1169,  0.0778],\n",
      "         [-0.1093, -0.2353, -0.1192,  ..., -0.2556, -0.0998,  0.0912],\n",
      "         [ 0.0341,  0.1026, -0.1478,  ..., -0.0673,  0.0026, -0.0192],\n",
      "         ...,\n",
      "         [-0.1102, -0.1423, -0.1389,  ..., -0.1314,  0.0038, -0.0911],\n",
      "         [ 0.0058, -0.1019, -0.1731,  ..., -0.0923,  0.0469, -0.0137],\n",
      "         [-0.0383, -0.0309, -0.1796,  ..., -0.1425,  0.0274, -0.0105]],\n",
      "\n",
      "        [[ 0.0726,  0.2631, -0.1048,  ..., -0.2296,  0.0811, -0.1001],\n",
      "         [ 0.0715,  0.3671, -0.0412,  ..., -0.1660, -0.0241, -0.0797],\n",
      "         [-0.0038,  0.3695, -0.1200,  ..., -0.2381,  0.0669,  0.0897],\n",
      "         ...,\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([2, 512])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 512])\n",
      "tensor([[-0.0696, -0.1330, -0.2040,  ..., -0.1271,  0.0158,  0.0750],\n",
      "        [ 0.0004,  0.2299, -0.0983,  ..., -0.0855,  0.0702, -0.1878]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[-0.0578],\n",
      "        [ 1.0316]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = pred[0].cuda()\n",
    "       \n",
    "# x = torch.Tensor(x).long().cuda()\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "print(torch.sum(distr,1).shape)\n",
    "print(torch.sum(x_attn,1).shape)\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn_unsq,1)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret_pred =  model.classifier(distr)#(bs,1)\n",
    "print(ret_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0234],\n",
      "        [0.9825]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5338, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2671, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterionGAN = torch.nn.MSELoss()\n",
    "pred_real = ret_label\n",
    "print(pred_real)\n",
    "loss_D_real = criterionGAN(pred_real, torch.ones((pred_real.shape[0],1),device='cuda'))\n",
    "print(loss_D_real)\n",
    "# Fake\n",
    "pred_fake = ret_pred\n",
    "loss_D_fake = criterionGAN(pred_fake, torch.zeros((pred_fake.shape[0],1),device='cuda'))\n",
    "# Combined loss and calculate gradients\n",
    "print(loss_D_fake)\n",
    "\n",
    "\n",
    "loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "print(loss_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ",attention_mask=x_attn is essential for the encoder inpuit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We previously use torch.mean to (x after embedding)*x_attn but the mean will be small for the long sentences, now we use torch.sum(distr,1)/torch.sum(x_attn,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the D judge the sentence by it's length\n",
    "## small output for the first part, but large for the latter part, so the longer the higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## so we change the output to [batch,0,512] ie, onlythe first word's sumQ*V and than droppout and then classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAKE dont end with 1, cuz i use model.generate but not self.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
