{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "def d(l):\n",
    "    return tokenizer.batch_decode(l,skip_special_tokens=True)\n",
    "def en(l):\n",
    "    return tokenize(l,tokenizer,512,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label =[[  461,     8, 10433,     7,    13,     8, 15481,     6,    34,    19,\n",
    "           614,    12,  3034,    24,    16,   325,  3767,    26,  7902,    54,\n",
    "          1590, 15121,  1364,   145,    24,     5,     1,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0],\n",
    "        [  148,   410,    59,   580,   140,   893,     5,     1,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0]]\n",
    "label = torch.Tensor(label).long().cuda()\n",
    "pred = [[ 1363,  1661,    18,    77,    18, 22098,  3527,  3527,  3527,  3527,\n",
    "          3527,    30,  9682,  9682,  9682,  9682,  9682,  9682,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606,   606,   606,   606,   606,   606,\n",
    "           606,   606,   606,   606,   606],\n",
    "        [   16,     8,  1611,  3545,  3545, 11729, 11729,     7,    21,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,  2958,\n",
    "          2958,  2958,  2958,  2958,  2958]]\n",
    "pred = torch.Tensor(pred).long().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./model/D_B.pt').train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label = en(['I have some sympathy with the last speaker: sometimes the Committee on Budgetary Control gets so involved with the important work they are doing, that they are unable to see the wood for the trees.', 'I also welcome the presentation on behalf of the Legal Affairs Committee and the Economic and Monetary Affairs Committee which took a balanced approach to the major and indeed legitimate concerns of the Committee on Budgetary Control about fraud.'])[0]\n",
    "# [           'I agree with the previous speaker: sometimes the Committee on Budgetary Control is so deep in its important work that it does not see the forest behind trees.', 'I also welcome the statements made on behalf of the Committee on Legal Affairs and the Internal Market and the Committee on Economic and Monetary Affairs, which took a fairly balanced position on the serious and quite justified concerns of the Committee on Budgetary Control with regard to fraud.']\n",
    "\n",
    "pred = en([ 'I agree with the previous speaker: sometimes the Committee on Budgetary Control is so deep in its important work that it does not see the forest behind trees.', 'I also welcome the statements made on behalf of the Committee on Legal Affairs and the Internal Market and the Committee on Economic and Monetary Affairs, which have taken a fairly balanced position on the serious and quite justified concerns of the Committee on Budgetary Control with regard to fraud.'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  461,     8, 10433,     7,    13,     8, 15481,     6,    34,    19,\n",
      "           614,    12,  3034,    24,    16,   325,  3767,    26,  7902,    54,\n",
      "          1590, 15121,  1364,   145,    24,     5,     1,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  148,   410,    59,   580,   140,   893,     5,     1,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], device='cuda:0')\n",
      "torch.Size([2, 127])\n",
      "\t\t---------embedding\n",
      "torch.Size([2, 127, 512])\n",
      "tensor([[[  2.5387,   8.5179,  -7.0770,  ...,  -4.6504, -12.1455,   5.2702],\n",
      "         [ 12.2070,   6.7962,  10.2948,  ...,  11.7214,   6.2806,  45.4166],\n",
      "         [  0.3637,   3.4859, -27.3800,  ...,   7.7382, -51.2711,   7.4479],\n",
      "         ...,\n",
      "         [ -1.9713,   0.1809,  -7.0931,  ...,  -0.4843,   2.7078,  -2.8701],\n",
      "         [ -1.9713,   0.1809,  -7.0931,  ...,  -0.4843,   2.7078,  -2.8701],\n",
      "         [ -1.9713,   0.1809,  -7.0931,  ...,  -0.4843,   2.7078,  -2.8701]],\n",
      "\n",
      "        [[ 16.2760,  -0.3107, -10.2164,  ...,  -3.1303,  -9.5884,  33.4790],\n",
      "         [ -7.6588, -13.4221, -13.0612,  ...,  -1.1190,  -5.0252,  20.1279],\n",
      "         [ -9.9868,   4.5142,  14.4234,  ..., -23.0618, -17.1389,  20.4855],\n",
      "         ...,\n",
      "         [ -1.9713,   0.1809,  -7.0931,  ...,  -0.4843,   2.7078,  -2.8701],\n",
      "         [ -1.9713,   0.1809,  -7.0931,  ...,  -0.4843,   2.7078,  -2.8701],\n",
      "         [ -1.9713,   0.1809,  -7.0931,  ...,  -0.4843,   2.7078,  -2.8701]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([2, 127, 512])\n",
      "tensor([[[ 0.2394,  0.0961, -0.1906,  ..., -0.2665,  0.2097, -0.2367],\n",
      "         [ 0.0703,  0.0836, -0.1460,  ..., -0.0595,  0.2734, -0.1938],\n",
      "         [-0.1163,  0.2519, -0.0814,  ..., -0.1797,  0.0155, -0.2749],\n",
      "         ...,\n",
      "         [-0.0163,  0.2742,  0.0000,  ...,  0.0046, -0.0780, -0.0766],\n",
      "         [ 0.1314,  0.1946, -0.0775,  ...,  0.0339, -0.1361, -0.2099],\n",
      "         [-0.1280,  0.0583, -0.1023,  ...,  0.2283,  0.0669, -0.1259]],\n",
      "\n",
      "        [[ 0.2681, -0.0690, -0.0000,  ..., -0.0672, -0.0530, -0.0000],\n",
      "         [-0.0776,  0.0044, -0.3069,  ..., -0.0752,  0.1184,  0.0364],\n",
      "         [-0.0113, -0.0000,  0.1027,  ..., -0.3555, -0.0409,  0.0999],\n",
      "         ...,\n",
      "         [ 0.2557,  0.0000,  0.2094,  ..., -0.0454,  0.0419, -0.1966],\n",
      "         [ 0.2706,  0.1638, -0.0887,  ..., -0.0601,  0.0193, -0.1305],\n",
      "         [ 0.0259,  0.0668,  0.1904,  ..., -0.0206,  0.0387, -0.2126]]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([2, 127, 512])\n",
      "tensor([[[ 0.2394,  0.0961, -0.1906,  ..., -0.2665,  0.2097, -0.2367],\n",
      "         [ 0.0703,  0.0836, -0.1460,  ..., -0.0595,  0.2734, -0.1938],\n",
      "         [-0.1163,  0.2519, -0.0814,  ..., -0.1797,  0.0155, -0.2749],\n",
      "         ...,\n",
      "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "        [[ 0.2681, -0.0690, -0.0000,  ..., -0.0672, -0.0530, -0.0000],\n",
      "         [-0.0776,  0.0044, -0.3069,  ..., -0.0752,  0.1184,  0.0364],\n",
      "         [-0.0113, -0.0000,  0.1027,  ..., -0.3555, -0.0409,  0.0999],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([2, 512])\n",
      "tensor([[-0.0326,  0.0738, -0.0800,  ..., -0.0546, -0.0028, -0.0272],\n",
      "        [-0.0326, -0.0311,  0.0021,  ..., -0.0654,  0.0130,  0.0514]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[0.1791],\n",
      "        [0.0869]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1906],\n",
      "        [0.1012]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = label.cuda()\n",
    "#delte 0 give huge diff\n",
    "\n",
    "\n",
    "# x = torch.Tensor(x).long().cuda()\n",
    "print(x)\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn_unsq,1)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret_label =  model.classifier(distr)#(bs,1)\n",
    "print(ret_label)\n",
    "\n",
    "print(model(x,x_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 255])\n",
      "\t\t---------embedding\n",
      "torch.Size([2, 255, 512])\n",
      "tensor([[[-15.8953,   0.3812,   3.2571,  ...,  -4.6822,  27.4347,  17.6696],\n",
      "         [ -1.9772,  -6.5443,  28.4213,  ..., -56.8083, -72.0014,   8.4897],\n",
      "         [ 17.9072,   5.1582,   7.1158,  ...,  17.9551,  -6.4665,  -2.5419],\n",
      "         ...,\n",
      "         [ 11.3657,  -8.1172,  13.6794,  ..., -25.2435,   3.1112,  -0.7454],\n",
      "         [ 11.3657,  -8.1172,  13.6794,  ..., -25.2435,   3.1112,  -0.7454],\n",
      "         [ 11.3657,  -8.1172,  13.6794,  ..., -25.2435,   3.1112,  -0.7454]],\n",
      "\n",
      "        [[ -5.6621,   1.9777,  -0.9974,  ...,  -8.8201,   4.2981,   0.9969],\n",
      "         [ 12.2070,   6.7962,  10.2948,  ...,  11.7214,   6.2806,  45.4166],\n",
      "         [ 16.8151, -12.1546,  30.7473,  ...,   1.9031,  -8.6687,  -0.9308],\n",
      "         ...,\n",
      "         [ -2.4141,  -7.9422,  11.3802,  ..., -25.6851,  13.1921,  13.8277],\n",
      "         [ -2.4141,  -7.9422,  11.3802,  ..., -25.6851,  13.1921,  13.8277],\n",
      "         [ -2.4141,  -7.9422,  11.3802,  ..., -25.6851,  13.1921,  13.8277]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([2, 255, 512])\n",
      "tensor([[[-0.0218,  0.0131,  0.0029,  ..., -0.2638,  0.1159, -0.1051],\n",
      "         [-0.0193, -0.2618, -0.0020,  ..., -0.3170, -0.0903,  0.1028],\n",
      "         [ 0.2173,  0.0206, -0.0082,  ..., -0.1374,  0.1499,  0.0843],\n",
      "         ...,\n",
      "         [ 0.0195, -0.1588,  0.1321,  ..., -0.2930,  0.3810, -0.3947],\n",
      "         [ 0.0000, -0.1233, -0.0336,  ..., -0.3779,  0.2601, -0.4692],\n",
      "         [ 0.0242, -0.0000, -0.0037,  ..., -0.4377,  0.1010, -0.0000]],\n",
      "\n",
      "        [[-0.1398, -0.1959, -0.0838,  ..., -0.3187, -0.0012, -0.5096],\n",
      "         [ 0.1887, -0.0745, -0.0247,  ..., -0.0521, -0.0983, -0.3634],\n",
      "         [ 0.0000, -0.0045,  0.1242,  ..., -0.1252, -0.0454, -0.4643],\n",
      "         ...,\n",
      "         [-0.0090, -0.3007,  0.0020,  ..., -0.5298,  0.1985, -0.2571],\n",
      "         [-0.1599, -0.0000, -0.0000,  ..., -0.4792,  0.1899, -0.3796],\n",
      "         [-0.0000, -0.2338,  0.0425,  ..., -0.2478,  0.2701, -0.4542]]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([2, 255, 512])\n",
      "tensor([[[-0.0218,  0.0131,  0.0029,  ..., -0.2638,  0.1159, -0.1051],\n",
      "         [-0.0193, -0.2618, -0.0020,  ..., -0.3170, -0.0903,  0.1028],\n",
      "         [ 0.2173,  0.0206, -0.0082,  ..., -0.1374,  0.1499,  0.0843],\n",
      "         ...,\n",
      "         [ 0.0195, -0.1588,  0.1321,  ..., -0.2930,  0.3810, -0.3947],\n",
      "         [ 0.0000, -0.1233, -0.0336,  ..., -0.3779,  0.2601, -0.4692],\n",
      "         [ 0.0242, -0.0000, -0.0037,  ..., -0.4377,  0.1010, -0.0000]],\n",
      "\n",
      "        [[-0.1398, -0.1959, -0.0838,  ..., -0.3187, -0.0012, -0.5096],\n",
      "         [ 0.1887, -0.0745, -0.0247,  ..., -0.0521, -0.0983, -0.3634],\n",
      "         [ 0.0000, -0.0045,  0.1242,  ..., -0.1252, -0.0454, -0.4643],\n",
      "         ...,\n",
      "         [-0.0090, -0.3007,  0.0020,  ..., -0.5298,  0.1985, -0.2571],\n",
      "         [-0.1599, -0.0000, -0.0000,  ..., -0.4792,  0.1899, -0.3796],\n",
      "         [-0.0000, -0.2338,  0.0425,  ..., -0.2478,  0.2701, -0.4542]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([2, 512])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 512])\n",
      "tensor([[-0.0191, -0.1676, -0.0185,  ..., -0.2895,  0.1795, -0.3113],\n",
      "        [-0.0341, -0.1999, -0.0262,  ..., -0.3535,  0.1839, -0.3387]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[0.2075],\n",
      "        [0.2015]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.2180],\n",
      "        [0.2106]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = pred.cuda()\n",
    "\n",
    "# x = torch.Tensor(x).long().cuda()\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "print(torch.sum(distr,1).shape)\n",
    "print(torch.sum(x_attn,1).shape)\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn_unsq,1)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret_pred =  model.classifier(distr)#(bs,1)\n",
    "print(ret_pred)\n",
    "\n",
    "\n",
    "print(model(x,x_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9922],\n",
      "        [0.9853]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9425, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4713, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterionGAN = torch.nn.MSELoss()\n",
    "pred_real = ret_label\n",
    "print(pred_real)\n",
    "loss_D_real = criterionGAN(pred_real, torch.ones((pred_real.shape[0],1),device='cuda'))\n",
    "print(loss_D_real)\n",
    "# Fake\n",
    "pred_fake = ret_pred\n",
    "print(pred_real)\n",
    "loss_D_fake = criterionGAN(pred_fake, torch.zeros((pred_fake.shape[0],1),device='cuda'))\n",
    "# Combined loss and calculate gradients\n",
    "print(loss_D_fake)\n",
    "\n",
    "\n",
    "loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "print(loss_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ",attention_mask=x_attn is essential for the encoder inpuit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We previously use torch.mean to (x after embedding)*x_attn but the mean will be small for the long sentences, now we use torch.sum(distr,1)/torch.sum(x_attn,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the D judge the sentence by it's length\n",
    "## small output for the first part, but large for the latter part, so the longer the higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## so we change the output to [batch,0,512] ie, onlythe first word's sumQ*V and than droppout and then classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAKE dont end with 1, cuz i use model.generate but not self.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
