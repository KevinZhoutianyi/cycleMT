{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "real =torch.Tensor(np.load('real.npy')).long().cuda()\n",
    "real_attn = torch.Tensor(np.load('real_attn.npy')).long().cuda()\n",
    "fake = torch.Tensor(np.load('fake.npy')).cuda()\n",
    "fake_attn = torch.Tensor(np.load('fake_attn.npy')).long().cuda()\n",
    "pred_fake =torch.Tensor( np.load('pred_fake.npy'))\n",
    "pred_real = torch.Tensor(np.load('pred_real.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8991],\n",
       "        [0.9323],\n",
       "        [0.7042]], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(real,real_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0240],\n",
       "        [0.0297],\n",
       "        [0.1063]], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(fake,fake_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Erst schaffthat gerade gerade der ESn demokratischen ParteiFraktion über solchevöllig e',\n",
       " 'Es sollte nicht dar die foriebung der prioritäten und deriken bei die',\n",
       " 'Ichens bitte ich die Herrn Kommissar daran,- und hierich']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d(torch.argmax(fake,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "def d(l):\n",
    "    return tokenizer.batch_decode(l,skip_special_tokens=True)\n",
    "def en(l):\n",
    "    return tokenize(l,tokenizer,512,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "en(['.'])\n",
    "label = en(['Somit hatte die Kommission bereits genügend Zeit, ihr Programm zu erarbeiten, und wir, um es kennenlernen und den Bürgern erklären zu können.',\n",
    " 'Frau Präsidentin, ich möchte zunächst darauf hinweisen, daß das, was Herr Poettering da sagt, nicht ganz logisch ist.',\n",
    " '(Das Parlament lehnt den Antrag ab.) Die Präsidentin.'])\n",
    "pred = en(['Erst schaffthat gerade gerade der ESn demokratischen ParteiFraktion über solchevöllig e',\n",
    " 'Es sollte nicht dar die foriebung der prioritäten und deriken bei die',\n",
    " 'Ichens bitte ich die Herrn Kommissar daran,- und hierich'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  264,  1538,  3310,    67,  5837,  5451,  3360,     3, 28078,  1743,\n",
       "             6,  1650,  7106,   170,     3,    49,  7987,     6,    64,   558,\n",
       "             6,   561,     3,    15,     7, 31844,    64,   177, 11726,    29,\n",
       "         24728,   170,   730,     5,     1,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [ 7672,     3, 20962,    77,     6,     3,   362,  6509,   170, 10151,\n",
       "          6101,     3, 31306,    35,     6,     3,    26,  7118,   211,     6,\n",
       "            47,  8816,  1908, 15583,    53,   836, 11865,     6,   311,  1897,\n",
       "             3, 25407,   229,     5,     1,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [   41, 17266, 13636,     3, 29723,   177, 20647,   703,     5,    61,\n",
       "           316,     3, 20962,    77,     5,     1,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  264,  1538,  3310,    67,  5837,  5451,  3360,     3, 28078,  1743,\n",
       "              6,  1650,  7106,   170,     3,    49,  7987,     6,    64,   558,\n",
       "              6,   561,     3,    15,     7, 31844,    64,   177, 11726,    29,\n",
       "          24728,   170,   730,     5,     1],\n",
       "         [ 7672,     3, 20962,    77,     6,     3,   362,  6509,   170, 10151,\n",
       "           6101,     3, 31306,    35,     6,     3,    26,  7118,   211,     6,\n",
       "             47,  8816,  1908, 15583,    53,   836, 11865,     6,   311,  1897,\n",
       "              3, 25407,   229,     5,     1],\n",
       "         [   41, 17266, 13636,     3, 29723,   177, 20647,   703,     5,    61,\n",
       "            316,     3, 20962,    77,     5,     1,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9344, 23336,   547,  6034,  6034,    74,     3,  3205,    29,     3,\n",
       "         30677,  1779, 17797, 31002,   510,  6466, 17139,     3,    15],\n",
       "        [ 1122,  3402,   311,   649,    67,    21,    23,    15, 11848,    74,\n",
       "          1884,  4130,    35,    64,    74,    23,  2217,   468,    67],\n",
       "        [ 1674,    35,     7,  9633,     3,   362,    67,  8816,    29,  5837,\n",
       "         11502,   291,     3, 13702,     6,    18,    64,  1382,   362]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(fake,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 9344, 23336,   547,  6034,  6034,    74,     3,  3205,    29,     3,\n",
       "          30677,  1779, 17797, 31002,   510,  6466, 17139,     3,    15,     1],\n",
       "         [ 1122,  3402,   311,   649,    67,    21,    23,    15, 11848,    74,\n",
       "           1884,  4130,    35,    64,    74,    23,  2217,   468,    67,     1],\n",
       "         [   27,  1559,     7,  9633,     3,   362,    67,  8816,    29,  5837,\n",
       "          11502,   291,     3, 13702,     6,    18,    64,  1382,   362,     1]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./model/D_A.pt').train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  264,  1538,  3310,    67,  5837,  5451,  3360,     3, 28078,  1743,\n",
      "             6,  1650,  7106,   170,     3,    49,  7987,     6,    64,   558,\n",
      "             6,   561,     3,    15,     7, 31844,    64,   177, 11726,    29,\n",
      "         24728,   170,   730,     5,     1],\n",
      "        [ 7672,     3, 20962,    77,     6,     3,   362,  6509,   170, 10151,\n",
      "          6101,     3, 31306,    35,     6,     3,    26,  7118,   211,     6,\n",
      "            47,  8816,  1908, 15583,    53,   836, 11865,     6,   311,  1897,\n",
      "             3, 25407,   229,     5,     1],\n",
      "        [   41, 17266, 13636,     3, 29723,   177, 20647,   703,     5,    61,\n",
      "           316,     3, 20962,    77,     5,     1,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]], device='cuda:0')\n",
      "torch.Size([3, 35])\n",
      "\t\t---------embedding\n",
      "torch.Size([3, 35, 512])\n",
      "tensor([[[ 18.1126,   3.8309,  28.8626,  ...,   1.6434, -15.6748,  17.7376],\n",
      "         [ -4.3621,   7.9871,  26.9876,  ...,   2.1121,  -1.9637,   1.1903],\n",
      "         [ 15.1748, -21.1126,   3.5184,  ..., -33.7358,  16.1126,   8.4873],\n",
      "         ...,\n",
      "         [  7.1121, -23.9876,  -2.2528,  ...,  16.4876,   0.6317,  10.1123],\n",
      "         [ -4.2684,   7.3934, -14.5498,  ...,  13.0498,  -2.7215,  -3.1278],\n",
      "         [ 12.6122,   8.1747, -11.6122,  ...,   7.9247,  -7.2996,   0.9325]],\n",
      "\n",
      "        [[  5.7371, -13.9248,  -3.9403,  ..., -12.1748,  -1.3387,  15.0498],\n",
      "         [ 11.2373,   8.0497,  14.1748,  ...,   9.7998,  -7.8621,  -3.5965],\n",
      "         [-27.6126, -22.9876,  16.2376,  ..., -41.9858, -40.9858,  14.6123],\n",
      "         ...,\n",
      "         [ 16.6126,  -3.0028,  -0.9871,  ...,  10.1123, -14.4873,   8.4873],\n",
      "         [ -4.2684,   7.3934, -14.5498,  ...,  13.0498,  -2.7215,  -3.1278],\n",
      "         [ 12.6122,   8.1747, -11.6122,  ...,   7.9247,  -7.2996,   0.9325]],\n",
      "\n",
      "        [[ -1.2137,   7.6121, -13.0498,  ...,   6.8934,   4.7371,   0.9950],\n",
      "         [ 17.4876,  -6.0496, -17.6126,  ..., -44.9858,  25.4876,  20.7376],\n",
      "         [ -1.1981, -17.1126,  16.6126,  ...,   8.3623, -32.9858,  -2.4246],\n",
      "         ...,\n",
      "         [ -2.0028,   0.2108,  -7.0809,  ...,  -0.3407,   2.6278,  -2.8778],\n",
      "         [ -2.0028,   0.2108,  -7.0809,  ...,  -0.3407,   2.6278,  -2.8778],\n",
      "         [ -2.0028,   0.2108,  -7.0809,  ...,  -0.3407,   2.6278,  -2.8778]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([3, 35, 512])\n",
      "tensor([[[ 0.3675,  0.1705,  0.4245,  ...,  0.0824, -0.0000, -0.5165],\n",
      "         [ 0.1578,  0.0957,  0.0000,  ..., -0.0624, -0.0053, -0.1920],\n",
      "         [ 0.0000,  0.0000, -0.1530,  ..., -0.1891,  0.1547, -0.1261],\n",
      "         ...,\n",
      "         [ 0.0461,  0.0357, -0.1273,  ...,  0.0345,  0.0904, -0.3311],\n",
      "         [ 0.0732,  0.1598, -0.0144,  ...,  0.1289, -0.1180, -0.1435],\n",
      "         [-0.0023,  0.0000, -0.0324,  ..., -0.0031,  0.0021, -0.0018]],\n",
      "\n",
      "        [[ 0.0937, -0.0925, -0.1641,  ..., -0.0868,  0.0000, -0.0403],\n",
      "         [-0.1717, -0.0019,  0.0081,  ...,  0.0000,  0.1188, -0.3774],\n",
      "         [-0.1676, -0.2215, -0.0831,  ..., -0.1704, -0.1224, -0.0000],\n",
      "         ...,\n",
      "         [-0.1466, -0.0212, -0.2113,  ..., -0.0565,  0.0494, -0.3405],\n",
      "         [ 0.0437,  0.2391, -0.0805,  ...,  0.0529, -0.2479, -0.1606],\n",
      "         [ 0.0000,  0.0302, -0.0081,  ..., -0.0432,  0.0036, -0.0008]],\n",
      "\n",
      "        [[ 0.0971,  0.2300, -0.0175,  ...,  0.0000, -0.1436, -0.1863],\n",
      "         [ 0.1464, -0.4109, -0.1365,  ..., -0.5328,  0.0305, -0.2492],\n",
      "         [ 0.0807, -0.0385, -0.1304,  ..., -0.1957,  0.0677, -0.1538],\n",
      "         ...,\n",
      "         [ 0.1825,  0.0252,  0.2195,  ..., -0.1183,  0.1026, -0.2844],\n",
      "         [ 0.0000,  0.0762,  0.1377,  ..., -0.0000, -0.0070, -0.1786],\n",
      "         [ 0.1280, -0.0175,  0.2355,  ..., -0.0852,  0.1224, -0.2029]]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([3, 35, 512])\n",
      "tensor([[[ 0.3675,  0.1705,  0.4245,  ...,  0.0824, -0.0000, -0.5165],\n",
      "         [ 0.1578,  0.0957,  0.0000,  ..., -0.0624, -0.0053, -0.1920],\n",
      "         [ 0.0000,  0.0000, -0.1530,  ..., -0.1891,  0.1547, -0.1261],\n",
      "         ...,\n",
      "         [ 0.0461,  0.0357, -0.1273,  ...,  0.0345,  0.0904, -0.3311],\n",
      "         [ 0.0732,  0.1598, -0.0144,  ...,  0.1289, -0.1180, -0.1435],\n",
      "         [-0.0023,  0.0000, -0.0324,  ..., -0.0031,  0.0021, -0.0018]],\n",
      "\n",
      "        [[ 0.0937, -0.0925, -0.1641,  ..., -0.0868,  0.0000, -0.0403],\n",
      "         [-0.1717, -0.0019,  0.0081,  ...,  0.0000,  0.1188, -0.3774],\n",
      "         [-0.1676, -0.2215, -0.0831,  ..., -0.1704, -0.1224, -0.0000],\n",
      "         ...,\n",
      "         [-0.1466, -0.0212, -0.2113,  ..., -0.0565,  0.0494, -0.3405],\n",
      "         [ 0.0437,  0.2391, -0.0805,  ...,  0.0529, -0.2479, -0.1606],\n",
      "         [ 0.0000,  0.0302, -0.0081,  ..., -0.0432,  0.0036, -0.0008]],\n",
      "\n",
      "        [[ 0.0971,  0.2300, -0.0175,  ...,  0.0000, -0.1436, -0.1863],\n",
      "         [ 0.1464, -0.4109, -0.1365,  ..., -0.5328,  0.0305, -0.2492],\n",
      "         [ 0.0807, -0.0385, -0.1304,  ..., -0.1957,  0.0677, -0.1538],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([3, 512])\n",
      "tensor([[ 0.0912,  0.0383, -0.1111,  ..., -0.0129,  0.1178, -0.1486],\n",
      "        [ 0.0256, -0.0142, -0.1113,  ..., -0.0171,  0.0224, -0.1963],\n",
      "        [ 0.0574,  0.0112, -0.0722,  ..., -0.0785, -0.0645, -0.1580]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[0.9373],\n",
      "        [0.8771],\n",
      "        [0.6696]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9131],\n",
      "        [0.8833],\n",
      "        [0.6597]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = label[0].cuda()\n",
    "#delte 0 give huge diff\n",
    "       \n",
    "# x = torch.Tensor(x).long().cuda()\n",
    "print(x)\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn_unsq,1)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret_label =  model.classifier(distr)#(bs,1)\n",
    "print(ret_label)\n",
    "\n",
    "print(model(x,x_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n",
      "\t\t---------embedding\n",
      "torch.Size([3, 20, 512])\n",
      "tensor([[[ -4.5809, -28.1126,  21.9876,  ...,  -5.9559, -39.7358,   4.1746],\n",
      "         [ 18.3626, -15.1748,  -6.3934,  ...,   2.0028,  23.9876,  14.9873],\n",
      "         [ 15.1748,  -6.5184, -12.6748,  ...,   3.5496,  -7.9246,  -6.7684],\n",
      "         ...,\n",
      "         [ 11.2373,   8.0497,  14.1748,  ...,   9.7998,  -7.8621,  -3.5965],\n",
      "         [ -7.9871,   6.1121,  18.4876,  ...,  14.2373,  14.7373,  -2.3465],\n",
      "         [ 12.6122,   8.1747, -11.6122,  ...,   7.9247,  -7.2996,   0.9325]],\n",
      "\n",
      "        [[ -2.1903,  -1.7606,  12.6123,  ..., -29.1126, -12.4873,  21.3626],\n",
      "         [ 17.4876, -18.8626,   1.7528,  ...,  11.6748,  11.3623,  19.4876],\n",
      "         [ -7.7996,   2.9246,  12.8623,  ...,  -9.0498, -31.4876,   9.1748],\n",
      "         ...,\n",
      "         [ -1.2293,  -4.7996,  -1.9871,  ...,   9.6748,  34.9858,   7.5496],\n",
      "         [  8.4248,   2.3153,  22.3626,  ...,  14.8623,  -4.7371,  -1.4246],\n",
      "         [ 12.6122,   8.1747, -11.6122,  ...,   7.9247,  -7.2996,   0.9325]],\n",
      "\n",
      "        [[ 15.7373,   3.0496,  14.8623,  ..., -10.9873,  -4.4246,   5.7059],\n",
      "         [-45.9858, -18.6126,  25.6126,  ..., -32.7358, -12.3623,  42.4858],\n",
      "         [  7.9871,   6.9246,  20.6126,  ...,  15.8623,  -3.1590,  19.8626],\n",
      "         ...,\n",
      "         [ -5.7996,  -1.0575,  -0.5848,  ...,  -1.9090, -17.3626,  12.7373],\n",
      "         [-21.7376, -22.3626,  22.6126,  ...,  -1.4793, -31.1126,  -3.1121],\n",
      "         [ 12.6122,   8.1747, -11.6122,  ...,   7.9247,  -7.2996,   0.9325]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([3, 20, 512])\n",
      "tensor([[[ 0.4137, -0.0909,  0.0501,  ..., -0.3040,  0.0132, -0.0824],\n",
      "         [ 0.0000,  0.0411, -0.0771,  ..., -0.0424,  0.1650, -0.3904],\n",
      "         [ 0.2216, -0.0636, -0.4391,  ..., -0.0000, -0.0623, -0.2217],\n",
      "         ...,\n",
      "         [ 0.3042, -0.0171, -0.1168,  ..., -0.0753, -0.0022, -0.1831],\n",
      "         [ 0.0978, -0.1987,  0.0295,  ..., -0.0000,  0.0335, -0.2663],\n",
      "         [-0.0236, -0.0282, -0.0066,  ...,  0.0175,  0.0238, -0.0000]],\n",
      "\n",
      "        [[ 0.3782,  0.0000,  0.0555,  ..., -0.3266, -0.0273, -0.0063],\n",
      "         [ 0.0682, -0.1858, -0.2985,  ..., -0.1628,  0.2530, -0.1436],\n",
      "         [ 0.1240, -0.1401, -0.2245,  ..., -0.2281,  0.0349, -0.0588],\n",
      "         ...,\n",
      "         [ 0.2229,  0.1126, -0.1125,  ..., -0.1310, -0.0773, -0.0000],\n",
      "         [ 0.0000,  0.1565, -0.1789,  ..., -0.0952,  0.0734, -0.4173],\n",
      "         [-0.0164,  0.0130, -0.0143,  ...,  0.0000,  0.0521, -0.0000]],\n",
      "\n",
      "        [[ 0.0363,  0.1483, -0.0816,  ..., -0.1632, -0.0000, -0.0000],\n",
      "         [-0.0358,  0.0667,  0.0179,  ..., -0.3824,  0.0098, -0.1380],\n",
      "         [ 0.1397,  0.0598,  0.1728,  ...,  0.0906, -0.0217,  0.0091],\n",
      "         ...,\n",
      "         [-0.1971, -0.0891, -0.1889,  ...,  0.0000, -0.0708, -0.0000],\n",
      "         [-0.0000, -0.1614, -0.0016,  ..., -0.1460, -0.1659, -0.2650],\n",
      "         [-0.0210,  0.0545,  0.0141,  ...,  0.0176,  0.0308, -0.0190]]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([3, 20, 512])\n",
      "tensor([[[ 0.4137, -0.0909,  0.0501,  ..., -0.3040,  0.0132, -0.0824],\n",
      "         [ 0.0000,  0.0411, -0.0771,  ..., -0.0424,  0.1650, -0.3904],\n",
      "         [ 0.2216, -0.0636, -0.4391,  ..., -0.0000, -0.0623, -0.2217],\n",
      "         ...,\n",
      "         [ 0.3042, -0.0171, -0.1168,  ..., -0.0753, -0.0022, -0.1831],\n",
      "         [ 0.0978, -0.1987,  0.0295,  ..., -0.0000,  0.0335, -0.2663],\n",
      "         [-0.0236, -0.0282, -0.0066,  ...,  0.0175,  0.0238, -0.0000]],\n",
      "\n",
      "        [[ 0.3782,  0.0000,  0.0555,  ..., -0.3266, -0.0273, -0.0063],\n",
      "         [ 0.0682, -0.1858, -0.2985,  ..., -0.1628,  0.2530, -0.1436],\n",
      "         [ 0.1240, -0.1401, -0.2245,  ..., -0.2281,  0.0349, -0.0588],\n",
      "         ...,\n",
      "         [ 0.2229,  0.1126, -0.1125,  ..., -0.1310, -0.0773, -0.0000],\n",
      "         [ 0.0000,  0.1565, -0.1789,  ..., -0.0952,  0.0734, -0.4173],\n",
      "         [-0.0164,  0.0130, -0.0143,  ...,  0.0000,  0.0521, -0.0000]],\n",
      "\n",
      "        [[ 0.0363,  0.1483, -0.0816,  ..., -0.1632, -0.0000, -0.0000],\n",
      "         [-0.0358,  0.0667,  0.0179,  ..., -0.3824,  0.0098, -0.1380],\n",
      "         [ 0.1397,  0.0598,  0.1728,  ...,  0.0906, -0.0217,  0.0091],\n",
      "         ...,\n",
      "         [-0.1971, -0.0891, -0.1889,  ...,  0.0000, -0.0708, -0.0000],\n",
      "         [-0.0000, -0.1614, -0.0016,  ..., -0.1460, -0.1659, -0.2650],\n",
      "         [-0.0210,  0.0545,  0.0141,  ...,  0.0176,  0.0308, -0.0190]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([3, 512])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 512])\n",
      "tensor([[ 0.0264,  0.0170, -0.1590,  ..., -0.1293,  0.0645, -0.2417],\n",
      "        [ 0.0970,  0.0811, -0.0209,  ..., -0.1237,  0.0877, -0.1421],\n",
      "        [ 0.0441, -0.0177, -0.0788,  ..., -0.1037, -0.0065, -0.1043]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[0.7525],\n",
      "        [0.8859],\n",
      "        [0.7189]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = pred[0].cuda()\n",
    "       \n",
    "# x = torch.Tensor(x).long().cuda()\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "print(torch.sum(distr,1).shape)\n",
    "print(torch.sum(x_attn,1).shape)\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn_unsq,1)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret_pred =  model.classifier(distr)#(bs,1)\n",
    "print(ret_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0427, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6226, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3327, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterionGAN = torch.nn.MSELoss()\n",
    "pred_real = ret_label\n",
    "loss_D_real = criterionGAN(pred_real, torch.ones((pred_real.shape[0],1),device='cuda'))\n",
    "print(loss_D_real)\n",
    "# Fake\n",
    "pred_fake = ret_pred\n",
    "loss_D_fake = criterionGAN(pred_fake, torch.zeros((pred_fake.shape[0],1),device='cuda'))\n",
    "# Combined loss and calculate gradients\n",
    "print(loss_D_fake)\n",
    "\n",
    "\n",
    "loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "print(loss_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ",attention_mask=x_attn is essential for the encoder inpuit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We previously use torch.mean to (x after embedding)*x_attn but the mean will be small for the long sentences, now we use torch.sum(distr,1)/torch.sum(x_attn,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the D judge the sentence by it's length\n",
    "## small output for the first part, but large for the latter part, so the longer the higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## so we change the output to [batch,0,512] ie, onlythe first word's sumQ*V and than droppout and then classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAKE dont end with 1, cuz i use model.generate but not self.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
