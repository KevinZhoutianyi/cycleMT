{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "def d(l):\n",
    "    return tokenizer.batch_decode(l,skip_special_tokens=True)\n",
    "def en(l):\n",
    "    return tokenize(l,tokenizer,512,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en(['.'])\n",
    "d([[3, 5, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_B = torch.zeros((3,4,5))\n",
    "fake_B[0][0][1] = 1\n",
    "fake_B[0][1][0] = 1\n",
    "fake_B[0][2][3] = 1\n",
    "fake_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a  = (fake_B[:, :,0] != 1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./model/D_B.pt').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9])\n",
      "\t\t---------embedding\n",
      "torch.Size([1, 9, 512])\n",
      "tensor([[[ 11.1214,   7.9124,  14.1215,  ...,   9.6461,  -7.8171,  -3.5773],\n",
      "         [ -4.2265,   7.2603, -14.4783,  ...,  12.9361,  -2.6143,  -3.0168],\n",
      "         [ 12.5392,   8.0912, -11.4242,  ...,   7.7111,  -7.1866,   0.8568],\n",
      "         ...,\n",
      "         [ -1.8653,   0.0877,  -6.9863,  ...,  -0.3819,   2.6015,  -2.7637],\n",
      "         [ -1.8653,   0.0877,  -6.9863,  ...,  -0.3819,   2.6015,  -2.7637],\n",
      "         [ -1.8653,   0.0877,  -6.9863,  ...,  -0.3819,   2.6015,  -2.7637]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([1, 9, 512])\n",
      "tensor([[[ 0.0265, -0.0650, -0.1443,  ...,  0.1849, -0.0047, -0.0293],\n",
      "         [-0.0760, -0.1067, -0.0037,  ..., -0.0455, -0.0526, -0.0423],\n",
      "         [ 0.0402,  0.0716, -0.1324,  ...,  0.0232,  0.0016,  0.0740],\n",
      "         ...,\n",
      "         [ 0.0589, -0.0985,  0.0467,  ...,  0.0047,  0.0442,  0.0597],\n",
      "         [ 0.0548, -0.0999,  0.0448,  ...,  0.0122,  0.0447,  0.0652],\n",
      "         [ 0.0524, -0.1017,  0.0442,  ...,  0.0110,  0.0437,  0.0637]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([1, 9, 512])\n",
      "tensor([[[ 0.0265, -0.0650, -0.1443,  ...,  0.1849, -0.0047, -0.0293],\n",
      "         [-0.0760, -0.1067, -0.0037,  ..., -0.0455, -0.0526, -0.0423],\n",
      "         [ 0.0402,  0.0716, -0.1324,  ...,  0.0232,  0.0016,  0.0740],\n",
      "         ...,\n",
      "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([1, 512])\n",
      "tensor([[-3.0786e-03, -3.3368e-02, -9.3453e-02, -2.0906e-01, -1.3075e-01,\n",
      "          4.5386e-02,  2.5686e-02, -6.8555e-02, -2.4324e-02,  2.5298e-02,\n",
      "         -6.2485e-02,  6.9358e-02, -3.7001e-02, -1.4351e-02, -1.7878e-01,\n",
      "          4.0022e-02,  5.2386e-02,  5.0885e-03,  4.7822e-02,  3.2828e-02,\n",
      "          2.4747e-02, -2.7549e-02,  2.9212e-02,  5.0151e-02, -1.1056e-02,\n",
      "         -4.1323e-02,  8.7266e-02, -4.7617e-02, -1.2365e-01, -3.9341e-02,\n",
      "         -7.0431e-02, -6.6899e-02, -1.7061e-01, -8.9918e-02,  2.0077e-02,\n",
      "          7.9293e-03,  1.7905e-02,  3.9361e-02, -1.0560e-01, -1.0968e-01,\n",
      "          5.2133e-02,  3.6747e-02, -4.2615e-03, -9.2318e-02, -1.9535e-03,\n",
      "          1.0509e-01,  1.1238e-01,  5.3060e-02, -2.3410e-02,  1.1079e-01,\n",
      "          6.9993e-02,  1.2653e-02,  4.5606e-02, -8.6937e-02, -1.8002e-02,\n",
      "          8.0350e-02, -9.7985e-02, -4.7619e-02, -4.5217e-02,  1.5714e-03,\n",
      "         -8.5890e-02,  5.4880e-02,  2.6992e-01, -4.1352e-03, -1.0792e-01,\n",
      "          5.8735e-02, -4.1461e-03, -1.1277e-02,  6.5070e-02,  3.7088e-03,\n",
      "         -2.7664e-03, -7.6846e-03, -9.3393e-03, -3.0755e-02, -6.7093e-02,\n",
      "         -5.3197e-03,  9.1742e-03,  1.9515e-01, -6.4827e-02,  5.5653e-02,\n",
      "          6.0487e-02,  2.7838e-02, -1.3706e-01, -9.7895e-02, -2.7327e-02,\n",
      "         -4.1332e-02, -1.0037e-01, -4.6603e-02,  2.7691e-02, -5.8549e-02,\n",
      "          3.4794e-02,  7.0862e-02, -6.9885e-02, -5.9534e-02,  5.4365e-02,\n",
      "         -7.3655e-02,  1.2213e-02, -6.4815e-02, -1.8414e-02, -1.6320e-02,\n",
      "          6.5109e-02,  4.1118e-02, -7.2085e-02, -2.9295e-02, -6.7061e-04,\n",
      "         -5.6496e-02,  4.6514e-02, -8.2791e-02, -2.7701e-03,  2.9670e-02,\n",
      "         -7.4655e-02, -2.7441e-02,  1.1738e-01,  6.5226e-03, -3.7978e-02,\n",
      "         -1.8981e-01,  5.7506e-02,  5.5367e-02, -2.0183e-02, -3.4496e-02,\n",
      "          1.0617e-01, -1.0520e-01, -3.7762e-02, -8.8494e-02,  6.6767e-03,\n",
      "         -2.3971e-02, -1.1095e-01, -6.7490e-02, -3.9337e-01, -2.4495e-05,\n",
      "         -1.5540e-02, -1.0397e-01, -1.4478e-01,  6.4578e-02, -7.4409e-02,\n",
      "         -8.8558e-02,  2.1824e-02,  8.9345e-02,  2.2181e-02,  1.3748e-01,\n",
      "         -1.0595e-01, -2.4259e-01,  3.9596e-02,  3.3267e-03, -6.4444e-02,\n",
      "         -6.1011e-02,  1.1212e-01,  2.6648e-01,  2.8917e-01, -3.4867e-01,\n",
      "          3.5951e-02, -1.1929e-01, -7.0040e-02,  1.3813e-01, -7.3902e-04,\n",
      "         -1.3301e-01, -8.1091e-02, -2.7685e-02,  4.0938e-02,  6.7403e-02,\n",
      "         -1.5720e-02, -2.9008e-02, -1.6833e-02,  4.0618e-02, -1.2004e-01,\n",
      "         -1.0967e-02, -7.0984e-02, -1.1185e-01,  6.7749e-02, -1.8254e-02,\n",
      "         -1.9698e-01,  9.5770e-02,  7.2777e-02, -7.7502e-02,  1.1465e-01,\n",
      "         -6.6036e-02,  1.2232e-01,  2.6112e-01,  8.0869e-02, -3.8510e-02,\n",
      "         -4.1673e-02, -1.2553e-01,  1.3657e-01, -6.3551e-02,  1.6467e-02,\n",
      "         -7.6605e-03, -8.6292e-02,  1.0395e-03,  8.0006e-02, -6.8018e-02,\n",
      "          7.7134e-02, -1.9548e-01,  1.7891e-02,  8.7570e-03, -9.8116e-02,\n",
      "          2.4891e-02, -2.0449e-02,  2.3116e-02, -3.1228e-02, -6.2150e-02,\n",
      "          2.6680e-01, -8.0883e-02, -4.3509e-02,  1.2283e-01, -4.7642e-02,\n",
      "         -5.9771e-02,  8.1233e-02,  2.2780e-01, -4.5863e-02, -1.4819e-02,\n",
      "         -2.3499e-02,  3.3041e-03, -1.3347e-01, -2.7944e-02, -1.0866e-01,\n",
      "         -6.0742e-02, -2.1523e-03, -1.7595e-01,  4.5412e-02,  7.6377e-02,\n",
      "         -8.3646e-02,  3.5044e-02,  7.8955e-02,  4.1193e-03,  1.3109e-01,\n",
      "          1.4080e-02, -7.0472e-02,  1.0916e-01, -3.3223e-02, -1.5785e-02,\n",
      "          2.8783e-02,  1.1863e-02,  8.1627e-02, -6.3457e-02, -1.5827e-02,\n",
      "         -1.2536e-01,  2.5475e-02,  4.9519e-03,  6.1396e-02,  1.8586e-02,\n",
      "          1.1837e-01,  3.8511e-02,  2.9809e-02,  2.2645e-02, -1.3752e-01,\n",
      "         -1.2959e-03,  1.7143e-02,  1.8733e-01, -2.6276e-02,  1.8540e-01,\n",
      "         -3.8934e-02,  5.4908e-02,  1.1032e-01, -3.6512e-02, -1.5796e-02,\n",
      "          6.7932e-02, -5.3265e-02, -9.7433e-02,  5.8534e-02,  9.5219e-02,\n",
      "          1.3534e-05, -1.7180e-01,  1.1947e-01,  8.9316e-02,  7.6265e-02,\n",
      "         -2.2215e-02,  6.2093e-03, -5.6007e-02,  6.4895e-02,  5.4048e-02,\n",
      "          6.5675e-02, -3.9628e-02,  9.5796e-02, -6.3603e-02, -1.8665e-02,\n",
      "         -5.8312e-02, -8.9354e-02, -7.6716e-03,  5.2340e-02,  4.1314e-02,\n",
      "         -1.7114e-02, -1.6428e-01,  2.5349e-02,  5.6654e-02, -6.5357e-02,\n",
      "          1.6220e-01,  1.9880e-02, -9.7023e-02,  4.6746e-02, -1.3659e-02,\n",
      "         -1.2571e-01,  6.8870e-02, -2.0130e-02,  1.0966e-01,  6.2573e-02,\n",
      "          7.1086e-02,  2.4783e-02,  4.9288e-02, -1.2396e-01,  1.6417e-01,\n",
      "         -7.3054e-02, -6.3393e-02,  7.3382e-02, -7.5622e-02,  2.0243e-02,\n",
      "         -3.7887e-02,  7.3219e-03,  4.6680e-02, -1.0654e-01, -7.7209e-02,\n",
      "         -2.1646e-01,  9.3869e-02,  5.3160e-02,  1.1901e-01, -1.4546e-01,\n",
      "         -2.9477e-02, -3.0019e-03,  2.5888e-02, -4.6997e-03, -5.7329e-02,\n",
      "         -6.6431e-03,  8.7943e-02,  2.7997e-02, -2.6508e-02,  7.1521e-03,\n",
      "         -2.5915e-02,  1.7814e-02, -8.5036e-02,  6.8563e-02, -1.5581e-01,\n",
      "          1.0560e-02, -9.4036e-03, -3.5637e-02,  2.6354e-02, -4.3794e-03,\n",
      "          1.1381e-01, -1.3396e-01, -1.1412e-01,  3.6539e-02,  4.9514e-02,\n",
      "          2.1680e-02, -1.5551e-02,  5.5768e-02,  1.0292e-01,  3.4478e-02,\n",
      "          7.2413e-02, -1.3660e-01, -1.4769e-02,  1.4094e-01, -1.1170e-01,\n",
      "          8.1654e-02, -8.1768e-02, -8.2624e-02,  1.6917e-02,  1.2922e-01,\n",
      "         -2.5786e-01,  1.2279e-02, -8.3211e-02, -3.8207e-02,  3.5042e-01,\n",
      "          6.0804e-02,  6.1922e-02, -1.0164e-01,  1.7784e-03, -1.0262e-01,\n",
      "          3.2723e-02,  7.0845e-02,  9.9650e-02,  5.5808e-02,  3.9658e-02,\n",
      "          4.2938e-02,  7.1957e-02,  9.3587e-03,  1.2993e-01,  2.3898e-01,\n",
      "         -3.8285e-02, -1.4554e-02,  1.1991e-02, -2.6572e-02, -6.2422e-02,\n",
      "         -9.4055e-02,  2.2693e-02,  1.4117e-01, -4.9581e-03, -1.9396e-02,\n",
      "          8.0756e-02,  1.9281e-01, -2.7783e-02, -1.7227e-02,  7.5527e-02,\n",
      "         -1.4551e-01,  5.5545e-02, -8.7520e-02,  7.3521e-02, -1.8755e-02,\n",
      "          8.0425e-02, -5.7605e-02,  9.5004e-02, -1.0189e-01, -1.2545e-01,\n",
      "          6.5601e-02,  1.0169e-01, -1.4051e-01,  7.0062e-02, -6.5815e-02,\n",
      "          4.1132e-02, -3.5540e-02, -9.1454e-02,  1.4634e-01,  1.9542e-02,\n",
      "         -1.4980e-01, -2.0343e-01, -6.6211e-03,  2.3993e-02,  2.6880e-02,\n",
      "          7.5629e-02,  1.0256e-01, -7.5255e-02, -5.8072e-02,  7.6741e-02,\n",
      "          1.1494e-01,  7.9965e-02,  1.2134e-02,  9.5685e-02,  1.2426e-01,\n",
      "          6.0153e-02,  1.0755e-02, -3.5734e-02, -1.7622e-01, -1.9604e-02,\n",
      "         -1.1680e-02, -2.1320e-02,  8.3902e-02,  1.5062e-01,  1.3221e-02,\n",
      "         -1.3296e-02, -7.4245e-02,  6.8938e-03, -5.1416e-02, -3.2069e-04,\n",
      "         -1.4090e-02, -1.5426e-01, -1.0407e-02, -1.3840e-02, -1.5226e-01,\n",
      "          1.3574e-01,  5.7709e-02,  2.2450e-02, -9.5761e-02, -3.3489e-03,\n",
      "         -1.2704e-02, -1.1696e-01,  4.1252e-02, -1.0904e-01,  3.3256e-02,\n",
      "         -7.6493e-03,  1.4291e-04, -1.0197e-01,  1.2138e-01,  3.7212e-02,\n",
      "          1.2729e-01, -4.3877e-02,  7.1186e-02,  6.9786e-02,  4.1000e-02,\n",
      "          5.3724e-02,  7.2474e-02,  6.5546e-02, -3.8412e-03,  7.7076e-02,\n",
      "          1.9215e-02, -1.3120e-02, -3.7862e-03, -2.1289e-03,  7.6237e-02,\n",
      "          1.0115e-01, -1.9139e-02, -6.9844e-02, -1.0032e-01,  1.4896e-01,\n",
      "          9.1075e-03,  2.3809e-02,  4.3667e-02, -1.0321e-01, -4.0005e-02,\n",
      "         -5.6203e-02,  2.1786e-02,  1.4033e-02, -7.0520e-02, -1.0171e-01,\n",
      "         -5.7790e-02,  1.4215e-02,  3.2363e-01, -9.0392e-02, -2.3334e-02,\n",
      "         -6.8182e-02,  3.1695e-02,  3.6182e-02, -8.3817e-02,  7.7416e-02,\n",
      "          6.3724e-02, -6.0748e-02,  2.5143e-01,  6.7882e-03,  7.7957e-02,\n",
      "         -1.3103e-01,  1.0161e-01, -4.9370e-02,  7.0548e-02,  5.4210e-02,\n",
      "         -1.8569e-02,  8.2141e-04]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[0.2042]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = [[3, 5, 1, \n",
    "0,0,0,0,0,0]]\n",
    "\n",
    "             #delte 0 give huge diff\n",
    "       \n",
    "x = torch.Tensor(x).long().cuda()\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn,1)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret =  model.classifier(distr)#(bs,1)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 90])\n",
      "\t\t---------embedding\n",
      "torch.Size([1, 90, 512])\n",
      "tensor([[[  5.9575,  -7.8626,  14.3691,  ..., -21.0999,   3.9790,  12.4444],\n",
      "         [ 23.7227,  -2.7165, -15.8925,  ...,  25.1313,  -6.2685,   2.0443],\n",
      "         [  5.2379, -38.2814,  46.2259,  ...,  -6.0190, -31.0495,  -2.6075],\n",
      "         ...,\n",
      "         [ -1.8653,   0.0877,  -6.9863,  ...,  -0.3819,   2.6015,  -2.7637],\n",
      "         [ -1.8653,   0.0877,  -6.9863,  ...,  -0.3819,   2.6015,  -2.7637],\n",
      "         [ -1.8653,   0.0877,  -6.9863,  ...,  -0.3819,   2.6015,  -2.7637]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([1, 90, 512])\n",
      "tensor([[[ 0.3019, -0.0240,  0.1268,  ..., -0.4100, -0.0095,  0.0712],\n",
      "         [ 0.3483, -0.0669,  0.2359,  ..., -0.2571,  0.0019,  0.0086],\n",
      "         [ 0.0245,  0.1051,  0.3846,  ..., -0.0195,  0.0081,  0.0784],\n",
      "         ...,\n",
      "         [ 0.1691, -0.0201,  0.1806,  ..., -0.1905,  0.0086,  0.1166],\n",
      "         [ 0.1677, -0.0203,  0.1807,  ..., -0.1902,  0.0084,  0.1153],\n",
      "         [ 0.1665, -0.0203,  0.1794,  ..., -0.1869,  0.0068,  0.1132]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([1, 90, 512])\n",
      "tensor([[[ 0.3019, -0.0240,  0.1268,  ..., -0.4100, -0.0095,  0.0712],\n",
      "         [ 0.3483, -0.0669,  0.2359,  ..., -0.2571,  0.0019,  0.0086],\n",
      "         [ 0.0245,  0.1051,  0.3846,  ..., -0.0195,  0.0081,  0.0784],\n",
      "         ...,\n",
      "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([1, 512])\n",
      "tensor([[-6.1565e-03,  1.0086e-03,  2.3846e-02,  4.4845e-02,  4.7016e-02,\n",
      "          1.2220e-01, -1.4124e-01,  7.7164e-02, -1.3255e-01,  3.7706e-02,\n",
      "         -3.0048e-02, -2.2230e-01, -6.4026e-02,  3.0203e-02, -1.4468e-01,\n",
      "         -7.5396e-02,  4.1746e-02,  1.8495e-01,  8.2945e-02, -6.8377e-02,\n",
      "          1.7523e-02, -3.8306e-02,  1.1100e-01, -1.1782e-01, -5.1114e-02,\n",
      "         -1.6539e-01,  5.4354e-03, -4.9170e-02, -8.2544e-02,  2.1148e-02,\n",
      "          1.0662e-03,  8.5223e-02, -1.7165e-02,  1.9320e-02, -4.9575e-02,\n",
      "         -9.7535e-03, -6.7045e-03,  4.8872e-02,  7.6656e-02, -7.4168e-03,\n",
      "         -9.2084e-02,  2.4486e-02,  1.4430e-01, -4.7481e-02,  2.2049e-01,\n",
      "          1.0764e-03,  3.0048e-02,  1.0658e-01, -2.5587e-02,  2.5810e-02,\n",
      "         -8.5307e-02,  4.4004e-02,  1.5840e-02, -9.7772e-02, -7.3787e-02,\n",
      "         -4.6467e-02, -4.6882e-02, -3.9564e-02, -4.5050e-02, -1.6089e-01,\n",
      "         -6.6931e-02,  1.2537e-01,  2.1029e-01,  1.3831e-01,  3.4217e-01,\n",
      "          1.4460e-02, -3.1039e-02,  4.9483e-02, -1.2536e-01,  1.8804e-01,\n",
      "          1.0545e-01,  5.0598e-02,  1.2680e-02, -5.4179e-02,  3.5614e-02,\n",
      "         -4.7351e-02,  1.2806e-02,  8.7090e-02, -1.5711e-01,  7.7968e-02,\n",
      "          8.1727e-02, -1.0363e-01,  1.1841e-01, -1.0856e-01,  2.2325e-02,\n",
      "          2.6942e-02,  1.6288e-02, -1.4169e-01, -3.5481e-02,  1.5727e-01,\n",
      "          1.9540e-02, -7.8439e-03,  3.1233e-02, -2.6572e-02, -2.2930e-03,\n",
      "         -9.6435e-02, -8.7663e-02, -2.8104e-02,  1.5194e-01, -2.4461e-02,\n",
      "          5.1129e-02, -6.6215e-02, -3.4358e-02, -3.2027e-02, -3.2439e-02,\n",
      "          1.9789e-02,  8.0666e-03, -1.0022e-01, -3.0368e-02, -2.7630e-02,\n",
      "          4.4018e-02, -5.7011e-03,  8.0342e-03, -9.3057e-02,  3.6287e-03,\n",
      "         -1.4427e-01, -6.7749e-02,  6.8138e-02,  4.0623e-03,  1.7395e-02,\n",
      "          1.1148e-02,  1.0964e-01, -7.5343e-03,  4.6828e-02, -4.4334e-02,\n",
      "          5.3114e-02,  2.7854e-01, -1.9876e-02,  1.7710e-02,  4.0837e-02,\n",
      "         -8.2356e-02,  3.9935e-02,  1.3197e-02,  1.8734e-02,  3.7766e-02,\n",
      "         -9.5165e-02,  1.2372e-01, -1.4419e-01,  5.6645e-02,  2.2495e-02,\n",
      "          1.2556e-03, -3.9591e-01, -9.1895e-03, -2.7108e-02,  5.7392e-03,\n",
      "         -1.0074e-01, -1.1038e-01,  1.3871e-01,  7.1991e-02, -2.7767e-01,\n",
      "          2.1016e-03,  5.3024e-02, -6.5704e-02,  1.2766e-01,  6.7283e-02,\n",
      "          2.0542e-02,  1.2488e-02,  6.3490e-02,  6.6807e-02,  1.9307e-01,\n",
      "         -3.7766e-02,  6.8922e-02, -2.4549e-02,  1.4144e-02, -6.2530e-02,\n",
      "          3.4785e-02, -3.7979e-02, -3.6024e-02,  8.2512e-02,  1.7919e-02,\n",
      "          8.6221e-02,  3.6966e-02, -2.2850e-02, -5.6300e-02,  3.7641e-02,\n",
      "          1.1797e-01,  4.6317e-02, -2.0841e-02,  4.7508e-02, -3.9440e-02,\n",
      "         -5.0867e-02, -7.6726e-02, -7.4334e-02,  4.5682e-02,  2.8249e-02,\n",
      "         -3.0244e-02, -3.2103e-02, -1.1197e-01,  2.3464e-02,  1.1813e-01,\n",
      "          1.0712e-01, -3.1263e-03,  3.2993e-03, -5.0818e-02, -3.1320e-02,\n",
      "         -1.9857e-02, -4.5181e-02,  7.6083e-02,  3.9183e-02,  6.3876e-02,\n",
      "          1.1694e-01, -8.8231e-02,  3.8470e-03, -5.1758e-02, -4.2112e-02,\n",
      "         -7.8482e-03, -1.6847e-01,  8.4440e-02,  1.3529e-02, -2.1291e-02,\n",
      "          1.1827e-01,  3.2509e-02, -3.0013e-02,  1.4695e-02,  3.2117e-04,\n",
      "         -5.3107e-02,  8.8796e-02, -2.9246e-02,  2.4870e-01,  1.6854e-03,\n",
      "          3.2946e-02, -2.3758e-02,  2.1556e-01,  2.2944e-02, -7.7734e-02,\n",
      "         -9.0543e-02,  6.2086e-02,  2.0647e-02, -3.1260e-01,  1.3555e-01,\n",
      "         -1.4685e-02, -4.2406e-02, -6.1440e-02, -1.8125e-01, -8.6886e-02,\n",
      "          1.5007e-02, -7.3023e-02, -7.3524e-02,  4.9191e-02,  8.3926e-02,\n",
      "          2.6053e-02,  3.5400e-02,  2.1701e-02, -1.9793e-02,  9.9232e-02,\n",
      "         -1.0736e-01, -7.2426e-03,  7.4822e-02, -1.8011e-03,  2.0780e-02,\n",
      "          7.8923e-02,  6.6309e-02, -8.3310e-03, -1.0988e-02, -8.8052e-02,\n",
      "          4.6837e-02,  8.1077e-02,  5.4867e-02,  7.5970e-03, -6.3527e-02,\n",
      "         -1.3275e-06, -8.8480e-03, -5.4476e-03, -1.1794e-01, -4.8015e-03,\n",
      "          7.0431e-02,  5.0425e-02, -8.2234e-03,  6.6416e-03, -7.8043e-02,\n",
      "          5.6269e-02,  5.6659e-02, -5.7442e-02, -2.4871e-01,  2.7189e-02,\n",
      "         -2.4587e-02, -1.6730e-02,  1.4701e-02,  4.7418e-02, -2.8126e-02,\n",
      "         -6.5975e-02, -9.5419e-02,  4.9962e-02,  3.7026e-02,  2.3216e-02,\n",
      "         -6.4873e-02, -2.1237e-02, -2.3438e-01,  6.4946e-02, -1.8425e-01,\n",
      "         -9.9309e-02, -3.4278e-02,  7.7116e-02,  3.7049e-02, -6.4959e-02,\n",
      "         -4.8182e-02,  9.3095e-03, -3.0203e-02,  3.1910e-02,  1.2288e-02,\n",
      "         -6.2672e-02,  6.9608e-02,  3.5789e-02,  2.7160e-02,  4.5646e-02,\n",
      "          3.5637e-02,  4.8563e-02,  5.9278e-02, -1.1719e-02,  6.0092e-02,\n",
      "          1.2777e-02, -2.2363e-02, -7.3013e-03, -9.2030e-02, -9.7331e-02,\n",
      "          6.5912e-02, -4.4058e-02,  9.8274e-04,  4.2796e-03, -7.7384e-03,\n",
      "         -1.0886e-01,  4.7417e-02,  2.2788e-02, -2.2657e-02,  1.6195e-02,\n",
      "          2.8997e-02, -5.5011e-04, -7.7175e-02, -1.1077e-02, -1.7947e-01,\n",
      "         -2.3204e-02, -1.5154e-01, -1.2395e-01,  9.0490e-02, -2.2054e-02,\n",
      "          2.2829e-04, -1.5304e-01, -7.1948e-02,  1.7343e-02,  1.8794e-01,\n",
      "          1.9024e-01,  6.4548e-02, -4.5179e-02,  1.3058e-02,  1.0225e-01,\n",
      "          7.5069e-02,  4.6683e-02,  9.9893e-02, -3.1347e-02,  3.1695e-02,\n",
      "          2.5076e-02,  3.5014e-02, -1.2222e-01, -6.2078e-02,  4.9648e-02,\n",
      "         -1.5159e-02, -5.4654e-02,  4.5930e-02,  1.1227e-01, -6.4655e-02,\n",
      "          3.2379e-02,  3.6457e-02,  5.0647e-02,  6.9537e-02, -4.3308e-02,\n",
      "         -2.9985e-02,  7.4895e-02, -8.0030e-02, -7.7583e-03, -7.3349e-02,\n",
      "         -2.0447e-02,  5.1891e-02,  3.2627e-02, -1.0262e-01,  8.7624e-02,\n",
      "          4.4511e-02,  7.7655e-02,  3.9682e-02, -2.2547e-04,  6.8897e-02,\n",
      "          5.9120e-02,  2.0390e-02,  9.2426e-02, -2.2941e-02, -6.3366e-02,\n",
      "          3.9394e-02, -1.3649e-01, -2.0540e-02, -5.9189e-02, -1.3007e-03,\n",
      "         -5.5859e-02,  8.8371e-03,  3.5390e-02,  1.2379e-03, -1.3174e-02,\n",
      "          9.0415e-02,  3.0881e-02,  5.4690e-03, -2.7508e-02, -2.1905e-01,\n",
      "         -1.2769e-02, -6.3096e-02, -6.0733e-02,  8.2059e-02, -2.6770e-02,\n",
      "         -4.9946e-03, -8.6943e-02, -4.7517e-02,  5.5426e-02,  5.2080e-02,\n",
      "         -2.5650e-01, -3.1824e-02, -8.0752e-02, -2.5469e-02,  1.0201e-03,\n",
      "          1.2777e-01,  1.5364e-01,  1.3664e-01, -2.8974e-02, -3.6980e-02,\n",
      "          1.1615e-01, -3.8054e-03,  7.2444e-03,  7.3052e-02, -2.9390e-02,\n",
      "          4.8029e-02, -7.2372e-02, -4.2213e-02, -8.9172e-02, -3.1505e-02,\n",
      "          7.3631e-03, -4.2943e-02,  1.0210e-02, -8.1052e-03,  2.9314e-02,\n",
      "          5.9380e-02, -1.6356e-03,  3.4372e-03,  5.5467e-02,  4.1278e-02,\n",
      "         -3.9764e-02, -7.8312e-02,  5.8270e-02,  3.0540e-02,  2.8982e-02,\n",
      "         -8.6597e-03,  4.8489e-02, -1.0829e-01,  1.2200e-02,  5.4949e-02,\n",
      "         -1.4998e-01,  1.0885e-02, -2.2305e-02,  2.5688e-02, -2.0330e-02,\n",
      "         -1.0516e-02, -1.6962e-02,  5.1943e-02,  7.4228e-03, -2.7122e-02,\n",
      "         -1.0851e-02, -1.0717e-02,  1.1959e-02,  2.3872e-03,  3.3530e-02,\n",
      "          5.5510e-02, -7.8789e-02, -6.2519e-02, -1.0049e-01,  8.7727e-02,\n",
      "          2.9701e-03,  8.6867e-02, -5.9095e-02, -2.9684e-02, -1.2713e-03,\n",
      "          1.8889e-01, -2.1186e-02,  7.2168e-02,  2.7460e-02, -1.1545e-03,\n",
      "          1.0723e-01,  6.8750e-02, -7.8705e-02, -2.8352e-02, -2.4178e-03,\n",
      "          5.5379e-02, -1.0840e-02,  6.6937e-02,  3.2433e-02,  3.6935e-02,\n",
      "         -1.4791e-01,  1.0436e-01,  5.3995e-02,  2.6513e-02, -4.3242e-02,\n",
      "         -9.2973e-03,  2.5842e-03,  8.3443e-02, -7.6370e-02, -2.1652e-02,\n",
      "          8.1107e-02,  2.8894e-01, -1.3266e-01,  6.2343e-02, -3.3660e-02,\n",
      "         -1.6671e-01,  7.5409e-02,  5.5595e-02,  8.6494e-02, -8.5017e-02,\n",
      "         -1.9846e-02,  1.2259e-01]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[0.6611]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = [[10180,   351, 19810,     7,    24,     6,    38,     8,  8160,  8675,\n",
    "            13,     8,  1611, 15375,     6,    62,   225,     6,    11,  5071,\n",
    "           398,     6,  2173,   981, 12361,   485,    11, 13567,    16,     8,\n",
    "          1028,  5808,     7,  1194,    11,  6572,    53,    13,    48,   540,\n",
    "             6, 10321,    69, 12123,     7,    11,   811,     7,  9098,    12,\n",
    "             3,  9582,   125,    33,   801,    38,    96, 12097,    21,   540,\n",
    "           121, 15600,    16,     8,  5334,    18, 14259,   433,     5,     1,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]\n",
    "\n",
    "             #delte 0 give huge diff\n",
    "       \n",
    "x = torch.Tensor(x).long().cuda()\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn,1)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret =  model.classifier(distr)#(bs,1)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9008]], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# x_emb = model.embedding(x_)#\n",
    "# print('x_emb.shape',x_emb.shape)\n",
    "# distr = model.encoder(inputs_embeds=x_emb).last_hidden_state#(bs,sentence length,512)\n",
    "# print('distr.shape',distr.shape)\n",
    "# x_attn= x_attn.unsqueeze(-1)\n",
    "# print('x_attn.shape',x_attn.shape)\n",
    "# distr = torch.mul(distr,x_attn)#previously\n",
    "\n",
    "# print('mutil',torch.sum(distr,1))\n",
    "# print('count',torch.sum(x_attn,1))\n",
    "# distr = torch.sum(distr,1)/torch.sum(x_attn,1)#(bs,512)\n",
    "# print(torch.mean(distr,-1))\n",
    "# print('mea',distr.shape)\n",
    "\n",
    "# ret =  model.classifier(distr)#(bs,1)\n",
    "# print(ret)\n",
    "# ret = model.relu(ret)#(bs,1)\n",
    "# print(ret)\n",
    "\n",
    "\n",
    "\n",
    "a_pred_dis = model(x_,x_attn)\n",
    "a_pred_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ",attention_mask=x_attn is essential for the encoder inpuit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We previously use torch.mean to (x after embedding)*x_attn but the mean will be small for the long sentences, now we use torch.sum(distr,1)/torch.sum(x_attn,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the D judge the sentence by it's length\n",
    "## small output for the first part, but large for the latter part, so the longer the higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## so we change the output to [batch,0,512] ie, onlythe first word's sumQ*V and than droppout and then classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
