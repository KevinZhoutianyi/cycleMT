{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "def d(l):\n",
    "    return tokenizer.batch_decode(l,skip_special_tokens=True)\n",
    "def en(l):\n",
    "    return tokenize(l,tokenizer,512,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_B = torch.zeros((3,4,5))\n",
    "fake_B[0][0][1] = 1\n",
    "fake_B[0][1][0] = 1\n",
    "fake_B[0][2][3] = 1\n",
    "fake_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a  = (fake_B[:, :,0] != 1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./model/D_A.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,y_attn = en(['a certain number of workers in certain operating sites, on which this type of directive a due due to practical circumstances circumstances, only a difficult difficult to to use.....', 'I myself from Schottland, a region with one of the largest fishing fishing fleets in the EU EU, will again again on the problem of fisheries and the definition of the definition of the working time..................'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label,label_attn = en(['There would remain though some workers in some working places where practical issues do not readily lend themselves to the implementation of this type of directive.', 'Coming from Scotland, with one of the largest EU fishing fleets, I would like once again to focus on the problem of fisheries and the defining of working time.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distr.shape torch.Size([2, 50, 512])\n",
      "distr.shape torch.Size([2, 512])\n",
      "ret.shape torch.Size([2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1544],\n",
       "        [0.0000]], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(y.cuda(),y_attn.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distr.shape torch.Size([2, 37, 512])\n",
      "distr.shape torch.Size([2, 512])\n",
      "ret.shape torch.Size([2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0700],\n",
       "        [0.0000]], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(label.cuda(),label_attn.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 70])\n",
      "torch.Size([2, 70])\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = [[   37,   351,    13,     8,   657,  1267,    24,    62,     6,    38,\n",
    "          8160,  8675,    13,     8,  1611, 15375,     7,     6,   225,  2173,\n",
    "           981,  1131,  1106,  6833,    11, 13567,    16,     8,  1942,    13,\n",
    "           175,  2731,    11,     8,  6572,    53,    24,     3,    35,  5756,\n",
    "             7,     6,    11,    28,    69, 12123,     7,    11,  7548,     6,\n",
    "            62,   241,    12,  1984,   125,    54,    36,   261,    38,    46,\n",
    "         11169,    13,     8,   167, 24367,     3, 30786,  1127,     5,     1],\n",
    "        [    3, 23531,     6,    66,   396,   557,     6,  1450,  4505,     7,\n",
    "            33,   271,  1869,    30,  1195,     3,  2544,   772,    44,     8,\n",
    "          1849,    13,     8,  2486,    33,   914,    59,   780,   964,     5,\n",
    "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]\n",
    "             \n",
    "\n",
    "x_ = torch.Tensor(x).long().cuda()\n",
    "x_ = x_[:,:]\n",
    "x_attn = (x_>0.5).long()\n",
    "x_attn_ =x_attn\n",
    "print(x_.shape)\n",
    "print(x_attn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_emb.shape torch.Size([2, 70, 512])\n",
      "distr.shape torch.Size([2, 70, 512])\n",
      "x_attn.shape torch.Size([2, 70, 1])\n",
      "mutil tensor([[[ 0.6085,  0.0000, -0.0000,  ..., -0.1359,  0.0513, -0.2267],\n",
      "         [ 0.5553,  0.1303, -0.0000,  ...,  0.0871,  0.0641, -0.0583],\n",
      "         [-0.0642,  0.0000, -0.0356,  ..., -0.0681,  0.1548, -0.1681],\n",
      "         ...,\n",
      "         [ 0.0430,  0.0789, -0.0459,  ...,  0.1795,  0.0000, -0.1211],\n",
      "         [-0.1309,  0.3895, -0.1361,  ..., -0.0000, -0.0494, -0.1605],\n",
      "         [ 0.0418,  0.0000, -0.0425,  ..., -0.0185, -0.0090, -0.0287]],\n",
      "\n",
      "        [[ 0.1594,  0.2739, -0.0671,  ...,  0.0419,  0.0021, -0.3445],\n",
      "         [ 0.7014,  0.3530,  0.2514,  ..., -0.1553, -0.0000, -0.0291],\n",
      "         [ 0.0173,  0.2503,  0.0305,  ..., -0.3229, -0.1662, -0.1849],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
      "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "count tensor([[70],\n",
      "        [31]], device='cuda:0')\n",
      "mea tensor([[ 0.0282,  0.0212, -0.0108,  ..., -0.0074, -0.0559,  0.0028],\n",
      "        [ 0.0394,  0.0759,  0.0682,  ..., -0.1459, -0.0812,  0.0046]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0187],\n",
      "        [0.3426]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0187],\n",
      "        [0.3426]], device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "distr.shape torch.Size([2, 70, 512])\n",
      "distr.shape torch.Size([2, 512])\n",
      "ret.shape torch.Size([2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0139],\n",
       "        [0.3691]], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_emb = model.embedding(x_)#\n",
    "print('x_emb.shape',x_emb.shape)\n",
    "distr = model.encoder(inputs_embeds=x_emb).last_hidden_state#(bs,sentence length,512)\n",
    "print('distr.shape',distr.shape)\n",
    "x_attn= x_attn.unsqueeze(-1)\n",
    "print('x_attn.shape',x_attn.shape)\n",
    "distr = torch.mul(distr,x_attn)#previously\n",
    "print('mutil',torch.sum(distr,1).shape)\n",
    "print('count',torch.sum(x_attn,1).shape)\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn,1)#(bs,512)\n",
    "print('mea',distr.shape)\n",
    "\n",
    "ret =  model.classifier(distr)#(bs,1)\n",
    "print(ret)\n",
    "ret = model.relu(ret)#(bs,1)\n",
    "print(ret)\n",
    "\n",
    "\n",
    "\n",
    "a_pred_dis = model(x_,x_attn_)\n",
    "a_pred_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We previously use torch.mean to (x after embedding)*x_attn but the mean will be small for the long sentences, now we use torch.sum(distr,1)/torch.sum(x_attn,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the D judge the sentence by it's length\n",
    "## small output for the first part, but large for the latter part, so the longer the higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## so we change the output to [batch,0,512] ie, onlythe first word's sumQ*V and than droppout and then classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
