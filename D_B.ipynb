{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "def d(l):\n",
    "    return tokenizer.batch_decode(l,skip_special_tokens=True)\n",
    "def en(l):\n",
    "    return tokenize(l,tokenizer,512,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr President, ladies and gentlemen, I would like to thank Mrs Schroedter for the sound report.',\n",
       " 'It is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it is true that it has taken a number of amendments that have been drafted.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = [[ 1363,  1661,     6, 14595,     6,    27,   133,   114,    12,  2763,\n",
    "          8667,   180, 10363,    15,    26,   449,    21,    46,  1287,   934,\n",
    "             5,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0],\n",
    "        [  451,    65,  2767,   139,     8,   962,    16,   128,  4963,    11,\n",
    "            16,     8,  4492,  5054,   255,   808,   905,    13,   186,    13,\n",
    "             8, 12123,     7,    24,    43,   118,   953,    26,  1918,    48,\n",
    "           934,     5,     1,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0]]\n",
    "pred = [[ 1363,  1661,     6, 10989,    11,  7569,   904,     6,    27,   133,\n",
    "           114,    12,  2763,  8667,   180, 10363,    15,    26,   449,    21,\n",
    "             8,  1345,   934,     5,     1,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0],\n",
    "        [   94,    19,  1176,    24,    34,    19,  1176,    24,    34,    19,\n",
    "          1176,    24,    34,    19,  1176,    24,    34,    19,  1176,    24,\n",
    "            34,    19,  1176,    24,    34,    19,  1176,    24,    34,    19,\n",
    "          1176,    24,    34,    19,  1176,    24,    34,    19,  1176,    24,\n",
    "            34,    19,  1176,    24,    34,    19,  1176,    24,    34,    19,\n",
    "          1176,    24,    34,    19,  1176,    24,    34,    19,  1176,    24,\n",
    "            34,    19,  1176,    24,    34,    19,  1176,    24,    34,    19,\n",
    "          1176,    24,    34,    19,  1176,    24,    34,    19,  1176,    24,\n",
    "            34,    19,  1176,    24,    34,    19,  1176,    24,    34,    19,\n",
    "          1176,    24,    34,    19,  1176,    24,    34,    19,  1176,    24,\n",
    "            34,    19,  1176,    24,    34,    19,  1176,    24,    34,    19,\n",
    "          1176,    24,    34,    65,  1026,     3,     9,   381,    13, 12123,\n",
    "             7,    24,    43,   118,     3, 23505,     5,     1]]\n",
    "pred = torch.Tensor(pred).long().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./model/D_B.pt').train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label = en(['I have some sympathy with the last speaker: sometimes the Committee on Budgetary Control gets so involved with the important work they are doing, that they are unable to see the wood for the trees.', 'I also welcome the presentation on behalf of the Legal Affairs Committee and the Economic and Monetary Affairs Committee which took a balanced approach to the major and indeed legitimate concerns of the Committee on Budgetary Control about fraud.'])[0]\n",
    "# [           'I agree with the previous speaker: sometimes the Committee on Budgetary Control is so deep in its important work that it does not see the forest behind trees.', 'I also welcome the statements made on behalf of the Committee on Legal Affairs and the Internal Market and the Committee on Economic and Monetary Affairs, which took a fairly balanced position on the serious and quite justified concerns of the Committee on Budgetary Control with regard to fraud.']\n",
    "\n",
    "pred = en([ 'I agree with the previous speaker: sometimes the Committee on Budgetary Control is so deep in its important work that it does not see the forest behind trees.', 'I also welcome the statements made on behalf of the Committee on Legal Affairs and the Internal Market and the Committee on Economic and Monetary Affairs, which have taken a fairly balanced position on the serious and quite justified concerns of the Committee on Budgetary Control with regard to fraud.'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   27,    43,   128, 27907,    28,     8,   336,  5873,    10,  1664,\n",
      "             8,  3201,    30, 12532,  1208,  4330,  2347,    78,  1381,    28,\n",
      "             8,   359,   161,    79,    33,   692,     6,    24,    79,    33,\n",
      "             3,  6319,    12,   217,     8,  1679,    21,     8,  3124,     5,\n",
      "             1,     0,     0,     0,     0],\n",
      "        [   27,    92,  2222,     8,  3831,    30,  6089,    13,     8, 11281,\n",
      "         12078,  3201,    11,     8,  9071,    11,  1290,  1582,  1208, 12078,\n",
      "          3201,    84,   808,     3,     9,  8965,  1295,    12,     8,   779,\n",
      "            11,  5071, 12372,  3315,    13,     8,  3201,    30, 12532,  1208,\n",
      "          4330,    81,  7712,     5,     1]], device='cuda:0')\n",
      "torch.Size([2, 45])\n",
      "\t\t---------embedding\n",
      "torch.Size([2, 45, 512])\n",
      "tensor([[[ 15.8255,   3.0661,  14.9297,  ..., -11.0776,  -4.4553,   5.7272],\n",
      "         [ 16.4459, -13.6218, -23.6599,  ...,   8.4843,   0.2263,  -0.3465],\n",
      "         [  1.5718,   1.1976,  16.3511,  ..., -13.4438,   8.6591,  36.0261],\n",
      "         ...,\n",
      "         [ -1.9713,   0.1810,  -7.0931,  ...,  -0.4844,   2.7078,  -2.8701],\n",
      "         [ -1.9713,   0.1810,  -7.0931,  ...,  -0.4844,   2.7078,  -2.8701],\n",
      "         [ -1.9713,   0.1810,  -7.0931,  ...,  -0.4844,   2.7078,  -2.8701]],\n",
      "\n",
      "        [[ 15.8255,   3.0661,  14.9297,  ..., -11.0776,  -4.4553,   5.7272],\n",
      "         [ 12.8770,   4.6584,  -1.4246,  ...,  -0.2715, -33.7260,  34.3136],\n",
      "         [ 24.1588,  -6.9753, -34.5270,  ..., -18.4298,  21.7420,   5.4933],\n",
      "         ...,\n",
      "         [-10.7290,  -9.2744, -27.7144,  ...,   5.9364,  -4.2273,   0.3175],\n",
      "         [ -4.3332,   7.3672, -14.5854,  ...,  13.0432,  -2.7207,  -3.1233],\n",
      "         [ 12.6463,   8.1982, -11.5313,  ...,   7.8180,  -7.2934,   0.9610]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([2, 45, 512])\n",
      "tensor([[[ 0.1151,  0.1534,  0.2321,  ..., -0.3200, -0.1177,  0.0586],\n",
      "         [ 0.1168,  0.4341, -0.0000,  ..., -0.0402, -0.0000,  0.0170],\n",
      "         [-0.0360,  0.2132, -0.2077,  ..., -0.0000,  0.0707, -0.0000],\n",
      "         ...,\n",
      "         [ 0.2925,  0.0133,  0.0000,  ..., -0.1041,  0.1363, -0.1460],\n",
      "         [ 0.2429,  0.1751,  0.0557,  ..., -0.2218,  0.0702, -0.0604],\n",
      "         [ 0.0398,  0.0399,  0.0000,  ..., -0.2699,  0.0509, -0.2482]],\n",
      "\n",
      "        [[-0.0875,  0.0625, -0.0813,  ..., -0.2280, -0.0254,  0.1259],\n",
      "         [ 0.0010,  0.2100,  0.1168,  ..., -0.1603, -0.1148, -0.0878],\n",
      "         [-0.0280,  0.0806, -0.1422,  ..., -0.1764, -0.0454,  0.1963],\n",
      "         ...,\n",
      "         [-0.0000, -0.0989, -0.1565,  ...,  0.1026, -0.1750,  0.0779],\n",
      "         [ 0.1716,  0.1835, -0.0587,  ..., -0.0403,  0.1310,  0.0000],\n",
      "         [ 0.1100, -0.0193, -0.0000,  ..., -0.0000, -0.0024,  0.0300]]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([2, 45, 512])\n",
      "tensor([[[ 0.1151,  0.1534,  0.2321,  ..., -0.3200, -0.1177,  0.0586],\n",
      "         [ 0.1168,  0.4341, -0.0000,  ..., -0.0402, -0.0000,  0.0170],\n",
      "         [-0.0360,  0.2132, -0.2077,  ..., -0.0000,  0.0707, -0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "        [[-0.0875,  0.0625, -0.0813,  ..., -0.2280, -0.0254,  0.1259],\n",
      "         [ 0.0010,  0.2100,  0.1168,  ..., -0.1603, -0.1148, -0.0878],\n",
      "         [-0.0280,  0.0806, -0.1422,  ..., -0.1764, -0.0454,  0.1963],\n",
      "         ...,\n",
      "         [-0.0000, -0.0989, -0.1565,  ...,  0.1026, -0.1750,  0.0779],\n",
      "         [ 0.1716,  0.1835, -0.0587,  ..., -0.0403,  0.1310,  0.0000],\n",
      "         [ 0.1100, -0.0193, -0.0000,  ..., -0.0000, -0.0024,  0.0300]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([2, 512])\n",
      "tensor([[-0.0314, -0.0070, -0.0073,  ..., -0.0635, -0.0185, -0.0199],\n",
      "        [-0.0480,  0.0266, -0.0706,  ..., -0.0945,  0.0293, -0.0956]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[0.1755],\n",
      "        [0.1934]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1814],\n",
      "        [0.1844]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = label.cuda()\n",
    "#delte 0 give huge diff\n",
    "\n",
    "\n",
    "# x = torch.Tensor(x).long().cuda()\n",
    "print(x)\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn_unsq,1)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret_label =  model.classifier(distr)#(bs,1)\n",
    "print(ret_label)\n",
    "\n",
    "print(model(x,x_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 57])\n",
      "\t\t---------embedding\n",
      "torch.Size([2, 57, 512])\n",
      "tensor([[[ 15.8255,   3.0661,  14.9297,  ..., -11.0776,  -4.4553,   5.7272],\n",
      "         [  2.9129, -16.4970, -32.1548,  ..., -16.8586, -30.2383,  18.4215],\n",
      "         [ -7.3098,   4.8835,   7.0894,  ...,  19.4460,  -4.3243,  17.3978],\n",
      "         ...,\n",
      "         [ -1.9713,   0.1810,  -7.0931,  ...,  -0.4844,   2.7078,  -2.8701],\n",
      "         [ -1.9713,   0.1810,  -7.0931,  ...,  -0.4844,   2.7078,  -2.8701],\n",
      "         [ -1.9713,   0.1810,  -7.0931,  ...,  -0.4844,   2.7078,  -2.8701]],\n",
      "\n",
      "        [[ 15.8255,   3.0661,  14.9297,  ..., -11.0776,  -4.4553,   5.7272],\n",
      "         [ 12.8770,   4.6584,  -1.4246,  ...,  -0.2715, -33.7260,  34.3136],\n",
      "         [ 24.1588,  -6.9753, -34.5270,  ..., -18.4298,  21.7420,   5.4933],\n",
      "         ...,\n",
      "         [-10.7290,  -9.2744, -27.7144,  ...,   5.9364,  -4.2273,   0.3175],\n",
      "         [ -4.3332,   7.3672, -14.5854,  ...,  13.0432,  -2.7207,  -3.1233],\n",
      "         [ 12.6463,   8.1982, -11.5313,  ...,   7.8180,  -7.2934,   0.9610]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([2, 57, 512])\n",
      "tensor([[[ 0.1794, -0.0042,  0.0572,  ..., -0.2585, -0.2541,  0.0558],\n",
      "         [ 0.0000,  0.2656,  0.0327,  ..., -0.1170, -0.3295,  0.1255],\n",
      "         [ 0.0188,  0.0992,  0.0428,  ..., -0.0000, -0.2210, -0.0048],\n",
      "         ...,\n",
      "         [ 0.1400,  0.0202,  0.1889,  ..., -0.0000, -0.2448, -0.1716],\n",
      "         [ 0.0592, -0.1011,  0.2305,  ..., -0.1288, -0.0860, -0.3581],\n",
      "         [ 0.2769, -0.2357,  0.1833,  ..., -0.0757,  0.0593, -0.0283]],\n",
      "\n",
      "        [[-0.0717,  0.1534,  0.1291,  ..., -0.0000, -0.2121,  0.0000],\n",
      "         [ 0.0000,  0.0320,  0.0154,  ..., -0.2049, -0.3860,  0.0993],\n",
      "         [-0.1095,  0.1929, -0.2151,  ..., -0.2361,  0.0478,  0.2294],\n",
      "         ...,\n",
      "         [ 0.0000, -0.0000, -0.1649,  ...,  0.0681, -0.2053,  0.0973],\n",
      "         [ 0.1385,  0.2298, -0.0784,  ...,  0.0079,  0.0306,  0.0602],\n",
      "         [ 0.0730,  0.0697, -0.0000,  ..., -0.0907,  0.0159,  0.0217]]],\n",
      "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([2, 57, 512])\n",
      "tensor([[[ 0.1794, -0.0042,  0.0572,  ..., -0.2585, -0.2541,  0.0558],\n",
      "         [ 0.0000,  0.2656,  0.0327,  ..., -0.1170, -0.3295,  0.1255],\n",
      "         [ 0.0188,  0.0992,  0.0428,  ..., -0.0000, -0.2210, -0.0048],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "        [[-0.0717,  0.1534,  0.1291,  ..., -0.0000, -0.2121,  0.0000],\n",
      "         [ 0.0000,  0.0320,  0.0154,  ..., -0.2049, -0.3860,  0.0993],\n",
      "         [-0.1095,  0.1929, -0.2151,  ..., -0.2361,  0.0478,  0.2294],\n",
      "         ...,\n",
      "         [ 0.0000, -0.0000, -0.1649,  ...,  0.0681, -0.2053,  0.0973],\n",
      "         [ 0.1385,  0.2298, -0.0784,  ...,  0.0079,  0.0306,  0.0602],\n",
      "         [ 0.0730,  0.0697, -0.0000,  ..., -0.0907,  0.0159,  0.0217]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([2, 512])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 512])\n",
      "tensor([[-0.0379, -0.0320,  0.0239,  ..., -0.0767, -0.0216, -0.0350],\n",
      "        [-0.0619,  0.0167, -0.0768,  ..., -0.0986,  0.0114, -0.0883]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[0.1659],\n",
      "        [0.1894]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1561],\n",
      "        [0.1792]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = pred.cuda()\n",
    "       \n",
    "# x = torch.Tensor(x).long().cuda()\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "print(torch.sum(distr,1).shape)\n",
    "print(torch.sum(x_attn,1).shape)\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn_unsq,1)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret_pred =  model.classifier(distr)#(bs,1)\n",
    "print(ret_pred)\n",
    "\n",
    "\n",
    "print(model(x,x_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1755],\n",
      "        [0.1934]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor(0.6651, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0317, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3484, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterionGAN = torch.nn.MSELoss()\n",
    "pred_real = ret_label\n",
    "print(pred_real)\n",
    "loss_D_real = criterionGAN(pred_real, torch.ones((pred_real.shape[0],1),device='cuda'))\n",
    "print(loss_D_real)\n",
    "# Fake\n",
    "pred_fake = ret_pred\n",
    "loss_D_fake = criterionGAN(pred_fake, torch.zeros((pred_fake.shape[0],1),device='cuda'))\n",
    "# Combined loss and calculate gradients\n",
    "print(loss_D_fake)\n",
    "\n",
    "\n",
    "loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "print(loss_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ",attention_mask=x_attn is essential for the encoder inpuit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We previously use torch.mean to (x after embedding)*x_attn but the mean will be small for the long sentences, now we use torch.sum(distr,1)/torch.sum(x_attn,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the D judge the sentence by it's length\n",
    "## small output for the first part, but large for the latter part, so the longer the higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## so we change the output to [batch,0,512] ie, onlythe first word's sumQ*V and than droppout and then classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAKE dont end with 1, cuz i use model.generate but not self.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
