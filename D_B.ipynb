{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "def d(l):\n",
    "    return tokenizer.batch_decode(l,skip_special_tokens=True)\n",
    "def en(l):\n",
    "    return tokenize(l,tokenizer,512,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./model/D_B.pt').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label = en(['Mr President, Commissioner, I would like to thank Mrs Schroedter for an excellent report.', 'She has gone into the issue in some depth and in the committee debate she took account of many of the amendments that have been tabled regarding this report.'])\n",
    "pred = en(['Mr President, ladies and gentlemen, I thank Mrs Schroedter for the sound report.', \"You have dealt with the issue in detail and, during the committee's discussion, you have taken account of many of the amendments that have been tabled to this report.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1363,  1661,     6, 14595,     6,    27,   133,   114,    12,  2763,\n",
      "          8667,   180, 10363,    15,    26,   449,    21,    46,  1287,   934,\n",
      "             5,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  451,    65,  2767,   139,     8,   962,    16,   128,  4963,    11,\n",
      "            16,     8,  4492,  5054,   255,   808,   905,    13,   186,    13,\n",
      "             8, 12123,     7,    24,    43,   118,   953,    26,  1918,    48,\n",
      "           934,     5,     1]], device='cuda:0')\n",
      "torch.Size([2, 33])\n",
      "\t\t---------embedding\n",
      "torch.Size([2, 33, 512])\n",
      "tensor([[[-15.8958,   0.3818,   3.2577,  ...,  -4.6827,  27.4352,  17.6700],\n",
      "         [ -1.9777,  -6.5448,  28.4217,  ..., -56.8084, -72.0019,   8.4903],\n",
      "         [ 12.6347,   7.9227, -11.0104,  ...,   7.9381,  -6.7786,  -3.4063],\n",
      "         ...,\n",
      "         [ -1.9719,   0.1815,  -7.0937,  ...,  -0.4849,   2.7084,  -2.8707],\n",
      "         [ -1.9719,   0.1815,  -7.0937,  ...,  -0.4849,   2.7084,  -2.8707],\n",
      "         [ -1.9719,   0.1815,  -7.0937,  ...,  -0.4849,   2.7084,  -2.8707]],\n",
      "\n",
      "        [[ -0.1411,  -8.4090,  34.7044,  ..., -26.5628,  10.9295,  26.7384],\n",
      "         [ 17.8510,  -3.4977, -18.3741,  ...,  11.3987,  -3.2593,  14.4906],\n",
      "         [  6.7126, -23.4255, -36.0951,  ...,  -6.1953,  42.7555,   7.8605],\n",
      "         ...,\n",
      "         [  8.8538,   7.8113,   5.6407,  ...,  16.5174, -43.0303,   1.5869],\n",
      "         [ -4.3337,   7.3677, -14.5860,  ...,  13.0438,  -2.7213,  -3.1239],\n",
      "         [ 12.6469,   8.1988, -11.5319,  ...,   7.8185,  -7.2939,   0.9616]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([2, 33, 512])\n",
      "tensor([[[-0.2333, -0.1078, -0.1121,  ..., -0.2905,  0.2500, -0.0700],\n",
      "         [-0.1601, -0.3573, -0.1677,  ..., -0.3452, -0.2443, -0.1279],\n",
      "         [ 0.1512,  0.1913, -0.2148,  ..., -0.0785,  0.0356, -0.1185],\n",
      "         ...,\n",
      "         [ 0.1418, -0.0419,  0.2261,  ...,  0.0007, -0.0311, -0.2571],\n",
      "         [ 0.1504, -0.0562,  0.2200,  ...,  0.0042, -0.0362, -0.2513],\n",
      "         [ 0.1438, -0.0517,  0.2163,  ...,  0.0043, -0.0352, -0.2527]],\n",
      "\n",
      "        [[ 0.0707, -0.1685,  0.4943,  ..., -0.4824, -0.1280,  0.0761],\n",
      "         [ 0.0696, -0.1959, -0.2147,  ..., -0.2336, -0.1024, -0.1043],\n",
      "         [-0.0543, -0.1884, -0.2647,  ..., -0.2735,  0.1037, -0.0976],\n",
      "         ...,\n",
      "         [-0.1748, -0.1058,  0.1258,  ...,  0.1424, -0.1052, -0.2056],\n",
      "         [-0.0238,  0.0725, -0.0777,  ...,  0.0470, -0.0198, -0.0400],\n",
      "         [ 0.1072,  0.0527, -0.0741,  ..., -0.0804,  0.0449, -0.0085]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([2, 33, 512])\n",
      "tensor([[[-0.2333, -0.1078, -0.1121,  ..., -0.2905,  0.2500, -0.0700],\n",
      "         [-0.1601, -0.3573, -0.1677,  ..., -0.3452, -0.2443, -0.1279],\n",
      "         [ 0.1512,  0.1913, -0.2148,  ..., -0.0785,  0.0356, -0.1185],\n",
      "         ...,\n",
      "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "        [[ 0.0707, -0.1685,  0.4943,  ..., -0.4824, -0.1280,  0.0761],\n",
      "         [ 0.0696, -0.1959, -0.2147,  ..., -0.2336, -0.1024, -0.1043],\n",
      "         [-0.0543, -0.1884, -0.2647,  ..., -0.2735,  0.1037, -0.0976],\n",
      "         ...,\n",
      "         [-0.1748, -0.1058,  0.1258,  ...,  0.1424, -0.1052, -0.2056],\n",
      "         [-0.0238,  0.0725, -0.0777,  ...,  0.0470, -0.0198, -0.0400],\n",
      "         [ 0.1072,  0.0527, -0.0741,  ..., -0.0804,  0.0449, -0.0085]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([2, 512])\n",
      "tensor([[-0.0833, -0.0565, -0.0866,  ..., -0.1126,  0.0179, -0.0147],\n",
      "        [-0.0237, -0.0548,  0.0119,  ..., -0.1384, -0.0119,  0.0487]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[0.1157],\n",
      "        [0.1156]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1157],\n",
      "        [0.1156]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = label[0].cuda()\n",
    "#delte 0 give huge diff\n",
    "       \n",
    "# x = torch.Tensor(x).long().cuda()\n",
    "print(x)\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn_unsq,1)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret_label =  model.classifier(distr)#(bs,1)\n",
    "print(ret_label)\n",
    "\n",
    "print(model(x,x_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 37])\n",
      "\t\t---------embedding\n",
      "torch.Size([2, 37, 512])\n",
      "tensor([[[-15.8958,   0.3818,   3.2577,  ...,  -4.6827,  27.4352,  17.6700],\n",
      "         [ -1.9777,  -6.5448,  28.4217,  ..., -56.8084, -72.0019,   8.4903],\n",
      "         [ 12.6347,   7.9227, -11.0104,  ...,   7.9381,  -6.7786,  -3.4063],\n",
      "         ...,\n",
      "         [ -1.9719,   0.1815,  -7.0937,  ...,  -0.4849,   2.7084,  -2.8707],\n",
      "         [ -1.9719,   0.1815,  -7.0937,  ...,  -0.4849,   2.7084,  -2.8707],\n",
      "         [ -1.9719,   0.1815,  -7.0937,  ...,  -0.4849,   2.7084,  -2.8707]],\n",
      "\n",
      "        [[ 16.2764,  -0.3113, -10.2170,  ...,  -3.1309,  -9.5889,  33.4791],\n",
      "         [ 16.4463, -13.6224, -23.6603,  ...,   8.4848,   0.2268,  -0.3471],\n",
      "         [-20.3548, -31.3420, -30.7264,  ...,   8.8942,   4.9634,  36.2160],\n",
      "         ...,\n",
      "         [  8.8538,   7.8113,   5.6407,  ...,  16.5174, -43.0303,   1.5869],\n",
      "         [ -4.3337,   7.3677, -14.5860,  ...,  13.0438,  -2.7213,  -3.1239],\n",
      "         [ 12.6469,   8.1988, -11.5319,  ...,   7.8185,  -7.2939,   0.9616]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([2, 37, 512])\n",
      "tensor([[[-0.2007, -0.0868, -0.1022,  ..., -0.3324,  0.2447, -0.0648],\n",
      "         [-0.0758, -0.3262, -0.1778,  ..., -0.4517, -0.2299, -0.0682],\n",
      "         [ 0.1883,  0.1764, -0.3654,  ..., -0.0584,  0.0387, -0.0034],\n",
      "         ...,\n",
      "         [ 0.1760, -0.0866,  0.2452,  ...,  0.0027, -0.0597, -0.2066],\n",
      "         [ 0.1860, -0.0953,  0.2438,  ...,  0.0053, -0.0634, -0.2039],\n",
      "         [ 0.1884, -0.0992,  0.2450,  ...,  0.0016, -0.0617, -0.2070]],\n",
      "\n",
      "        [[ 0.1014, -0.3394, -0.3609,  ..., -0.2624, -0.0551,  0.0194],\n",
      "         [ 0.0211, -0.2600, -0.4244,  ..., -0.1192,  0.0532, -0.0201],\n",
      "         [-0.1736, -0.3628,  0.0219,  ...,  0.0680,  0.1149,  0.1020],\n",
      "         ...,\n",
      "         [-0.1393, -0.1033,  0.1837,  ...,  0.1345, -0.1503, -0.1288],\n",
      "         [-0.0437,  0.0556, -0.1172,  ...,  0.0253, -0.0113, -0.0225],\n",
      "         [ 0.0957,  0.0278, -0.0718,  ..., -0.0481,  0.0249, -0.0239]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([2, 37, 512])\n",
      "tensor([[[-0.2007, -0.0868, -0.1022,  ..., -0.3324,  0.2447, -0.0648],\n",
      "         [-0.0758, -0.3262, -0.1778,  ..., -0.4517, -0.2299, -0.0682],\n",
      "         [ 0.1883,  0.1764, -0.3654,  ..., -0.0584,  0.0387, -0.0034],\n",
      "         ...,\n",
      "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "        [[ 0.1014, -0.3394, -0.3609,  ..., -0.2624, -0.0551,  0.0194],\n",
      "         [ 0.0211, -0.2600, -0.4244,  ..., -0.1192,  0.0532, -0.0201],\n",
      "         [-0.1736, -0.3628,  0.0219,  ...,  0.0680,  0.1149,  0.1020],\n",
      "         ...,\n",
      "         [-0.1393, -0.1033,  0.1837,  ...,  0.1345, -0.1503, -0.1288],\n",
      "         [-0.0437,  0.0556, -0.1172,  ...,  0.0253, -0.0113, -0.0225],\n",
      "         [ 0.0957,  0.0278, -0.0718,  ..., -0.0481,  0.0249, -0.0239]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([2, 512])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 512])\n",
      "tensor([[-0.0695, -0.0100, -0.1081,  ..., -0.1243,  0.0298, -0.0316],\n",
      "        [-0.0421, -0.0648, -0.0204,  ..., -0.1121, -0.0343,  0.0775]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[0.1179],\n",
      "        [0.1202]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = pred[0].cuda()\n",
    "       \n",
    "# x = torch.Tensor(x).long().cuda()\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "print(torch.sum(distr,1).shape)\n",
    "print(torch.sum(x_attn,1).shape)\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn_unsq,1)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret_pred =  model.classifier(distr)#(bs,1)\n",
    "print(ret_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7821, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0142, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3981, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterionGAN = torch.nn.MSELoss()\n",
    "pred_real = ret_label\n",
    "loss_D_real = criterionGAN(pred_real, torch.ones((pred_real.shape[0],1),device='cuda'))\n",
    "print(loss_D_real)\n",
    "# Fake\n",
    "pred_fake = ret_pred\n",
    "loss_D_fake = criterionGAN(pred_fake, torch.zeros((pred_fake.shape[0],1),device='cuda'))\n",
    "# Combined loss and calculate gradients\n",
    "print(loss_D_fake)\n",
    "\n",
    "\n",
    "loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "print(loss_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ",attention_mask=x_attn is essential for the encoder inpuit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We previously use torch.mean to (x after embedding)*x_attn but the mean will be small for the long sentences, now we use torch.sum(distr,1)/torch.sum(x_attn,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the D judge the sentence by it's length\n",
    "## small output for the first part, but large for the latter part, so the longer the higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## so we change the output to [batch,0,512] ie, onlythe first word's sumQ*V and than droppout and then classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAKE dont end with 1, cuz i use model.generate but not self.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
