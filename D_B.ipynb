{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "def d(l):\n",
    "    return tokenizer.batch_decode(l,skip_special_tokens=True)\n",
    "def en(l):\n",
    "    return tokenize(l,tokenizer,512,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_B = torch.zeros((3,4,5))\n",
    "fake_B[0][0][1] = 1\n",
    "fake_B[0][1][0] = 1\n",
    "fake_B[0][2][3] = 1\n",
    "fake_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a  = (fake_B[:, :,0] != 1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./model/D_B.pt').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 144])\n",
      "\t\t---------embedding\n",
      "torch.Size([1, 144, 512])\n",
      "tensor([[[  6.0501,  -7.9552,  14.4621,  ..., -21.1924,   4.0715,  12.5375],\n",
      "         [ 23.8152,  -2.8088, -15.9855,  ...,  25.2238,  -6.3611,   2.1363],\n",
      "         [  5.3305, -38.3739,  46.3184,  ...,  -6.1115, -31.1421,  -2.6996],\n",
      "         ...,\n",
      "         [ -1.9571,   0.1690,  -7.0790,  ...,  -0.4705,   2.6937,  -2.8559],\n",
      "         [ -1.9571,   0.1690,  -7.0790,  ...,  -0.4705,   2.6937,  -2.8559],\n",
      "         [ -1.9571,   0.1690,  -7.0790,  ...,  -0.4705,   2.6937,  -2.8559]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([1, 144, 512])\n",
      "tensor([[[ 0.4458, -0.1212,  0.0547,  ..., -0.3194,  0.0034, -0.1237],\n",
      "         [ 0.5885, -0.2483,  0.2204,  ..., -0.1367,  0.0660, -0.0868],\n",
      "         [ 0.0803,  0.0777,  0.3199,  ...,  0.1108, -0.1654,  0.0257],\n",
      "         ...,\n",
      "         [ 0.2039,  0.0961,  0.2980,  ..., -0.0319, -0.1689, -0.1566],\n",
      "         [ 0.2093,  0.0700,  0.3007,  ..., -0.0212, -0.1572, -0.1362],\n",
      "         [ 0.2110,  0.0712,  0.3037,  ..., -0.0201, -0.1573, -0.1323]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([1, 144, 512])\n",
      "tensor([[[ 0.4458, -0.1212,  0.0547,  ..., -0.3194,  0.0034, -0.1237],\n",
      "         [ 0.5885, -0.2483,  0.2204,  ..., -0.1367,  0.0660, -0.0868],\n",
      "         [ 0.0803,  0.0777,  0.3199,  ...,  0.1108, -0.1654,  0.0257],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([1, 512])\n",
      "tensor([[ 4.4581e-01, -1.2124e-01,  5.4711e-02,  1.2781e-02,  2.3276e-02,\n",
      "          1.5415e-01, -3.8547e-02, -4.0413e-01, -1.1273e-01, -4.0495e-01,\n",
      "         -2.1897e-01, -1.8155e-01,  6.4916e-02,  1.9078e-01, -7.2995e-02,\n",
      "         -3.8352e-01, -3.9011e-01,  2.4646e-01,  2.5230e-01,  2.6310e-01,\n",
      "         -1.4036e-01,  2.0502e-01, -3.0179e-01, -1.8098e-01,  3.3333e-01,\n",
      "         -7.1275e-02, -2.1816e-01,  1.9163e-01, -9.3076e-02,  2.1963e-02,\n",
      "         -2.3192e-01,  1.0307e-01,  6.5124e-02, -7.8582e-02,  1.1677e-01,\n",
      "          5.0866e-03,  1.5561e-01,  5.7995e-03, -8.9204e-02,  8.8261e-02,\n",
      "         -2.9739e-01, -8.8622e-03,  1.9224e-01, -3.6794e-02,  9.4794e-02,\n",
      "         -1.3017e-01,  2.1547e-01,  2.9984e-01,  5.4829e-02,  1.6208e-01,\n",
      "         -2.8510e-01,  6.7178e-02, -2.8914e-01, -1.0551e-01,  1.0656e-02,\n",
      "          2.0171e-01, -9.4181e-02,  3.2842e-01, -1.9378e-01, -1.1362e-01,\n",
      "         -2.3407e-01,  8.0334e-02, -6.5574e-02,  1.7772e-01, -2.6566e-02,\n",
      "          9.9968e-02, -5.9266e-01,  2.0238e-01, -6.5754e-02,  4.1938e-01,\n",
      "          3.3433e-01, -2.6199e-01, -1.6744e-01, -1.4146e-01,  4.3207e-01,\n",
      "         -1.9036e-01,  5.8686e-02, -2.4668e-01,  1.3648e-01, -1.4334e-02,\n",
      "         -1.1988e-01, -1.4855e-02,  1.3612e-01, -1.6849e-01,  4.1847e-02,\n",
      "          5.5928e-02, -3.8739e-01,  4.1238e-02,  9.3388e-02, -9.5083e-02,\n",
      "         -8.8182e-02,  5.7502e-02, -9.2895e-02,  1.5051e-01,  3.3349e-02,\n",
      "         -1.4755e-01,  1.5501e-01,  4.5642e-03, -9.5309e-03, -3.3775e-01,\n",
      "          9.8648e-02,  9.6119e-02, -1.0691e-01, -3.8557e-02, -7.4999e-02,\n",
      "          2.1723e-01,  2.5756e-01,  1.3823e-01,  1.0638e-01, -1.7322e-02,\n",
      "         -1.1269e-01, -1.7409e-01,  1.8332e-01,  3.3824e-01,  2.4493e-01,\n",
      "         -1.2067e-01, -3.2960e-01,  1.3213e-01,  5.6046e-02, -2.1673e-01,\n",
      "         -1.7542e-01,  2.5428e-02, -2.8502e-01, -2.6692e-01, -1.3244e-01,\n",
      "         -5.6108e-02,  4.3571e-01, -4.2614e-01,  1.5096e-01,  2.2240e-01,\n",
      "         -6.4736e-02,  1.4273e-01,  3.1014e-01,  1.2551e-01, -1.1363e-01,\n",
      "          4.6819e-01,  1.2522e-01,  1.6109e-01,  8.7311e-02,  2.3766e-01,\n",
      "          2.6316e-01, -7.0986e-02, -1.0367e-01, -5.4809e-02,  2.3668e-01,\n",
      "         -1.5104e-02, -1.2242e-01,  1.0245e-01,  2.6545e-01,  5.0599e-03,\n",
      "         -1.8202e-01,  2.5106e-01,  8.6069e-02,  2.7882e-02,  2.7872e-01,\n",
      "          8.0900e-02, -6.0275e-02,  1.1756e-01, -1.0994e-02,  1.0114e-01,\n",
      "         -1.1062e-02,  7.4387e-02, -2.3155e-01, -1.1897e-01,  1.7195e-01,\n",
      "         -3.9243e-02, -1.7939e-01, -2.9386e-02, -2.2599e-01, -2.7684e-01,\n",
      "          1.7130e-01,  1.9218e-01,  1.2264e-01,  2.6162e-01,  2.0782e-01,\n",
      "          9.1867e-02, -1.7266e-01, -1.1976e-01,  3.8105e-02, -1.3387e-01,\n",
      "         -1.8627e-01,  1.5990e-01, -1.3473e-01,  1.5245e-01,  1.9557e-01,\n",
      "         -1.1274e-01, -7.6066e-03,  1.2275e-01,  1.4504e-01, -1.0672e-01,\n",
      "         -1.3691e-01,  6.4572e-02, -5.9928e-02,  1.4741e-02,  1.8277e-01,\n",
      "          1.7231e-01,  1.6540e-01, -1.1713e-01, -8.2623e-02,  4.2366e-02,\n",
      "          2.1567e-02, -1.6748e-01,  1.0625e-01, -1.7095e-03, -1.4606e-01,\n",
      "          2.4395e-01, -2.5029e-01, -2.2248e-02,  5.3793e-03, -1.0469e-01,\n",
      "          1.7566e-01, -2.2446e-01, -1.3708e-02, -3.2255e-02, -2.8939e-02,\n",
      "          2.8922e-02, -2.0492e-01,  2.1529e-01,  5.1163e-02, -1.3572e-01,\n",
      "          9.9001e-02, -8.9539e-02,  4.3392e-02,  1.0079e-01, -1.0843e-01,\n",
      "         -2.7948e-01,  2.2365e-01,  4.6232e-01, -2.9899e-02,  3.3418e-01,\n",
      "         -1.1857e-01,  2.0859e-01, -9.2809e-02, -2.1330e-01, -3.7847e-01,\n",
      "          1.2237e-01, -2.1863e-01,  6.8446e-02,  3.1764e-01, -2.5058e-01,\n",
      "          9.0203e-02,  1.5056e-01, -1.9285e-01, -3.0307e-01,  2.3524e-01,\n",
      "         -1.5225e-01, -4.0884e-02,  2.3624e-01,  1.1545e-01,  2.2259e-01,\n",
      "          9.1421e-02, -8.3814e-02,  2.4747e-01,  1.5801e-01, -1.6008e-01,\n",
      "         -4.2373e-03,  3.1727e-01,  1.1333e-01,  6.4502e-02, -1.1509e-01,\n",
      "         -5.0094e-03, -7.2423e-02,  3.9341e-02, -4.9150e-02,  1.6142e-02,\n",
      "          8.2879e-02, -2.5344e-02, -8.0207e-02, -3.5286e-01,  2.6242e-01,\n",
      "          2.6542e-02, -2.4734e-01,  5.7548e-02, -5.7200e-02, -2.0033e-01,\n",
      "         -1.4161e-02,  1.1839e-01,  7.2840e-02,  3.2291e-01,  1.0387e-01,\n",
      "          2.2055e-02,  1.3841e-02, -4.3517e-01,  3.0749e-02,  2.1649e-02,\n",
      "          1.4774e-01,  4.1815e-02, -3.4791e-01, -4.1118e-01, -2.6405e-02,\n",
      "         -4.1440e-01, -3.2447e-01, -1.4215e-01,  3.7285e-01, -5.2425e-02,\n",
      "          1.2625e-01,  2.2351e-01, -7.9139e-02,  4.9640e-03, -8.7250e-03,\n",
      "          5.3719e-02, -1.4913e-01,  3.4920e-02,  8.8774e-02, -3.5661e-01,\n",
      "         -2.2630e-01, -8.1645e-03,  1.8204e-01, -3.6761e-02, -4.3078e-02,\n",
      "         -4.8144e-02,  1.7772e-01, -6.6857e-02,  1.7102e-01, -4.3094e-01,\n",
      "          2.0262e-02,  5.3962e-03,  1.9703e-01,  2.8737e-01,  7.0250e-02,\n",
      "          8.0534e-02, -3.6732e-01,  1.1789e-01, -2.9278e-01, -7.2889e-02,\n",
      "         -9.9517e-02,  1.7474e-01, -3.0858e-01,  2.0918e-01,  1.3802e-01,\n",
      "         -1.7964e-02, -3.6668e-01, -1.0719e-01,  1.8895e-01,  2.0001e-01,\n",
      "         -1.7801e-01,  7.0287e-02, -7.4955e-02,  1.3342e-01,  2.1150e-01,\n",
      "          1.2093e-01, -1.3393e-01,  7.1327e-02, -1.4131e-01,  2.3978e-01,\n",
      "         -3.0805e-01, -3.8303e-02,  7.5401e-02, -1.1522e-01, -5.1125e-02,\n",
      "         -1.3112e-01,  3.0999e-02, -2.4410e-02, -5.9040e-02, -3.2665e-01,\n",
      "          3.1068e-01, -1.2930e-01, -3.3516e-01, -1.3549e-01,  2.9113e-02,\n",
      "         -1.0780e-01,  2.8939e-01,  3.2171e-01,  3.1206e-01,  1.4955e-01,\n",
      "          7.8090e-02,  1.7229e-02, -1.8997e-01, -3.8604e-02, -1.0197e-01,\n",
      "          2.7373e-01, -1.0223e-01,  1.2860e-01, -1.1432e-01,  1.3154e-01,\n",
      "         -6.9652e-02,  2.3706e-02,  4.2561e-01, -4.0106e-01,  7.0142e-02,\n",
      "         -1.8181e-01,  3.5591e-01, -2.6780e-02, -7.1174e-02,  1.1600e-01,\n",
      "          1.0621e-02, -1.7880e-01,  2.3189e-01, -2.2747e-01,  2.7470e-01,\n",
      "          1.3286e-01,  4.7239e-02, -4.0483e-01,  1.4649e-01, -2.0563e-01,\n",
      "         -3.2620e-01, -3.2311e-01, -1.0267e-01,  2.6684e-02,  2.8486e-02,\n",
      "         -1.0655e-02, -2.1127e-01,  1.7627e-01, -5.2146e-02,  2.3460e-01,\n",
      "          1.0289e-01,  1.0023e-01,  2.0034e-01,  4.3203e-02, -2.7150e-01,\n",
      "         -3.5041e-02,  4.8374e-02,  6.7278e-02,  2.1020e-01, -6.9408e-01,\n",
      "         -1.9859e-04,  8.8973e-02, -7.2892e-03, -1.2563e-01, -2.9735e-01,\n",
      "          5.3523e-03, -1.3225e-01,  7.9565e-02, -2.5941e-01,  1.2998e-01,\n",
      "         -1.2010e-01,  1.7478e-01, -1.0792e-01,  2.9376e-01,  8.3680e-02,\n",
      "         -4.4750e-01,  2.8234e-01,  4.8916e-02,  2.0850e-02, -1.3634e-01,\n",
      "         -1.0805e-01,  3.1527e-02, -1.2715e-01, -2.6972e-01,  1.0108e-01,\n",
      "          1.5670e-01,  4.7140e-03, -9.3621e-02,  2.6062e-01,  2.9749e-01,\n",
      "         -1.0014e-01, -2.1886e-02, -1.4133e-01, -2.3357e-02, -3.1607e-01,\n",
      "         -1.9044e-01,  2.3076e-01,  5.7443e-02, -1.1509e-01, -2.8838e-02,\n",
      "         -5.5821e-01,  1.0919e-01, -4.9445e-02,  2.9690e-01,  5.2433e-02,\n",
      "         -1.0855e-01, -2.1468e-01,  2.2771e-01,  1.2664e-01,  3.4204e-01,\n",
      "          2.3766e-02, -1.3253e-01, -3.5387e-01, -2.3662e-02,  4.5045e-02,\n",
      "         -1.5766e-01, -1.8741e-02,  5.2245e-01, -1.5327e-01,  2.3718e-01,\n",
      "          9.0872e-02, -2.7640e-01,  7.4575e-02,  2.9967e-02, -1.4409e-01,\n",
      "         -7.0961e-02, -8.9910e-02, -1.8188e-01,  5.1724e-05,  2.1494e-01,\n",
      "          6.6562e-02, -9.3260e-02,  1.4597e-01, -1.9962e-01,  1.1311e-01,\n",
      "         -1.0774e-01, -2.6451e-02,  1.2101e-01,  1.2832e-01, -8.9880e-02,\n",
      "         -9.2852e-02, -2.7673e-02,  1.5501e-01, -3.5444e-02, -7.9744e-02,\n",
      "         -1.3253e-02, -8.1346e-02,  3.6077e-02,  1.6598e-01,  2.9422e-02,\n",
      "         -6.3439e-02,  1.8083e-01,  1.4419e-02,  1.2337e-01, -3.1941e-01,\n",
      "          3.4422e-03, -1.2367e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[0.0991]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = [[10180,   351, 19810,     7,    24,     6,    38,     8,  8160,  8675,\n",
    "            13,     8,  1611, 15375,     6,    62,   225,     6,    11,  5071,\n",
    "           398,     6,  2173,   981, 12361,   485,    11, 13567,    16,     8,\n",
    "          1028,  5808,     7,  1194,    11,  6572,    53,    13,    48,   540,\n",
    "             6, 10321,    69, 12123,     7,    11,   811,     7,  9098,    12,\n",
    "             3,  9582,   125,    33,   801,    38,    96, 12097,    21,   540,\n",
    "           121, 15600,    16,     8,  5334,    18, 14259,   433,     5,     1,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0]]\n",
    "\n",
    "             #delte 0 give huge diff\n",
    "       \n",
    "x = torch.Tensor(x).long().cuda()\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "distr = distr[:,0,:]\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret =  model.classifier(distr)#(bs,1)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 90])\n",
      "\t\t---------embedding\n",
      "torch.Size([1, 90, 512])\n",
      "tensor([[[  6.0501,  -7.9552,  14.4621,  ..., -21.1924,   4.0715,  12.5375],\n",
      "         [ 23.8152,  -2.8088, -15.9855,  ...,  25.2238,  -6.3611,   2.1363],\n",
      "         [  5.3305, -38.3739,  46.3184,  ...,  -6.1115, -31.1421,  -2.6996],\n",
      "         ...,\n",
      "         [ -1.9571,   0.1690,  -7.0790,  ...,  -0.4705,   2.6937,  -2.8559],\n",
      "         [ -1.9571,   0.1690,  -7.0790,  ...,  -0.4705,   2.6937,  -2.8559],\n",
      "         [ -1.9571,   0.1690,  -7.0790,  ...,  -0.4705,   2.6937,  -2.8559]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([1, 90, 512])\n",
      "tensor([[[ 0.4459, -0.1211,  0.0547,  ..., -0.3193,  0.0034, -0.1236],\n",
      "         [ 0.5885, -0.2484,  0.2204,  ..., -0.1367,  0.0661, -0.0868],\n",
      "         [ 0.0802,  0.0776,  0.3199,  ...,  0.1108, -0.1654,  0.0257],\n",
      "         ...,\n",
      "         [ 0.2089,  0.0910,  0.2934,  ..., -0.0097, -0.1524, -0.1347],\n",
      "         [ 0.2074,  0.0918,  0.2950,  ..., -0.0076, -0.1534, -0.1358],\n",
      "         [ 0.2029,  0.0898,  0.2896,  ..., -0.0022, -0.1603, -0.1410]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([1, 90, 512])\n",
      "tensor([[[ 0.4459, -0.1211,  0.0547,  ..., -0.3193,  0.0034, -0.1236],\n",
      "         [ 0.5885, -0.2484,  0.2204,  ..., -0.1367,  0.0661, -0.0868],\n",
      "         [ 0.0802,  0.0776,  0.3199,  ...,  0.1108, -0.1654,  0.0257],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([1, 512])\n",
      "tensor([[ 4.4586e-01, -1.2115e-01,  5.4724e-02,  1.2890e-02,  2.3082e-02,\n",
      "          1.5412e-01, -3.8528e-02, -4.0423e-01, -1.1271e-01, -4.0500e-01,\n",
      "         -2.1888e-01, -1.8164e-01,  6.4963e-02,  1.9092e-01, -7.2959e-02,\n",
      "         -3.8344e-01, -3.9003e-01,  2.4638e-01,  2.5232e-01,  2.6321e-01,\n",
      "         -1.4040e-01,  2.0503e-01, -3.0174e-01, -1.8084e-01,  3.3334e-01,\n",
      "         -7.1095e-02, -2.1812e-01,  1.9147e-01, -9.3195e-02,  2.1952e-02,\n",
      "         -2.3188e-01,  1.0304e-01,  6.5267e-02, -7.8560e-02,  1.1689e-01,\n",
      "          5.0230e-03,  1.5565e-01,  5.7025e-03, -8.9193e-02,  8.8293e-02,\n",
      "         -2.9734e-01, -8.7702e-03,  1.9224e-01, -3.6896e-02,  9.4753e-02,\n",
      "         -1.3012e-01,  2.1547e-01,  2.9993e-01,  5.4918e-02,  1.6205e-01,\n",
      "         -2.8498e-01,  6.7164e-02, -2.8917e-01, -1.0540e-01,  1.0553e-02,\n",
      "          2.0170e-01, -9.4270e-02,  3.2848e-01, -1.9384e-01, -1.1353e-01,\n",
      "         -2.3405e-01,  8.0410e-02, -6.5546e-02,  1.7782e-01, -2.6557e-02,\n",
      "          1.0003e-01, -5.9267e-01,  2.0243e-01, -6.5568e-02,  4.1943e-01,\n",
      "          3.3438e-01, -2.6199e-01, -1.6734e-01, -1.4144e-01,  4.3216e-01,\n",
      "         -1.9035e-01,  5.8613e-02, -2.4683e-01,  1.3648e-01, -1.4428e-02,\n",
      "         -1.2000e-01, -1.4916e-02,  1.3612e-01, -1.6838e-01,  4.1732e-02,\n",
      "          5.5947e-02, -3.8744e-01,  4.1196e-02,  9.3545e-02, -9.4925e-02,\n",
      "         -8.8335e-02,  5.7431e-02, -9.2814e-02,  1.5063e-01,  3.3398e-02,\n",
      "         -1.4744e-01,  1.5498e-01,  4.6727e-03, -9.4194e-03, -3.3776e-01,\n",
      "          9.8654e-02,  9.5991e-02, -1.0694e-01, -3.8644e-02, -7.4914e-02,\n",
      "          2.1705e-01,  2.5757e-01,  1.3817e-01,  1.0641e-01, -1.7404e-02,\n",
      "         -1.1257e-01, -1.7434e-01,  1.8331e-01,  3.3833e-01,  2.4489e-01,\n",
      "         -1.2069e-01, -3.2951e-01,  1.3213e-01,  5.5920e-02, -2.1665e-01,\n",
      "         -1.7534e-01,  2.5322e-02, -2.8503e-01, -2.6696e-01, -1.3239e-01,\n",
      "         -5.6156e-02,  4.3549e-01, -4.2611e-01,  1.5100e-01,  2.2253e-01,\n",
      "         -6.4732e-02,  1.4271e-01,  3.1022e-01,  1.2548e-01, -1.1370e-01,\n",
      "          4.6833e-01,  1.2513e-01,  1.6106e-01,  8.7263e-02,  2.3771e-01,\n",
      "          2.6311e-01, -7.0890e-02, -1.0369e-01, -5.4818e-02,  2.3668e-01,\n",
      "         -1.5059e-02, -1.2238e-01,  1.0248e-01,  2.6542e-01,  5.0181e-03,\n",
      "         -1.8197e-01,  2.5110e-01,  8.6190e-02,  2.7730e-02,  2.7881e-01,\n",
      "          8.0908e-02, -6.0254e-02,  1.1774e-01, -1.1150e-02,  1.0123e-01,\n",
      "         -1.0968e-02,  7.4300e-02, -2.3163e-01, -1.1903e-01,  1.7200e-01,\n",
      "         -3.9356e-02, -1.7952e-01, -2.9357e-02, -2.2594e-01, -2.7680e-01,\n",
      "          1.7124e-01,  1.9208e-01,  1.2279e-01,  2.6156e-01,  2.0788e-01,\n",
      "          9.1817e-02, -1.7293e-01, -1.1985e-01,  3.7898e-02, -1.3385e-01,\n",
      "         -1.8640e-01,  1.5988e-01, -1.3462e-01,  1.5238e-01,  1.9565e-01,\n",
      "         -1.1269e-01, -7.4794e-03,  1.2275e-01,  1.4523e-01, -1.0663e-01,\n",
      "         -1.3701e-01,  6.4537e-02, -6.0043e-02,  1.4707e-02,  1.8271e-01,\n",
      "          1.7225e-01,  1.6547e-01, -1.1709e-01, -8.2723e-02,  4.2450e-02,\n",
      "          2.1734e-02, -1.6744e-01,  1.0624e-01, -1.6964e-03, -1.4610e-01,\n",
      "          2.4403e-01, -2.5028e-01, -2.2146e-02,  5.5005e-03, -1.0475e-01,\n",
      "          1.7559e-01, -2.2455e-01, -1.3709e-02, -3.2324e-02, -2.9057e-02,\n",
      "          2.8964e-02, -2.0511e-01,  2.1526e-01,  5.1290e-02, -1.3582e-01,\n",
      "          9.9062e-02, -8.9605e-02,  4.3284e-02,  1.0069e-01, -1.0818e-01,\n",
      "         -2.7954e-01,  2.2378e-01,  4.6227e-01, -2.9963e-02,  3.3397e-01,\n",
      "         -1.1864e-01,  2.0858e-01, -9.2885e-02, -2.1337e-01, -3.7849e-01,\n",
      "          1.2233e-01, -2.1850e-01,  6.8367e-02,  3.1767e-01, -2.5071e-01,\n",
      "          9.0172e-02,  1.5066e-01, -1.9301e-01, -3.0303e-01,  2.3503e-01,\n",
      "         -1.5237e-01, -4.0844e-02,  2.3615e-01,  1.1559e-01,  2.2258e-01,\n",
      "          9.1561e-02, -8.3780e-02,  2.4762e-01,  1.5804e-01, -1.6012e-01,\n",
      "         -4.2730e-03,  3.1711e-01,  1.1360e-01,  6.4501e-02, -1.1514e-01,\n",
      "         -5.0247e-03, -7.2399e-02,  3.9328e-02, -4.9143e-02,  1.6153e-02,\n",
      "          8.2930e-02, -2.5355e-02, -8.0248e-02, -3.5288e-01,  2.6252e-01,\n",
      "          2.6453e-02, -2.4730e-01,  5.7609e-02, -5.7079e-02, -2.0044e-01,\n",
      "         -1.4179e-02,  1.1840e-01,  7.2774e-02,  3.2290e-01,  1.0386e-01,\n",
      "          2.2030e-02,  1.3842e-02, -4.3523e-01,  3.0808e-02,  2.1685e-02,\n",
      "          1.4759e-01,  4.1830e-02, -3.4783e-01, -4.1124e-01, -2.6410e-02,\n",
      "         -4.1423e-01, -3.2453e-01, -1.4217e-01,  3.7290e-01, -5.2395e-02,\n",
      "          1.2625e-01,  2.2331e-01, -7.9174e-02,  4.9249e-03, -8.6954e-03,\n",
      "          5.3777e-02, -1.4919e-01,  3.5011e-02,  8.8929e-02, -3.5657e-01,\n",
      "         -2.2639e-01, -8.1408e-03,  1.8187e-01, -3.6784e-02, -4.3167e-02,\n",
      "         -4.8089e-02,  1.7775e-01, -6.6968e-02,  1.7098e-01, -4.3101e-01,\n",
      "          2.0199e-02,  5.2304e-03,  1.9707e-01,  2.8738e-01,  7.0337e-02,\n",
      "          8.0766e-02, -3.6731e-01,  1.1788e-01, -2.9282e-01, -7.2791e-02,\n",
      "         -9.9467e-02,  1.7471e-01, -3.0867e-01,  2.0913e-01,  1.3815e-01,\n",
      "         -1.8020e-02, -3.6668e-01, -1.0730e-01,  1.8882e-01,  2.0004e-01,\n",
      "         -1.7796e-01,  7.0304e-02, -7.4963e-02,  1.3316e-01,  2.1148e-01,\n",
      "          1.2084e-01, -1.3383e-01,  7.1516e-02, -1.4127e-01,  2.3986e-01,\n",
      "         -3.0795e-01, -3.8333e-02,  7.5301e-02, -1.1513e-01, -5.1147e-02,\n",
      "         -1.3114e-01,  3.0916e-02, -2.4485e-02, -5.8888e-02, -3.2668e-01,\n",
      "          3.1066e-01, -1.2925e-01, -3.3520e-01, -1.3545e-01,  2.9206e-02,\n",
      "         -1.0787e-01,  2.8933e-01,  3.2185e-01,  3.1203e-01,  1.4951e-01,\n",
      "          7.8109e-02,  1.7175e-02, -1.8999e-01, -3.8641e-02, -1.0188e-01,\n",
      "          2.7369e-01, -1.0220e-01,  1.2844e-01, -1.1425e-01,  1.3173e-01,\n",
      "         -6.9459e-02,  2.3555e-02,  4.2563e-01, -4.0102e-01,  7.0367e-02,\n",
      "         -1.8167e-01,  3.5589e-01, -2.6746e-02, -7.1080e-02,  1.1596e-01,\n",
      "          1.0709e-02, -1.7888e-01,  2.3181e-01, -2.2737e-01,  2.7469e-01,\n",
      "          1.3284e-01,  4.7198e-02, -4.0472e-01,  1.4649e-01, -2.0549e-01,\n",
      "         -3.2609e-01, -3.2305e-01, -1.0265e-01,  2.6659e-02,  2.8538e-02,\n",
      "         -1.0634e-02, -2.1133e-01,  1.7609e-01, -5.2117e-02,  2.3452e-01,\n",
      "          1.0301e-01,  1.0026e-01,  2.0035e-01,  4.3152e-02, -2.7166e-01,\n",
      "         -3.4948e-02,  4.8457e-02,  6.7328e-02,  2.1033e-01, -6.9411e-01,\n",
      "         -1.7415e-04,  8.8941e-02, -7.3952e-03, -1.2565e-01, -2.9724e-01,\n",
      "          5.3807e-03, -1.3224e-01,  7.9579e-02, -2.5949e-01,  1.3001e-01,\n",
      "         -1.2015e-01,  1.7482e-01, -1.0795e-01,  2.9388e-01,  8.3666e-02,\n",
      "         -4.4747e-01,  2.8241e-01,  4.8818e-02,  2.0853e-02, -1.3630e-01,\n",
      "         -1.0785e-01,  3.1522e-02, -1.2723e-01, -2.6988e-01,  1.0104e-01,\n",
      "          1.5660e-01,  4.8841e-03, -9.3675e-02,  2.6065e-01,  2.9747e-01,\n",
      "         -1.0022e-01, -2.1884e-02, -1.4131e-01, -2.3339e-02, -3.1599e-01,\n",
      "         -1.9048e-01,  2.3084e-01,  5.7213e-02, -1.1507e-01, -2.8808e-02,\n",
      "         -5.5827e-01,  1.0932e-01, -4.9486e-02,  2.9695e-01,  5.2483e-02,\n",
      "         -1.0850e-01, -2.1465e-01,  2.2766e-01,  1.2653e-01,  3.4208e-01,\n",
      "          2.3600e-02, -1.3255e-01, -3.5376e-01, -2.3375e-02,  4.4977e-02,\n",
      "         -1.5768e-01, -1.8732e-02,  5.2263e-01, -1.5329e-01,  2.3689e-01,\n",
      "          9.0792e-02, -2.7648e-01,  7.4705e-02,  2.9913e-02, -1.4399e-01,\n",
      "         -7.0969e-02, -8.9767e-02, -1.8189e-01, -1.1661e-05,  2.1493e-01,\n",
      "          6.6423e-02, -9.3313e-02,  1.4608e-01, -1.9961e-01,  1.1300e-01,\n",
      "         -1.0777e-01, -2.6387e-02,  1.2103e-01,  1.2827e-01, -8.9810e-02,\n",
      "         -9.2863e-02, -2.7724e-02,  1.5504e-01, -3.5398e-02, -7.9888e-02,\n",
      "         -1.3254e-02, -8.1438e-02,  3.5914e-02,  1.6598e-01,  2.9478e-02,\n",
      "         -6.3398e-02,  1.8086e-01,  1.4364e-02,  1.2320e-01, -3.1930e-01,\n",
      "          3.4139e-03, -1.2363e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[0.0990]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = [[10180,   351, 19810,     7,    24,     6,    38,     8,  8160,  8675,\n",
    "            13,     8,  1611, 15375,     6,    62,   225,     6,    11,  5071,\n",
    "           398,     6,  2173,   981, 12361,   485,    11, 13567,    16,     8,\n",
    "          1028,  5808,     7,  1194,    11,  6572,    53,    13,    48,   540,\n",
    "             6, 10321,    69, 12123,     7,    11,   811,     7,  9098,    12,\n",
    "             3,  9582,   125,    33,   801,    38,    96, 12097,    21,   540,\n",
    "           121, 15600,    16,     8,  5334,    18, 14259,   433,     5,     1,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]\n",
    "\n",
    "             #delte 0 give huge diff\n",
    "       \n",
    "x = torch.Tensor(x).long().cuda()\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "distr = distr[:,0,:]\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret =  model.classifier(distr)#(bs,1)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9008]], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# x_emb = model.embedding(x_)#\n",
    "# print('x_emb.shape',x_emb.shape)\n",
    "# distr = model.encoder(inputs_embeds=x_emb).last_hidden_state#(bs,sentence length,512)\n",
    "# print('distr.shape',distr.shape)\n",
    "# x_attn= x_attn.unsqueeze(-1)\n",
    "# print('x_attn.shape',x_attn.shape)\n",
    "# distr = torch.mul(distr,x_attn)#previously\n",
    "\n",
    "# print('mutil',torch.sum(distr,1))\n",
    "# print('count',torch.sum(x_attn,1))\n",
    "# distr = torch.sum(distr,1)/torch.sum(x_attn,1)#(bs,512)\n",
    "# print(torch.mean(distr,-1))\n",
    "# print('mea',distr.shape)\n",
    "\n",
    "# ret =  model.classifier(distr)#(bs,1)\n",
    "# print(ret)\n",
    "# ret = model.relu(ret)#(bs,1)\n",
    "# print(ret)\n",
    "\n",
    "\n",
    "\n",
    "a_pred_dis = model(x_,x_attn)\n",
    "a_pred_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ",attention_mask=x_attn is essential for the encoder inpuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We previously use torch.mean to (x after embedding)*x_attn but the mean will be small for the long sentences, now we use torch.sum(distr,1)/torch.sum(x_attn,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the D judge the sentence by it's length\n",
    "## small output for the first part, but large for the latter part, so the longer the higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## so we change the output to [batch,0,512] ie, onlythe first word's sumQ*V and than droppout and then classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
