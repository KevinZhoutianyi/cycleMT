{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.getcwd() \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "from test import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "from basic_model import *\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string\n",
    "from cycle import *\n",
    "from train import *\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "def d(l):\n",
    "    return tokenizer.batch_decode(l,skip_special_tokens=True)\n",
    "def en(l):\n",
    "    return tokenize(l,tokenizer,512,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "en(['.'])\n",
    "label = en(['Sehr geehrte Kollegen! Ich danke Frau Schroedter für den fundierten Bericht.', 'Sie hat sich eingehend mit der Problematik beschäftigt und während der Behandlung im Ausschuß viele zu dem vorliegenden Bericht eingegangene nderungsanträge berücksichtigt.'])\n",
    "pred = en(['Herr Kommissar, liebe Kolleginnen und Kollegen!', 'Sie hat in der Aussprache im Ausschuss viele nderungsanträge in Bezug auf diesen Bericht berücksichtigt.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    3, 18976,   873,  7392,    17,    15, 23420,    55,  1674, 12957,\n",
       "             15,  7672,   180, 10363,    15,    26,   449,   218,   177,  3069,\n",
       "           6305, 19146,     5,     1,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  292,     3,   547,   289,   236,  8615,    26,   181,    74,  5289,\n",
       "              9,  4414, 23029,    64,     3,  5729,    74, 12196,   256,  1392,\n",
       "            860,  6712,  2584,   170,   340,   426, 22646, 19146,   236, 16416,\n",
       "             15,     3, 20181,     7,   288,  7921,   397, 29699,     5,     1]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 8816,  5837, 11502,   291,     6, 23803, 12561,  5772,  4015,    64,\n",
       "          23420,    55,     1,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  292,     3,   547,    16,    74,  1392, 19107,   256,  1392, 18820,\n",
       "           2584,     3, 20181,     7,   288,  7921,   397,    16, 20297,   219,\n",
       "              3,  5162, 19146, 29699,     5,     1]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./model/D_A.pt').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    3, 18976,   873,  7392,    17,    15, 23420,    55,  1674, 12957,\n",
      "            15,  7672,   180, 10363,    15,    26,   449,   218,   177,  3069,\n",
      "          6305, 19146,     5,     1,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  292,     3,   547,   289,   236,  8615,    26,   181,    74,  5289,\n",
      "             9,  4414, 23029,    64,     3,  5729,    74, 12196,   256,  1392,\n",
      "           860,  6712,  2584,   170,   340,   426, 22646, 19146,   236, 16416,\n",
      "            15,     3, 20181,     7,   288,  7921,   397, 29699,     5,     1]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 40])\n",
      "\t\t---------embedding\n",
      "torch.Size([2, 40, 512])\n",
      "tensor([[[ 11.2423,   8.0548,  14.1798,  ...,   9.8048,  -7.8672,  -3.6016],\n",
      "         [-20.7424, -14.8048,  -2.7735,  ..., -33.7422,  28.2424,  27.2424],\n",
      "         [ 17.9924, -13.6173,   4.1172,  ...,   4.8047,  33.2422,   1.1485],\n",
      "         ...,\n",
      "         [ -2.0079,   0.2159,  -7.0860,  ...,  -0.3458,   2.6329,  -2.8829],\n",
      "         [ -2.0079,   0.2159,  -7.0860,  ...,  -0.3458,   2.6329,  -2.8829],\n",
      "         [ -2.0079,   0.2159,  -7.0860,  ...,  -0.3458,   2.6329,  -2.8829]],\n",
      "\n",
      "        [[ -9.2423,  -7.9297,  17.2424,  ...,  18.8674,  17.2424,  18.3674],\n",
      "         [ 11.2423,   8.0548,  14.1798,  ...,   9.8048,  -7.8672,  -3.6016],\n",
      "         [ 15.1798,  -6.5234, -12.6798,  ...,   3.5547,  -7.9297,  -6.7734],\n",
      "         ...,\n",
      "         [-10.1798,  -5.5547,  -3.6641,  ..., -15.2423,  -1.7032,   7.8984],\n",
      "         [ -4.2734,   7.3984, -14.5548,  ...,  13.0548,  -2.7266,  -3.1329],\n",
      "         [ 12.6173,   8.1798, -11.6173,  ...,   7.9297,  -7.3047,   0.9376]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([2, 40, 512])\n",
      "tensor([[[ 1.0227e-02,  7.6968e-03, -3.7677e-03,  ..., -1.4894e-03,\n",
      "          -7.1285e-03, -8.8973e-03],\n",
      "         [ 8.5759e-02, -8.6700e-02, -9.8275e-02,  ..., -2.5853e-01,\n",
      "           2.1354e-01, -9.5928e-03],\n",
      "         [ 1.6585e-01, -2.4469e-04,  1.1532e-02,  ..., -5.1199e-03,\n",
      "           2.6414e-01, -3.2429e-01],\n",
      "         ...,\n",
      "         [ 9.5125e-02,  4.8417e-02,  1.1213e-01,  ..., -2.3261e-01,\n",
      "           1.1690e-01, -1.5210e-01],\n",
      "         [ 9.3169e-02,  4.7268e-02,  1.1441e-01,  ..., -2.3323e-01,\n",
      "           1.1436e-01, -1.5435e-01],\n",
      "         [ 8.7124e-02,  5.0334e-02,  1.1660e-01,  ..., -2.2848e-01,\n",
      "           1.2538e-01, -1.6144e-01]],\n",
      "\n",
      "        [[ 3.1574e-01, -8.9357e-02, -1.2113e-01,  ..., -1.3225e-01,\n",
      "           4.2652e-02, -4.7455e-02],\n",
      "         [ 1.6862e-01,  1.1286e-01, -1.4159e-02,  ...,  9.1183e-02,\n",
      "           1.2466e-01, -1.9976e-01],\n",
      "         [ 1.9081e-01, -6.0202e-02, -2.3745e-01,  ..., -6.4766e-02,\n",
      "           1.8470e-01,  4.4790e-02],\n",
      "         ...,\n",
      "         [-2.2813e-02,  9.1965e-02, -1.2995e-01,  ...,  4.9876e-02,\n",
      "          -3.0482e-03, -5.0338e-02],\n",
      "         [ 7.6063e-02,  1.7229e-01, -5.2829e-02,  ...,  6.2679e-02,\n",
      "          -2.2419e-03, -2.8513e-01],\n",
      "         [-1.6726e-03,  5.1924e-03, -8.8875e-03,  ...,  1.0171e-03,\n",
      "           4.5939e-03, -4.5313e-02]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([2, 40, 512])\n",
      "tensor([[[ 1.0227e-02,  7.6968e-03, -3.7677e-03,  ..., -1.4894e-03,\n",
      "          -7.1285e-03, -8.8973e-03],\n",
      "         [ 8.5759e-02, -8.6700e-02, -9.8275e-02,  ..., -2.5853e-01,\n",
      "           2.1354e-01, -9.5928e-03],\n",
      "         [ 1.6585e-01, -2.4469e-04,  1.1532e-02,  ..., -5.1199e-03,\n",
      "           2.6414e-01, -3.2429e-01],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "           0.0000e+00, -0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "           0.0000e+00, -0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "           0.0000e+00, -0.0000e+00]],\n",
      "\n",
      "        [[ 3.1574e-01, -8.9357e-02, -1.2113e-01,  ..., -1.3225e-01,\n",
      "           4.2652e-02, -4.7455e-02],\n",
      "         [ 1.6862e-01,  1.1286e-01, -1.4159e-02,  ...,  9.1183e-02,\n",
      "           1.2466e-01, -1.9976e-01],\n",
      "         [ 1.9081e-01, -6.0202e-02, -2.3745e-01,  ..., -6.4766e-02,\n",
      "           1.8470e-01,  4.4790e-02],\n",
      "         ...,\n",
      "         [-2.2813e-02,  9.1965e-02, -1.2995e-01,  ...,  4.9876e-02,\n",
      "          -3.0482e-03, -5.0338e-02],\n",
      "         [ 7.6063e-02,  1.7229e-01, -5.2829e-02,  ...,  6.2679e-02,\n",
      "          -2.2419e-03, -2.8513e-01],\n",
      "         [-1.6726e-03,  5.1924e-03, -8.8875e-03,  ...,  1.0171e-03,\n",
      "           4.5939e-03, -4.5313e-02]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([2, 512])\n",
      "tensor([[ 0.0583,  0.0757, -0.0879,  ...,  0.0088,  0.1127, -0.1250],\n",
      "        [ 0.1269,  0.0932, -0.0840,  ..., -0.0374,  0.1536, -0.1391]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[0.7381],\n",
      "        [0.8658]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.7381],\n",
      "        [0.8658]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = label[0].cuda()\n",
    "#delte 0 give huge diff\n",
    "       \n",
    "# x = torch.Tensor(x).long().cuda()\n",
    "print(x)\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn_unsq,1)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret =  model.classifier(distr)#(bs,1)\n",
    "print(ret)\n",
    "\n",
    "print(model(x,x_attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 26])\n",
      "\t\t---------embedding\n",
      "torch.Size([2, 26, 512])\n",
      "tensor([[[-21.7424, -25.6174,  -2.0547,  ...,  14.5548,  42.2422,  13.1173],\n",
      "         [ 14.1173,  -0.1182,  11.7423,  ...,  31.9924,  13.9298,   8.4923],\n",
      "         [  5.1172, -21.1174,   6.1172,  ...,  38.2422,  10.8673,  38.7422],\n",
      "         ...,\n",
      "         [ -2.0079,   0.2159,  -7.0860,  ...,  -0.3458,   2.6329,  -2.8829],\n",
      "         [ -2.0079,   0.2159,  -7.0860,  ...,  -0.3458,   2.6329,  -2.8829],\n",
      "         [ -2.0079,   0.2159,  -7.0860,  ...,  -0.3458,   2.6329,  -2.8829]],\n",
      "\n",
      "        [[ -9.2423,  -7.9297,  17.2424,  ...,  18.8674,  17.2424,  18.3674],\n",
      "         [ 11.2423,   8.0548,  14.1798,  ...,   9.8048,  -7.8672,  -3.6016],\n",
      "         [ 15.1798,  -6.5234, -12.6798,  ...,   3.5547,  -7.9297,  -6.7734],\n",
      "         ...,\n",
      "         [-10.1798,  -5.5547,  -3.6641,  ..., -15.2423,  -1.7032,   7.8984],\n",
      "         [ -4.2734,   7.3984, -14.5548,  ...,  13.0548,  -2.7266,  -3.1329],\n",
      "         [ 12.6173,   8.1798, -11.6173,  ...,   7.9297,  -7.3047,   0.9376]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\t\t---------encode\n",
      "torch.Size([2, 26, 512])\n",
      "tensor([[[ 0.1551,  0.1373, -0.2153,  ..., -0.1373,  0.3136, -0.1141],\n",
      "         [ 0.0762, -0.0373, -0.0197,  ..., -0.0451,  0.2870, -0.2036],\n",
      "         [ 0.0886, -0.0481, -0.0673,  ..., -0.0509,  0.1784, -0.2394],\n",
      "         ...,\n",
      "         [ 0.1514,  0.0281,  0.1920,  ..., -0.1284,  0.1863, -0.1333],\n",
      "         [ 0.1559,  0.0300,  0.1864,  ..., -0.1273,  0.1943, -0.1454],\n",
      "         [ 0.1584,  0.0283,  0.1857,  ..., -0.1312,  0.1919, -0.1400]],\n",
      "\n",
      "        [[ 0.1802, -0.0877, -0.1914,  ..., -0.1031,  0.0611, -0.0013],\n",
      "         [ 0.0814,  0.1269, -0.1387,  ...,  0.0864,  0.1833, -0.1673],\n",
      "         [ 0.0951,  0.0275, -0.3097,  ...,  0.0037,  0.2216, -0.0114],\n",
      "         ...,\n",
      "         [-0.0468,  0.0613, -0.1618,  ...,  0.1045,  0.0493, -0.0485],\n",
      "         [-0.0074,  0.1531, -0.0910,  ...,  0.0615, -0.0257, -0.2584],\n",
      "         [ 0.0034,  0.0175, -0.0117,  ...,  0.0011, -0.0005, -0.0337]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mask the vector\n",
      "torch.Size([2, 26, 512])\n",
      "tensor([[[ 0.1551,  0.1373, -0.2153,  ..., -0.1373,  0.3136, -0.1141],\n",
      "         [ 0.0762, -0.0373, -0.0197,  ..., -0.0451,  0.2870, -0.2036],\n",
      "         [ 0.0886, -0.0481, -0.0673,  ..., -0.0509,  0.1784, -0.2394],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "        [[ 0.1802, -0.0877, -0.1914,  ..., -0.1031,  0.0611, -0.0013],\n",
      "         [ 0.0814,  0.1269, -0.1387,  ...,  0.0864,  0.1833, -0.1673],\n",
      "         [ 0.0951,  0.0275, -0.3097,  ...,  0.0037,  0.2216, -0.0114],\n",
      "         ...,\n",
      "         [-0.0468,  0.0613, -0.1618,  ...,  0.1045,  0.0493, -0.0485],\n",
      "         [-0.0074,  0.1531, -0.0910,  ...,  0.0615, -0.0257, -0.2584],\n",
      "         [ 0.0034,  0.0175, -0.0117,  ...,  0.0011, -0.0005, -0.0337]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\t\t---------mean of the unmask \n",
      "torch.Size([2, 512])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 512])\n",
      "tensor([[ 0.0282,  0.0702, -0.1655,  ..., -0.0573,  0.1841, -0.1624],\n",
      "        [ 0.0728,  0.1187, -0.1113,  ..., -0.0204,  0.1650, -0.1353]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "\t\t---------classifer \n",
      "tensor([[0.7068],\n",
      "        [0.8332]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#real sentences\n",
    "x = pred[0].cuda()\n",
    "       \n",
    "# x = torch.Tensor(x).long().cuda()\n",
    "print(x.shape)\n",
    "x_attn = (x>0.5).long()\n",
    "\n",
    "print('\\t\\t---------embedding')\n",
    "x_emb = model.embedding(x)\n",
    "print(x_emb.shape)\n",
    "print(x_emb)# same for padding\n",
    "\n",
    "print('\\t\\t---------encode')\n",
    "distr = model.encoder(inputs_embeds=x_emb,attention_mask=x_attn).last_hidden_state#(bs,sentence length,512)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------mask the vector')\n",
    "x_attn_unsq= x_attn.unsqueeze(-1)\n",
    "distr = torch.mul(distr,x_attn_unsq)#previously\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "\n",
    "print('\\t\\t---------mean of the unmask ')\n",
    "print(torch.sum(distr,1).shape)\n",
    "print(torch.sum(x_attn,1).shape)\n",
    "distr = torch.sum(distr,1)/torch.sum(x_attn_unsq,1)\n",
    "print(distr.shape)\n",
    "print(distr)\n",
    "\n",
    "print('\\t\\t---------classifer ')\n",
    "ret =  model.classifier(distr)#(bs,1)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2.],\n",
      "        [1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000],\n",
       "        [1.0000, 1.0000]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "b = torch.ones(2,1)\n",
    "b[0][0]=2\n",
    "print(a)\n",
    "print(b)\n",
    "a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9008]], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# x_emb = model.embedding(x_)#\n",
    "# print('x_emb.shape',x_emb.shape)\n",
    "# distr = model.encoder(inputs_embeds=x_emb).last_hidden_state#(bs,sentence length,512)\n",
    "# print('distr.shape',distr.shape)\n",
    "# x_attn= x_attn.unsqueeze(-1)\n",
    "# print('x_attn.shape',x_attn.shape)\n",
    "# distr = torch.mul(distr,x_attn)#previously\n",
    "\n",
    "# print('mutil',torch.sum(distr,1))\n",
    "# print('count',torch.sum(x_attn,1))\n",
    "# distr = torch.sum(distr,1)/torch.sum(x_attn,1)#(bs,512)\n",
    "# print(torch.mean(distr,-1))\n",
    "# print('mea',distr.shape)\n",
    "\n",
    "# ret =  model.classifier(distr)#(bs,1)\n",
    "# print(ret)\n",
    "# ret = model.relu(ret)#(bs,1)\n",
    "# print(ret)\n",
    "\n",
    "\n",
    "\n",
    "a_pred_dis = model(x_,x_attn)\n",
    "a_pred_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ",attention_mask=x_attn is essential for the encoder inpuit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We previously use torch.mean to (x after embedding)*x_attn but the mean will be small for the long sentences, now we use torch.sum(distr,1)/torch.sum(x_attn,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the D judge the sentence by it's length\n",
    "## small output for the first part, but large for the latter part, so the longer the higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## so we change the output to [batch,0,512] ie, onlythe first word's sumQ*V and than droppout and then classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
